{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running importance analysis with Hail\n",
    "=====================================\n",
    "\n",
    "This is an *VariantSpark* example notebook.\n",
    "\n",
    "\n",
    "One of the main applications of VariantSpark is discovery of genomic variants correlated with a response variable (e.g. case vs control) using random forest gini importance.\n",
    "\n",
    "The `chr22_1000.vcf` is a very small sample of the chromosome 22 VCF file from the 1000 Genomes Project.\n",
    "\n",
    "`chr22-labels.csv` is a CSV file with sample response variables (labels). In fact the labels directly represent the number of alternative alleles for each sample at a specific genomic position. E.g.: column 22_16050408 has labels derived from variants in chromosome 22 position 16050408. We would expect then that position 22:16050408 in the VCF file is strongly correlated with the label 22_16050408.\n",
    "\n",
    "Both data sets are located in the `..\\data` directory.\n",
    "\n",
    "This notebook demonstrates how to run importance analysis on these data with *VariantSpark* Hail integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Create a `HailContext` using `SparkContext` object (here injected as `sc`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.2.1\n",
      "SparkUI available at http://140.253.176.47:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.1-74bf1eb\n"
     ]
    }
   ],
   "source": [
    "from hail import HailContext\n",
    "import varspark.hail\n",
    "hc = HailContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load Hail variant dataset  `vds` from a sample `.vcf` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-11 12:05:17 Hail: INFO: No multiallelics detected.\n",
      "2018-07-11 12:05:17 Hail: INFO: Coerced almost-sorted dataset\n"
     ]
    }
   ],
   "source": [
    "vds = hc.import_vcf('../data/chr22_1000.vcf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load labels into Hail table `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-11 12:05:17 Hail: INFO: Reading table to impute column types\n",
      "2018-07-11 12:05:17 Hail: INFO: Finished type imputation\n",
      "  Loading column `sample' as type String (imputed)\n",
      "  Loading column `22_16050408' as type Int (imputed)\n",
      "  Loading column `22_16050612' as type Int (imputed)\n",
      "  Loading column `22_16050678' as type Int (imputed)\n",
      "  Loading column `22_16050984' as type Int (imputed)\n",
      "  Loading column `22_16051107' as type Int (imputed)\n",
      "  Loading column `22_16051249' as type Int (imputed)\n",
      "  Loading column `22_16051347' as type Int (imputed)\n",
      "  Loading column `22_16051453' as type Int (imputed)\n",
      "  Loading column `22_16051477' as type Int (imputed)\n",
      "  Loading column `22_16051480' as type Int (imputed)\n"
     ]
    }
   ],
   "source": [
    "labels = hc.import_table('../data/chr22-labels.csv', key=\"sample\", impute = True, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Annotate dataset samples with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vds = vds.annotate_samples_table(labels, root=\"sa.pheno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Run the importance analysis and retrieve important variants (as Hail table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "via = vds.importance_analysis(\"sa.pheno.`22_16050408`\", n_trees = 500, seed = 13L,  batch_size = 20)\n",
    "iv  = via.important_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest OOB error: 0.014652014652\n"
     ]
    }
   ],
   "source": [
    "print(\"Random forest OOB error: %s\" % via.oob_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------+------------------+--------------------+\n",
      "|variant.contig|variant.start|variant.ref|variant.altAlleles|          importance|\n",
      "+--------------+-------------+-----------+------------------+--------------------+\n",
      "|            22|     16050408|          T|           [[T,C]]|9.040375514083192E-4|\n",
      "|            22|     16051480|          T|           [[T,C]]|8.397590270282872E-4|\n",
      "|            22|     16050678|          C|           [[C,T]]|6.948530160834497E-4|\n",
      "|            22|     16052838|          T|           [[T,A]]|6.855251834675504E-4|\n",
      "|            22|     16053197|          G|           [[G,T]]| 6.45022919591987E-4|\n",
      "|            22|     16051107|          C|           [[C,A]]|6.206310454636674E-4|\n",
      "|            22|     16053435|          G|           [[G,T]]|5.515279651774938E-4|\n",
      "|            22|     16052656|          T|           [[T,C]]|5.061636050055417E-4|\n",
      "|            22|     16053509|          A|           [[A,G]]|4.640749808744204...|\n",
      "|            22|     16051882|          C|           [[C,T]]|4.288113897578818...|\n",
      "|            22|     16053727|          T|           [[T,G]]|3.394497537922537E-4|\n",
      "|            22|     16054283|          C|           [[C,T]]|3.350158697511629E-4|\n",
      "|            22|     16053797|          T|           [[T,C]]|3.207611581834927E-4|\n",
      "|            22|     16050612|          C|           [[C,G]]|2.580734105682278E-4|\n",
      "|            22|     16053881|          A|           [[A,C]]|1.555713300501837...|\n",
      "|            22|     16052250|          A|           [[A,G]]|1.254138216378914E-4|\n",
      "|            22|     16052080|          G|           [[G,A]]|2.616244760462994...|\n",
      "|            22|     16053001|          A|           [[A,T]]|2.150887036556173...|\n",
      "|            22|     17734760|          G|           [[G,A]]|2.024932515976314...|\n",
      "|            22|     17786438|          A|           [[A,G]]|1.944329842971620...|\n",
      "+--------------+-------------+-----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iv.to_dataframe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on using *VariantSpark* and the Python API and Hail integration please visit the [documentation](http://variantspark.readthedocs.io/en/latest/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
