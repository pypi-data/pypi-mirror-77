{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_of_usage.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzMfrFLTZR5gfAKQ94xvOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panos108/Training_ANN_with_Pytorch/blob/master/example_of_usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzL77vGM6vTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "22c5ea0b-e225-4381-dc8e-d111a1f0519a"
      },
      "source": [
        "!pip install simpleTorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Training_ANN_with_Pytorch'...\n",
            "remote: Enumerating objects: 47, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 47 (delta 22), reused 34 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (47/47), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lqcajUGGVce",
        "colab_type": "text"
      },
      "source": [
        "**Construct ANN**\n",
        "\n",
        "You first need to initialize the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4c5oDDVGbmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(state_size, 20)\n",
        "        self.layer2 = nn.Linear(20, 20)\n",
        "        self.layer3 = nn.Linear(20, 20)\n",
        "        self.action = nn.Linear(20, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        m      = torch.nn.LeakyReLU(0.1)#0.01)\n",
        "        layer1 = m(self.layer1(state))\n",
        "        layer2 =m(self.layer2(layer1))\n",
        "        layer3 = m(self.layer3(layer2))\n",
        "        action = (self.action(layer3))\n",
        "        return (action)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q09XsmRVGe9m",
        "colab_type": "text"
      },
      "source": [
        "**Generate the input-output data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBapBJ4jGjt0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from simpleTorch.train_ann import *\n",
        "x_m = 1000  # number of samples\n",
        "x = np.random.default_rng().uniform(-5, 5, x_m)\n",
        "y = np.random.default_rng().uniform(-5, 5, x_m)\n",
        "X = np.array((x, y)).T\n",
        "\n",
        "F = []\n",
        "for i in range(0, x_m, 1):\n",
        "    f = np.array([[100 * (y[i] - x[i] ** 2) ** 2 + (1 - x[i]) ** 2, x[i]*y[i]]])\n",
        "    F.append(*f.reshape(1,2))\n",
        "F = np.array(F)\n",
        "if f.ndim==1:\n",
        "    F = F.reshape(-1, 1)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-gl6fkaGltX",
        "colab_type": "text"
      },
      "source": [
        "**Initialize the neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYS09Lz3Gn8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(X.shape[1],F.shape[1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UirFyffaGsU9",
        "colab_type": "text"
      },
      "source": [
        "**Perform the training with all the default values of the class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVH3TYB2GxT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5d6aff3f-0fbf-4d91-830f-1213e671ba42"
      },
      "source": [
        "ANN = train_ann(model, X, F)\n",
        "# You can have different options to choose from lise optimizer, objective function, to plot or print\n",
        "#learning_rate = 0.01\n",
        "#ANN =  train_ann(model, X, F, optimizer=optim.Adam(model.parameters(), lr=learning_rate), \n",
        "#                 loss_fn=nn.MSELoss(), learning_rate=learning_rate, print_val=False, plot=True, validation_set=0.33, auto_normalize=True, epoch=200,batch_size=68)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1  Batches:  1  Loss:  0.4089989960193634\n",
            "Epoch:  1  Batches:  2  Loss:  0.4203018844127655\n",
            "Epoch:  1  Batches:  3  Loss:  0.39718592166900635\n",
            "Epoch:  1  Batches:  4  Loss:  0.41700658202171326\n",
            "Epoch:  1  Batches:  5  Loss:  0.41289544105529785\n",
            "Epoch:  1  Batches:  6  Loss:  0.41768401861190796\n",
            "Epoch:  1  Batches:  7  Loss:  0.42315372824668884\n",
            "Epoch:  1  Batches:  8  Loss:  0.32236647605895996\n",
            "Epoch:  1  Batches:  9  Loss:  0.40296828746795654\n",
            "Epoch:  1  Batches:  10  Loss:  0.3424319624900818\n",
            "Epoch:  2  Batches:  11  Loss:  0.3782684803009033\n",
            "Epoch:  2  Batches:  12  Loss:  0.36535248160362244\n",
            "Epoch:  2  Batches:  13  Loss:  0.35532382130622864\n",
            "Epoch:  2  Batches:  14  Loss:  0.37336084246635437\n",
            "Epoch:  2  Batches:  15  Loss:  0.36508145928382874\n",
            "Epoch:  2  Batches:  16  Loss:  0.3408660292625427\n",
            "Epoch:  2  Batches:  17  Loss:  0.33446812629699707\n",
            "Epoch:  2  Batches:  18  Loss:  0.3316862881183624\n",
            "Epoch:  2  Batches:  19  Loss:  0.3182566463947296\n",
            "Epoch:  2  Batches:  20  Loss:  0.335420697927475\n",
            "Epoch:  3  Batches:  21  Loss:  0.35291996598243713\n",
            "Epoch:  3  Batches:  22  Loss:  0.3189968764781952\n",
            "Epoch:  3  Batches:  23  Loss:  0.3020966053009033\n",
            "Epoch:  3  Batches:  24  Loss:  0.3325807750225067\n",
            "Epoch:  3  Batches:  25  Loss:  0.30188286304473877\n",
            "Epoch:  3  Batches:  26  Loss:  0.3248664438724518\n",
            "Epoch:  3  Batches:  27  Loss:  0.28027892112731934\n",
            "Epoch:  3  Batches:  28  Loss:  0.2676757872104645\n",
            "Epoch:  3  Batches:  29  Loss:  0.30300140380859375\n",
            "Epoch:  3  Batches:  30  Loss:  0.2610119879245758\n",
            "Epoch:  4  Batches:  31  Loss:  0.2557279169559479\n",
            "Epoch:  4  Batches:  32  Loss:  0.2900276780128479\n",
            "Epoch:  4  Batches:  33  Loss:  0.27548748254776\n",
            "Epoch:  4  Batches:  34  Loss:  0.29094308614730835\n",
            "Epoch:  4  Batches:  35  Loss:  0.25506582856178284\n",
            "Epoch:  4  Batches:  36  Loss:  0.25708848237991333\n",
            "Epoch:  4  Batches:  37  Loss:  0.22880418598651886\n",
            "Epoch:  4  Batches:  38  Loss:  0.2469233274459839\n",
            "Epoch:  4  Batches:  39  Loss:  0.22798526287078857\n",
            "Epoch:  4  Batches:  40  Loss:  0.24650314450263977\n",
            "Epoch:  5  Batches:  41  Loss:  0.23576068878173828\n",
            "Epoch:  5  Batches:  42  Loss:  0.2436940222978592\n",
            "Epoch:  5  Batches:  43  Loss:  0.2096143662929535\n",
            "Epoch:  5  Batches:  44  Loss:  0.19490505754947662\n",
            "Epoch:  5  Batches:  45  Loss:  0.20704743266105652\n",
            "Epoch:  5  Batches:  46  Loss:  0.1843414157629013\n",
            "Epoch:  5  Batches:  47  Loss:  0.208095520734787\n",
            "Epoch:  5  Batches:  48  Loss:  0.19375115633010864\n",
            "Epoch:  5  Batches:  49  Loss:  0.17525504529476166\n",
            "Epoch:  5  Batches:  50  Loss:  0.20697073638439178\n",
            "Epoch:  6  Batches:  51  Loss:  0.21699823439121246\n",
            "Epoch:  6  Batches:  52  Loss:  0.15663622319698334\n",
            "Epoch:  6  Batches:  53  Loss:  0.15170268714427948\n",
            "Epoch:  6  Batches:  54  Loss:  0.13521996140480042\n",
            "Epoch:  6  Batches:  55  Loss:  0.16066491603851318\n",
            "Epoch:  6  Batches:  56  Loss:  0.1740918904542923\n",
            "Epoch:  6  Batches:  57  Loss:  0.14088882505893707\n",
            "Epoch:  6  Batches:  58  Loss:  0.1650388389825821\n",
            "Epoch:  6  Batches:  59  Loss:  0.17618811130523682\n",
            "Epoch:  6  Batches:  60  Loss:  0.11179035156965256\n",
            "Epoch:  7  Batches:  61  Loss:  0.1444900929927826\n",
            "Epoch:  7  Batches:  62  Loss:  0.14476731419563293\n",
            "Epoch:  7  Batches:  63  Loss:  0.1459064930677414\n",
            "Epoch:  7  Batches:  64  Loss:  0.13980692625045776\n",
            "Epoch:  7  Batches:  65  Loss:  0.10599379241466522\n",
            "Epoch:  7  Batches:  66  Loss:  0.15883420407772064\n",
            "Epoch:  7  Batches:  67  Loss:  0.13413512706756592\n",
            "Epoch:  7  Batches:  68  Loss:  0.12305903434753418\n",
            "Epoch:  7  Batches:  69  Loss:  0.07888597995042801\n",
            "Epoch:  7  Batches:  70  Loss:  0.19056931138038635\n",
            "Epoch:  8  Batches:  71  Loss:  0.12695004045963287\n",
            "Epoch:  8  Batches:  72  Loss:  0.1259242296218872\n",
            "Epoch:  8  Batches:  73  Loss:  0.1500474512577057\n",
            "Epoch:  8  Batches:  74  Loss:  0.14395709335803986\n",
            "Epoch:  8  Batches:  75  Loss:  0.1518411785364151\n",
            "Epoch:  8  Batches:  76  Loss:  0.1375294178724289\n",
            "Epoch:  8  Batches:  77  Loss:  0.14062818884849548\n",
            "Epoch:  8  Batches:  78  Loss:  0.0925690308213234\n",
            "Epoch:  8  Batches:  79  Loss:  0.1381635069847107\n",
            "Epoch:  8  Batches:  80  Loss:  0.07252523303031921\n",
            "Epoch:  9  Batches:  81  Loss:  0.1425255686044693\n",
            "Epoch:  9  Batches:  82  Loss:  0.11286608129739761\n",
            "Epoch:  9  Batches:  83  Loss:  0.14002029597759247\n",
            "Epoch:  9  Batches:  84  Loss:  0.12044993788003922\n",
            "Epoch:  9  Batches:  85  Loss:  0.10533452779054642\n",
            "Epoch:  9  Batches:  86  Loss:  0.09803589433431625\n",
            "Epoch:  9  Batches:  87  Loss:  0.13304312527179718\n",
            "Epoch:  9  Batches:  88  Loss:  0.11698600649833679\n",
            "Epoch:  9  Batches:  89  Loss:  0.12469957768917084\n",
            "Epoch:  9  Batches:  90  Loss:  0.10206452012062073\n",
            "Epoch:  10  Batches:  91  Loss:  0.12788492441177368\n",
            "Epoch:  10  Batches:  92  Loss:  0.10574392974376678\n",
            "Epoch:  10  Batches:  93  Loss:  0.0638243705034256\n",
            "Epoch:  10  Batches:  94  Loss:  0.1057438850402832\n",
            "Epoch:  10  Batches:  95  Loss:  0.12211807817220688\n",
            "Epoch:  10  Batches:  96  Loss:  0.15033935010433197\n",
            "Epoch:  10  Batches:  97  Loss:  0.09456383436918259\n",
            "Epoch:  10  Batches:  98  Loss:  0.12047065049409866\n",
            "Epoch:  10  Batches:  99  Loss:  0.1505052000284195\n",
            "Epoch:  10  Batches:  100  Loss:  0.08126618713140488\n",
            "Epoch:  11  Batches:  101  Loss:  0.10090228170156479\n",
            "Epoch:  11  Batches:  102  Loss:  0.1408289223909378\n",
            "Epoch:  11  Batches:  103  Loss:  0.1033322662115097\n",
            "Epoch:  11  Batches:  104  Loss:  0.0949920192360878\n",
            "Epoch:  11  Batches:  105  Loss:  0.09517794847488403\n",
            "Epoch:  11  Batches:  106  Loss:  0.13684654235839844\n",
            "Epoch:  11  Batches:  107  Loss:  0.08897240459918976\n",
            "Epoch:  11  Batches:  108  Loss:  0.09781694412231445\n",
            "Epoch:  11  Batches:  109  Loss:  0.10237190872430801\n",
            "Epoch:  11  Batches:  110  Loss:  0.07458678632974625\n",
            "Epoch:  12  Batches:  111  Loss:  0.10384756326675415\n",
            "Epoch:  12  Batches:  112  Loss:  0.1346110701560974\n",
            "Epoch:  12  Batches:  113  Loss:  0.1274706870317459\n",
            "Epoch:  12  Batches:  114  Loss:  0.08190643787384033\n",
            "Epoch:  12  Batches:  115  Loss:  0.10472740978002548\n",
            "Epoch:  12  Batches:  116  Loss:  0.0913446694612503\n",
            "Epoch:  12  Batches:  117  Loss:  0.06347695738077164\n",
            "Epoch:  12  Batches:  118  Loss:  0.0989626944065094\n",
            "Epoch:  12  Batches:  119  Loss:  0.06141979619860649\n",
            "Epoch:  12  Batches:  120  Loss:  0.08169091492891312\n",
            "Epoch:  13  Batches:  121  Loss:  0.10075806826353073\n",
            "Epoch:  13  Batches:  122  Loss:  0.09283026307821274\n",
            "Epoch:  13  Batches:  123  Loss:  0.08985410630702972\n",
            "Epoch:  13  Batches:  124  Loss:  0.07486193627119064\n",
            "Epoch:  13  Batches:  125  Loss:  0.08162535727024078\n",
            "Epoch:  13  Batches:  126  Loss:  0.10444103181362152\n",
            "Epoch:  13  Batches:  127  Loss:  0.09043370932340622\n",
            "Epoch:  13  Batches:  128  Loss:  0.08566240221261978\n",
            "Epoch:  13  Batches:  129  Loss:  0.07342907786369324\n",
            "Epoch:  13  Batches:  130  Loss:  0.06454832851886749\n",
            "Epoch:  14  Batches:  131  Loss:  0.08767843246459961\n",
            "Epoch:  14  Batches:  132  Loss:  0.10703296959400177\n",
            "Epoch:  14  Batches:  133  Loss:  0.0711527019739151\n",
            "Epoch:  14  Batches:  134  Loss:  0.09319893270730972\n",
            "Epoch:  14  Batches:  135  Loss:  0.06902782618999481\n",
            "Epoch:  14  Batches:  136  Loss:  0.0711025595664978\n",
            "Epoch:  14  Batches:  137  Loss:  0.08030829578638077\n",
            "Epoch:  14  Batches:  138  Loss:  0.08971543610095978\n",
            "Epoch:  14  Batches:  139  Loss:  0.04683820530772209\n",
            "Epoch:  14  Batches:  140  Loss:  0.06357624381780624\n",
            "Epoch:  15  Batches:  141  Loss:  0.08719976991415024\n",
            "Epoch:  15  Batches:  142  Loss:  0.06107761710882187\n",
            "Epoch:  15  Batches:  143  Loss:  0.07528684288263321\n",
            "Epoch:  15  Batches:  144  Loss:  0.042248912155628204\n",
            "Epoch:  15  Batches:  145  Loss:  0.049680083990097046\n",
            "Epoch:  15  Batches:  146  Loss:  0.08276491612195969\n",
            "Epoch:  15  Batches:  147  Loss:  0.06059664115309715\n",
            "Epoch:  15  Batches:  148  Loss:  0.08967358618974686\n",
            "Epoch:  15  Batches:  149  Loss:  0.07056278735399246\n",
            "Epoch:  15  Batches:  150  Loss:  0.09469030052423477\n",
            "Epoch:  16  Batches:  151  Loss:  0.06305469572544098\n",
            "Epoch:  16  Batches:  152  Loss:  0.05580592155456543\n",
            "Epoch:  16  Batches:  153  Loss:  0.0844382792711258\n",
            "Epoch:  16  Batches:  154  Loss:  0.05963990092277527\n",
            "Epoch:  16  Batches:  155  Loss:  0.0763465017080307\n",
            "Epoch:  16  Batches:  156  Loss:  0.07020791620016098\n",
            "Epoch:  16  Batches:  157  Loss:  0.07030150294303894\n",
            "Epoch:  16  Batches:  158  Loss:  0.05064766854047775\n",
            "Epoch:  16  Batches:  159  Loss:  0.0549037903547287\n",
            "Epoch:  16  Batches:  160  Loss:  0.0627874806523323\n",
            "Epoch:  17  Batches:  161  Loss:  0.06375990062952042\n",
            "Epoch:  17  Batches:  162  Loss:  0.058571767061948776\n",
            "Epoch:  17  Batches:  163  Loss:  0.04067772626876831\n",
            "Epoch:  17  Batches:  164  Loss:  0.04760380834341049\n",
            "Epoch:  17  Batches:  165  Loss:  0.06716656684875488\n",
            "Epoch:  17  Batches:  166  Loss:  0.06262433528900146\n",
            "Epoch:  17  Batches:  167  Loss:  0.06221563741564751\n",
            "Epoch:  17  Batches:  168  Loss:  0.051414605230093\n",
            "Epoch:  17  Batches:  169  Loss:  0.058688610792160034\n",
            "Epoch:  17  Batches:  170  Loss:  0.09412434697151184\n",
            "Epoch:  18  Batches:  171  Loss:  0.06250554323196411\n",
            "Epoch:  18  Batches:  172  Loss:  0.04707546532154083\n",
            "Epoch:  18  Batches:  173  Loss:  0.04454375058412552\n",
            "Epoch:  18  Batches:  174  Loss:  0.04675523191690445\n",
            "Epoch:  18  Batches:  175  Loss:  0.0441487580537796\n",
            "Epoch:  18  Batches:  176  Loss:  0.05429598316550255\n",
            "Epoch:  18  Batches:  177  Loss:  0.07254163920879364\n",
            "Epoch:  18  Batches:  178  Loss:  0.0477910041809082\n",
            "Epoch:  18  Batches:  179  Loss:  0.07148784399032593\n",
            "Epoch:  18  Batches:  180  Loss:  0.06717544049024582\n",
            "Epoch:  19  Batches:  181  Loss:  0.035319723188877106\n",
            "Epoch:  19  Batches:  182  Loss:  0.04530838131904602\n",
            "Epoch:  19  Batches:  183  Loss:  0.07192107290029526\n",
            "Epoch:  19  Batches:  184  Loss:  0.0497448593378067\n",
            "Epoch:  19  Batches:  185  Loss:  0.05931469425559044\n",
            "Epoch:  19  Batches:  186  Loss:  0.06476753205060959\n",
            "Epoch:  19  Batches:  187  Loss:  0.04352081939578056\n",
            "Epoch:  19  Batches:  188  Loss:  0.0653018206357956\n",
            "Epoch:  19  Batches:  189  Loss:  0.04264082387089729\n",
            "Epoch:  19  Batches:  190  Loss:  0.03323180601000786\n",
            "Epoch:  20  Batches:  191  Loss:  0.06922738254070282\n",
            "Epoch:  20  Batches:  192  Loss:  0.038159582763910294\n",
            "Epoch:  20  Batches:  193  Loss:  0.043872956186532974\n",
            "Epoch:  20  Batches:  194  Loss:  0.059722740203142166\n",
            "Epoch:  20  Batches:  195  Loss:  0.032682083547115326\n",
            "Epoch:  20  Batches:  196  Loss:  0.04220960661768913\n",
            "Epoch:  20  Batches:  197  Loss:  0.04601291939616203\n",
            "Epoch:  20  Batches:  198  Loss:  0.047043733298778534\n",
            "Epoch:  20  Batches:  199  Loss:  0.03478023037314415\n",
            "Epoch:  20  Batches:  200  Loss:  0.05715744197368622\n",
            "Epoch:  21  Batches:  201  Loss:  0.04383160173892975\n",
            "Epoch:  21  Batches:  202  Loss:  0.046575695276260376\n",
            "Epoch:  21  Batches:  203  Loss:  0.026411309838294983\n",
            "Epoch:  21  Batches:  204  Loss:  0.05868992581963539\n",
            "Epoch:  21  Batches:  205  Loss:  0.05147097632288933\n",
            "Epoch:  21  Batches:  206  Loss:  0.042036037892103195\n",
            "Epoch:  21  Batches:  207  Loss:  0.039083078503608704\n",
            "Epoch:  21  Batches:  208  Loss:  0.04220357909798622\n",
            "Epoch:  21  Batches:  209  Loss:  0.03677137941122055\n",
            "Epoch:  21  Batches:  210  Loss:  0.030604425817728043\n",
            "Epoch:  22  Batches:  211  Loss:  0.0368468202650547\n",
            "Epoch:  22  Batches:  212  Loss:  0.03940802440047264\n",
            "Epoch:  22  Batches:  213  Loss:  0.025643937289714813\n",
            "Epoch:  22  Batches:  214  Loss:  0.03426344320178032\n",
            "Epoch:  22  Batches:  215  Loss:  0.031364377588033676\n",
            "Epoch:  22  Batches:  216  Loss:  0.03751126304268837\n",
            "Epoch:  22  Batches:  217  Loss:  0.04210245609283447\n",
            "Epoch:  22  Batches:  218  Loss:  0.024571819230914116\n",
            "Epoch:  22  Batches:  219  Loss:  0.0498218908905983\n",
            "Epoch:  22  Batches:  220  Loss:  0.04326893016695976\n",
            "Epoch:  23  Batches:  221  Loss:  0.04444357752799988\n",
            "Epoch:  23  Batches:  222  Loss:  0.032155346125364304\n",
            "Epoch:  23  Batches:  223  Loss:  0.01466665230691433\n",
            "Epoch:  23  Batches:  224  Loss:  0.017276573926210403\n",
            "Epoch:  23  Batches:  225  Loss:  0.03761730343103409\n",
            "Epoch:  23  Batches:  226  Loss:  0.01804083026945591\n",
            "Epoch:  23  Batches:  227  Loss:  0.029319152235984802\n",
            "Epoch:  23  Batches:  228  Loss:  0.03762548416852951\n",
            "Epoch:  23  Batches:  229  Loss:  0.024154487997293472\n",
            "Epoch:  23  Batches:  230  Loss:  0.041551049798727036\n",
            "Epoch:  24  Batches:  231  Loss:  0.020779099315404892\n",
            "Epoch:  24  Batches:  232  Loss:  0.013026873581111431\n",
            "Epoch:  24  Batches:  233  Loss:  0.020159563049674034\n",
            "Epoch:  24  Batches:  234  Loss:  0.030593879520893097\n",
            "Epoch:  24  Batches:  235  Loss:  0.02882366068661213\n",
            "Epoch:  24  Batches:  236  Loss:  0.028530189767479897\n",
            "Epoch:  24  Batches:  237  Loss:  0.019475674256682396\n",
            "Epoch:  24  Batches:  238  Loss:  0.03127117455005646\n",
            "Epoch:  24  Batches:  239  Loss:  0.023977719247341156\n",
            "Epoch:  24  Batches:  240  Loss:  0.01924295537173748\n",
            "Epoch:  25  Batches:  241  Loss:  0.019114676862955093\n",
            "Epoch:  25  Batches:  242  Loss:  0.029544346034526825\n",
            "Epoch:  25  Batches:  243  Loss:  0.01938227377831936\n",
            "Epoch:  25  Batches:  244  Loss:  0.018790816888213158\n",
            "Epoch:  25  Batches:  245  Loss:  0.01706363447010517\n",
            "Epoch:  25  Batches:  246  Loss:  0.02139025181531906\n",
            "Epoch:  25  Batches:  247  Loss:  0.01582808792591095\n",
            "Epoch:  25  Batches:  248  Loss:  0.023070205003023148\n",
            "Epoch:  25  Batches:  249  Loss:  0.01410826575011015\n",
            "Epoch:  25  Batches:  250  Loss:  0.019420167431235313\n",
            "Epoch:  26  Batches:  251  Loss:  0.01666080765426159\n",
            "Epoch:  26  Batches:  252  Loss:  0.009195059537887573\n",
            "Epoch:  26  Batches:  253  Loss:  0.014178366400301456\n",
            "Epoch:  26  Batches:  254  Loss:  0.012936655431985855\n",
            "Epoch:  26  Batches:  255  Loss:  0.02482607774436474\n",
            "Epoch:  26  Batches:  256  Loss:  0.01363309659063816\n",
            "Epoch:  26  Batches:  257  Loss:  0.027108129113912582\n",
            "Epoch:  26  Batches:  258  Loss:  0.014499335549771786\n",
            "Epoch:  26  Batches:  259  Loss:  0.020227307453751564\n",
            "Epoch:  26  Batches:  260  Loss:  0.017060372978448868\n",
            "Epoch:  27  Batches:  261  Loss:  0.016883766278624535\n",
            "Epoch:  27  Batches:  262  Loss:  0.01271827518939972\n",
            "Epoch:  27  Batches:  263  Loss:  0.014282298274338245\n",
            "Epoch:  27  Batches:  264  Loss:  0.01351973693817854\n",
            "Epoch:  27  Batches:  265  Loss:  0.014859366230666637\n",
            "Epoch:  27  Batches:  266  Loss:  0.016150671988725662\n",
            "Epoch:  27  Batches:  267  Loss:  0.011350183747708797\n",
            "Epoch:  27  Batches:  268  Loss:  0.018797220662236214\n",
            "Epoch:  27  Batches:  269  Loss:  0.01231148187071085\n",
            "Epoch:  27  Batches:  270  Loss:  0.022684985771775246\n",
            "Epoch:  28  Batches:  271  Loss:  0.010297168046236038\n",
            "Epoch:  28  Batches:  272  Loss:  0.008016899228096008\n",
            "Epoch:  28  Batches:  273  Loss:  0.018524065613746643\n",
            "Epoch:  28  Batches:  274  Loss:  0.011590532958507538\n",
            "Epoch:  28  Batches:  275  Loss:  0.01573249138891697\n",
            "Epoch:  28  Batches:  276  Loss:  0.017709171399474144\n",
            "Epoch:  28  Batches:  277  Loss:  0.009286047890782356\n",
            "Epoch:  28  Batches:  278  Loss:  0.016639098525047302\n",
            "Epoch:  28  Batches:  279  Loss:  0.010724705643951893\n",
            "Epoch:  28  Batches:  280  Loss:  0.01499150786548853\n",
            "Epoch:  29  Batches:  281  Loss:  0.012293301522731781\n",
            "Epoch:  29  Batches:  282  Loss:  0.013600661419332027\n",
            "Epoch:  29  Batches:  283  Loss:  0.010618661530315876\n",
            "Epoch:  29  Batches:  284  Loss:  0.01312942523509264\n",
            "Epoch:  29  Batches:  285  Loss:  0.014670051634311676\n",
            "Epoch:  29  Batches:  286  Loss:  0.010700021870434284\n",
            "Epoch:  29  Batches:  287  Loss:  0.011845932342112064\n",
            "Epoch:  29  Batches:  288  Loss:  0.012832907028496265\n",
            "Epoch:  29  Batches:  289  Loss:  0.010648428462445736\n",
            "Epoch:  29  Batches:  290  Loss:  0.011843654327094555\n",
            "Epoch:  30  Batches:  291  Loss:  0.008308680728077888\n",
            "Epoch:  30  Batches:  292  Loss:  0.012175654992461205\n",
            "Epoch:  30  Batches:  293  Loss:  0.0129441162571311\n",
            "Epoch:  30  Batches:  294  Loss:  0.00924401544034481\n",
            "Epoch:  30  Batches:  295  Loss:  0.01116300281137228\n",
            "Epoch:  30  Batches:  296  Loss:  0.0112536009401083\n",
            "Epoch:  30  Batches:  297  Loss:  0.011095968075096607\n",
            "Epoch:  30  Batches:  298  Loss:  0.011197520419955254\n",
            "Epoch:  30  Batches:  299  Loss:  0.016259141266345978\n",
            "Epoch:  30  Batches:  300  Loss:  0.005781426094472408\n",
            "Epoch:  31  Batches:  301  Loss:  0.012498131953179836\n",
            "Epoch:  31  Batches:  302  Loss:  0.008030988276004791\n",
            "Epoch:  31  Batches:  303  Loss:  0.00898519717156887\n",
            "Epoch:  31  Batches:  304  Loss:  0.013801587745547295\n",
            "Epoch:  31  Batches:  305  Loss:  0.01021894346922636\n",
            "Epoch:  31  Batches:  306  Loss:  0.011136320419609547\n",
            "Epoch:  31  Batches:  307  Loss:  0.010046648792922497\n",
            "Epoch:  31  Batches:  308  Loss:  0.00998583436012268\n",
            "Epoch:  31  Batches:  309  Loss:  0.008577787317335606\n",
            "Epoch:  31  Batches:  310  Loss:  0.0082466509193182\n",
            "Epoch:  32  Batches:  311  Loss:  0.015697741881012917\n",
            "Epoch:  32  Batches:  312  Loss:  0.013103085570037365\n",
            "Epoch:  32  Batches:  313  Loss:  0.011435223743319511\n",
            "Epoch:  32  Batches:  314  Loss:  0.008106254041194916\n",
            "Epoch:  32  Batches:  315  Loss:  0.007981628179550171\n",
            "Epoch:  32  Batches:  316  Loss:  0.004706952720880508\n",
            "Epoch:  32  Batches:  317  Loss:  0.008614914491772652\n",
            "Epoch:  32  Batches:  318  Loss:  0.008273538202047348\n",
            "Epoch:  32  Batches:  319  Loss:  0.009394887834787369\n",
            "Epoch:  32  Batches:  320  Loss:  0.0061972183175385\n",
            "Epoch:  33  Batches:  321  Loss:  0.007845504209399223\n",
            "Epoch:  33  Batches:  322  Loss:  0.0077851009555161\n",
            "Epoch:  33  Batches:  323  Loss:  0.008794203400611877\n",
            "Epoch:  33  Batches:  324  Loss:  0.005525724496692419\n",
            "Epoch:  33  Batches:  325  Loss:  0.006564584095031023\n",
            "Epoch:  33  Batches:  326  Loss:  0.008001377806067467\n",
            "Epoch:  33  Batches:  327  Loss:  0.013791807927191257\n",
            "Epoch:  33  Batches:  328  Loss:  0.009433851577341557\n",
            "Epoch:  33  Batches:  329  Loss:  0.008333220146596432\n",
            "Epoch:  33  Batches:  330  Loss:  0.008474856615066528\n",
            "Epoch:  34  Batches:  331  Loss:  0.008263468742370605\n",
            "Epoch:  34  Batches:  332  Loss:  0.009950077161192894\n",
            "Epoch:  34  Batches:  333  Loss:  0.007907978259027004\n",
            "Epoch:  34  Batches:  334  Loss:  0.00584891764447093\n",
            "Epoch:  34  Batches:  335  Loss:  0.005211581476032734\n",
            "Epoch:  34  Batches:  336  Loss:  0.007386962417513132\n",
            "Epoch:  34  Batches:  337  Loss:  0.0073333969339728355\n",
            "Epoch:  34  Batches:  338  Loss:  0.008960453793406487\n",
            "Epoch:  34  Batches:  339  Loss:  0.0071503035724163055\n",
            "Epoch:  34  Batches:  340  Loss:  0.009404044598340988\n",
            "Epoch:  35  Batches:  341  Loss:  0.00642998144030571\n",
            "Epoch:  35  Batches:  342  Loss:  0.005758451763540506\n",
            "Epoch:  35  Batches:  343  Loss:  0.005196009762585163\n",
            "Epoch:  35  Batches:  344  Loss:  0.007435978390276432\n",
            "Epoch:  35  Batches:  345  Loss:  0.006227829027920961\n",
            "Epoch:  35  Batches:  346  Loss:  0.012948057614266872\n",
            "Epoch:  35  Batches:  347  Loss:  0.005752934142947197\n",
            "Epoch:  35  Batches:  348  Loss:  0.009710309095680714\n",
            "Epoch:  35  Batches:  349  Loss:  0.0057648164220154285\n",
            "Epoch:  35  Batches:  350  Loss:  0.006417182274162769\n",
            "Epoch:  36  Batches:  351  Loss:  0.0056708864867687225\n",
            "Epoch:  36  Batches:  352  Loss:  0.00607993733137846\n",
            "Epoch:  36  Batches:  353  Loss:  0.008218045346438885\n",
            "Epoch:  36  Batches:  354  Loss:  0.0056569986045360565\n",
            "Epoch:  36  Batches:  355  Loss:  0.00752646941691637\n",
            "Epoch:  36  Batches:  356  Loss:  0.007653351407498121\n",
            "Epoch:  36  Batches:  357  Loss:  0.005131902173161507\n",
            "Epoch:  36  Batches:  358  Loss:  0.007957518100738525\n",
            "Epoch:  36  Batches:  359  Loss:  0.005685293581336737\n",
            "Epoch:  36  Batches:  360  Loss:  0.007387996651232243\n",
            "Epoch:  37  Batches:  361  Loss:  0.00624860730022192\n",
            "Epoch:  37  Batches:  362  Loss:  0.007161008194088936\n",
            "Epoch:  37  Batches:  363  Loss:  0.0038269548676908016\n",
            "Epoch:  37  Batches:  364  Loss:  0.008319374173879623\n",
            "Epoch:  37  Batches:  365  Loss:  0.00737471180036664\n",
            "Epoch:  37  Batches:  366  Loss:  0.006706446409225464\n",
            "Epoch:  37  Batches:  367  Loss:  0.006563682109117508\n",
            "Epoch:  37  Batches:  368  Loss:  0.004342664498835802\n",
            "Epoch:  37  Batches:  369  Loss:  0.00481253070756793\n",
            "Epoch:  37  Batches:  370  Loss:  0.006576209329068661\n",
            "Epoch:  38  Batches:  371  Loss:  0.007619963027536869\n",
            "Epoch:  38  Batches:  372  Loss:  0.004688012879341841\n",
            "Epoch:  38  Batches:  373  Loss:  0.008637947030365467\n",
            "Epoch:  38  Batches:  374  Loss:  0.004531364422291517\n",
            "Epoch:  38  Batches:  375  Loss:  0.004457755014300346\n",
            "Epoch:  38  Batches:  376  Loss:  0.004957109689712524\n",
            "Epoch:  38  Batches:  377  Loss:  0.005738280713558197\n",
            "Epoch:  38  Batches:  378  Loss:  0.005037776194512844\n",
            "Epoch:  38  Batches:  379  Loss:  0.004981400910764933\n",
            "Epoch:  38  Batches:  380  Loss:  0.006912172306329012\n",
            "Epoch:  39  Batches:  381  Loss:  0.0043624453246593475\n",
            "Epoch:  39  Batches:  382  Loss:  0.005058242008090019\n",
            "Epoch:  39  Batches:  383  Loss:  0.006160474848002195\n",
            "Epoch:  39  Batches:  384  Loss:  0.006390153896063566\n",
            "Epoch:  39  Batches:  385  Loss:  0.004889946896582842\n",
            "Epoch:  39  Batches:  386  Loss:  0.003978255204856396\n",
            "Epoch:  39  Batches:  387  Loss:  0.0074634673073887825\n",
            "Epoch:  39  Batches:  388  Loss:  0.00596905592828989\n",
            "Epoch:  39  Batches:  389  Loss:  0.004231203813105822\n",
            "Epoch:  39  Batches:  390  Loss:  0.005926145706325769\n",
            "Epoch:  40  Batches:  391  Loss:  0.0030857480596750975\n",
            "Epoch:  40  Batches:  392  Loss:  0.00433753989636898\n",
            "Epoch:  40  Batches:  393  Loss:  0.004780405666679144\n",
            "Epoch:  40  Batches:  394  Loss:  0.006324668414890766\n",
            "Epoch:  40  Batches:  395  Loss:  0.0056182765401899815\n",
            "Epoch:  40  Batches:  396  Loss:  0.0046758465468883514\n",
            "Epoch:  40  Batches:  397  Loss:  0.005925762467086315\n",
            "Epoch:  40  Batches:  398  Loss:  0.006264569703489542\n",
            "Epoch:  40  Batches:  399  Loss:  0.005159296095371246\n",
            "Epoch:  40  Batches:  400  Loss:  0.004025847651064396\n",
            "Epoch:  41  Batches:  401  Loss:  0.005096548702567816\n",
            "Epoch:  41  Batches:  402  Loss:  0.0052078100852668285\n",
            "Epoch:  41  Batches:  403  Loss:  0.004632076248526573\n",
            "Epoch:  41  Batches:  404  Loss:  0.0038621341809630394\n",
            "Epoch:  41  Batches:  405  Loss:  0.006609455682337284\n",
            "Epoch:  41  Batches:  406  Loss:  0.004774954169988632\n",
            "Epoch:  41  Batches:  407  Loss:  0.004570390563458204\n",
            "Epoch:  41  Batches:  408  Loss:  0.0036810715682804585\n",
            "Epoch:  41  Batches:  409  Loss:  0.004137829411774874\n",
            "Epoch:  41  Batches:  410  Loss:  0.0037136056926101446\n",
            "Epoch:  42  Batches:  411  Loss:  0.004785576369613409\n",
            "Epoch:  42  Batches:  412  Loss:  0.005875788629055023\n",
            "Epoch:  42  Batches:  413  Loss:  0.004819849971681833\n",
            "Epoch:  42  Batches:  414  Loss:  0.006076708436012268\n",
            "Epoch:  42  Batches:  415  Loss:  0.003329532453790307\n",
            "Epoch:  42  Batches:  416  Loss:  0.0034018659498542547\n",
            "Epoch:  42  Batches:  417  Loss:  0.003170384792611003\n",
            "Epoch:  42  Batches:  418  Loss:  0.003182970220223069\n",
            "Epoch:  42  Batches:  419  Loss:  0.004647683352231979\n",
            "Epoch:  42  Batches:  420  Loss:  0.002902483567595482\n",
            "Epoch:  43  Batches:  421  Loss:  0.0040021371096372604\n",
            "Epoch:  43  Batches:  422  Loss:  0.005765251815319061\n",
            "Epoch:  43  Batches:  423  Loss:  0.0029475654009729624\n",
            "Epoch:  43  Batches:  424  Loss:  0.003214953001588583\n",
            "Epoch:  43  Batches:  425  Loss:  0.0033277478069067\n",
            "Epoch:  43  Batches:  426  Loss:  0.0031143941450864077\n",
            "Epoch:  43  Batches:  427  Loss:  0.002847533207386732\n",
            "Epoch:  43  Batches:  428  Loss:  0.004362772684544325\n",
            "Epoch:  43  Batches:  429  Loss:  0.005548796616494656\n",
            "Epoch:  43  Batches:  430  Loss:  0.004178247880190611\n",
            "Epoch:  44  Batches:  431  Loss:  0.0037493272684514523\n",
            "Epoch:  44  Batches:  432  Loss:  0.0026541780680418015\n",
            "Epoch:  44  Batches:  433  Loss:  0.003119474044069648\n",
            "Epoch:  44  Batches:  434  Loss:  0.004393254406750202\n",
            "Epoch:  44  Batches:  435  Loss:  0.005759609863162041\n",
            "Epoch:  44  Batches:  436  Loss:  0.002700014039874077\n",
            "Epoch:  44  Batches:  437  Loss:  0.004814887885004282\n",
            "Epoch:  44  Batches:  438  Loss:  0.003379422938451171\n",
            "Epoch:  44  Batches:  439  Loss:  0.0024604189675301313\n",
            "Epoch:  44  Batches:  440  Loss:  0.0037267450243234634\n",
            "Epoch:  45  Batches:  441  Loss:  0.0023530006874352694\n",
            "Epoch:  45  Batches:  442  Loss:  0.003575361566618085\n",
            "Epoch:  45  Batches:  443  Loss:  0.003972261678427458\n",
            "Epoch:  45  Batches:  444  Loss:  0.0026218031998723745\n",
            "Epoch:  45  Batches:  445  Loss:  0.004721119534224272\n",
            "Epoch:  45  Batches:  446  Loss:  0.004123604390770197\n",
            "Epoch:  45  Batches:  447  Loss:  0.0017956268275156617\n",
            "Epoch:  45  Batches:  448  Loss:  0.0028055005241185427\n",
            "Epoch:  45  Batches:  449  Loss:  0.003952793776988983\n",
            "Epoch:  45  Batches:  450  Loss:  0.004043219611048698\n",
            "Epoch:  46  Batches:  451  Loss:  0.0036414756905287504\n",
            "Epoch:  46  Batches:  452  Loss:  0.002376845106482506\n",
            "Epoch:  46  Batches:  453  Loss:  0.002022746717557311\n",
            "Epoch:  46  Batches:  454  Loss:  0.0034878107253462076\n",
            "Epoch:  46  Batches:  455  Loss:  0.004059500526636839\n",
            "Epoch:  46  Batches:  456  Loss:  0.0040515693835914135\n",
            "Epoch:  46  Batches:  457  Loss:  0.0033054647501558065\n",
            "Epoch:  46  Batches:  458  Loss:  0.003230174072086811\n",
            "Epoch:  46  Batches:  459  Loss:  0.0023792877327650785\n",
            "Epoch:  46  Batches:  460  Loss:  0.003465817077085376\n",
            "Epoch:  47  Batches:  461  Loss:  0.0023455913178622723\n",
            "Epoch:  47  Batches:  462  Loss:  0.0023483915720134974\n",
            "Epoch:  47  Batches:  463  Loss:  0.0026303231716156006\n",
            "Epoch:  47  Batches:  464  Loss:  0.0028435317799448967\n",
            "Epoch:  47  Batches:  465  Loss:  0.0028333067893981934\n",
            "Epoch:  47  Batches:  466  Loss:  0.003324282355606556\n",
            "Epoch:  47  Batches:  467  Loss:  0.00237847538664937\n",
            "Epoch:  47  Batches:  468  Loss:  0.004639866296201944\n",
            "Epoch:  47  Batches:  469  Loss:  0.0024096849374473095\n",
            "Epoch:  47  Batches:  470  Loss:  0.0044213710352778435\n",
            "Epoch:  48  Batches:  471  Loss:  0.0028011579997837543\n",
            "Epoch:  48  Batches:  472  Loss:  0.0027455450035631657\n",
            "Epoch:  48  Batches:  473  Loss:  0.002057364908978343\n",
            "Epoch:  48  Batches:  474  Loss:  0.0031674434430897236\n",
            "Epoch:  48  Batches:  475  Loss:  0.0023871485609561205\n",
            "Epoch:  48  Batches:  476  Loss:  0.0038084322586655617\n",
            "Epoch:  48  Batches:  477  Loss:  0.0024320343509316444\n",
            "Epoch:  48  Batches:  478  Loss:  0.0032077187206596136\n",
            "Epoch:  48  Batches:  479  Loss:  0.0037800839636474848\n",
            "Epoch:  48  Batches:  480  Loss:  0.0019239820539951324\n",
            "Epoch:  49  Batches:  481  Loss:  0.0025552494917064905\n",
            "Epoch:  49  Batches:  482  Loss:  0.002946591004729271\n",
            "Epoch:  49  Batches:  483  Loss:  0.0018872981891036034\n",
            "Epoch:  49  Batches:  484  Loss:  0.001944993739016354\n",
            "Epoch:  49  Batches:  485  Loss:  0.003014644840732217\n",
            "Epoch:  49  Batches:  486  Loss:  0.002341113518923521\n",
            "Epoch:  49  Batches:  487  Loss:  0.0018965450581163168\n",
            "Epoch:  49  Batches:  488  Loss:  0.004420862998813391\n",
            "Epoch:  49  Batches:  489  Loss:  0.0026644105091691017\n",
            "Epoch:  49  Batches:  490  Loss:  0.0031878435984253883\n",
            "Epoch:  50  Batches:  491  Loss:  0.0031618925277143717\n",
            "Epoch:  50  Batches:  492  Loss:  0.0038583625573664904\n",
            "Epoch:  50  Batches:  493  Loss:  0.001878079492598772\n",
            "Epoch:  50  Batches:  494  Loss:  0.0026821556966751814\n",
            "Epoch:  50  Batches:  495  Loss:  0.0029545959550887346\n",
            "Epoch:  50  Batches:  496  Loss:  0.002491528633981943\n",
            "Epoch:  50  Batches:  497  Loss:  0.002722548320889473\n",
            "Epoch:  50  Batches:  498  Loss:  0.0018532323883846402\n",
            "Epoch:  50  Batches:  499  Loss:  0.0020625453907996416\n",
            "Epoch:  50  Batches:  500  Loss:  0.0017759071197360754\n",
            "Epoch:  51  Batches:  501  Loss:  0.0033418110106140375\n",
            "Epoch:  51  Batches:  502  Loss:  0.001965117175132036\n",
            "Epoch:  51  Batches:  503  Loss:  0.0016182896215468645\n",
            "Epoch:  51  Batches:  504  Loss:  0.0015919454162940383\n",
            "Epoch:  51  Batches:  505  Loss:  0.0030762420501559973\n",
            "Epoch:  51  Batches:  506  Loss:  0.0018171397969126701\n",
            "Epoch:  51  Batches:  507  Loss:  0.0020282191690057516\n",
            "Epoch:  51  Batches:  508  Loss:  0.004474620800465345\n",
            "Epoch:  51  Batches:  509  Loss:  0.0017700386233627796\n",
            "Epoch:  51  Batches:  510  Loss:  0.0027626254595816135\n",
            "Epoch:  52  Batches:  511  Loss:  0.0017527531599625945\n",
            "Epoch:  52  Batches:  512  Loss:  0.002528171520680189\n",
            "Epoch:  52  Batches:  513  Loss:  0.0021043475717306137\n",
            "Epoch:  52  Batches:  514  Loss:  0.003269673092290759\n",
            "Epoch:  52  Batches:  515  Loss:  0.0025802080053836107\n",
            "Epoch:  52  Batches:  516  Loss:  0.002055587014183402\n",
            "Epoch:  52  Batches:  517  Loss:  0.001715783029794693\n",
            "Epoch:  52  Batches:  518  Loss:  0.0018927298951894045\n",
            "Epoch:  52  Batches:  519  Loss:  0.0020503730047494173\n",
            "Epoch:  52  Batches:  520  Loss:  0.002850030781701207\n",
            "Epoch:  53  Batches:  521  Loss:  0.0022405951749533415\n",
            "Epoch:  53  Batches:  522  Loss:  0.001623219228349626\n",
            "Epoch:  53  Batches:  523  Loss:  0.003339865943416953\n",
            "Epoch:  53  Batches:  524  Loss:  0.0019098235061392188\n",
            "Epoch:  53  Batches:  525  Loss:  0.0025807125493884087\n",
            "Epoch:  53  Batches:  526  Loss:  0.001877853530459106\n",
            "Epoch:  53  Batches:  527  Loss:  0.002290812786668539\n",
            "Epoch:  53  Batches:  528  Loss:  0.0016229951288551092\n",
            "Epoch:  53  Batches:  529  Loss:  0.0023209047503769398\n",
            "Epoch:  53  Batches:  530  Loss:  0.0015830162446945906\n",
            "Epoch:  54  Batches:  531  Loss:  0.0015861932188272476\n",
            "Epoch:  54  Batches:  532  Loss:  0.0025795225519686937\n",
            "Epoch:  54  Batches:  533  Loss:  0.0018629891565069556\n",
            "Epoch:  54  Batches:  534  Loss:  0.0015596545999869704\n",
            "Epoch:  54  Batches:  535  Loss:  0.0023956671357154846\n",
            "Epoch:  54  Batches:  536  Loss:  0.0023655316326767206\n",
            "Epoch:  54  Batches:  537  Loss:  0.0022905690129846334\n",
            "Epoch:  54  Batches:  538  Loss:  0.0018470652867108583\n",
            "Epoch:  54  Batches:  539  Loss:  0.0015584524953737855\n",
            "Epoch:  54  Batches:  540  Loss:  0.0024547562934458256\n",
            "Epoch:  55  Batches:  541  Loss:  0.001422210829332471\n",
            "Epoch:  55  Batches:  542  Loss:  0.001977071166038513\n",
            "Epoch:  55  Batches:  543  Loss:  0.0022286195307970047\n",
            "Epoch:  55  Batches:  544  Loss:  0.003290086518973112\n",
            "Epoch:  55  Batches:  545  Loss:  0.002541686873883009\n",
            "Epoch:  55  Batches:  546  Loss:  0.0012383228167891502\n",
            "Epoch:  55  Batches:  547  Loss:  0.001208077883347869\n",
            "Epoch:  55  Batches:  548  Loss:  0.0028700493276119232\n",
            "Epoch:  55  Batches:  549  Loss:  0.0012280894443392754\n",
            "Epoch:  55  Batches:  550  Loss:  0.0017341398634016514\n",
            "Epoch:  56  Batches:  551  Loss:  0.0032176023814827204\n",
            "Epoch:  56  Batches:  552  Loss:  0.002059241756796837\n",
            "Epoch:  56  Batches:  553  Loss:  0.001240440527908504\n",
            "Epoch:  56  Batches:  554  Loss:  0.0022758664563298225\n",
            "Epoch:  56  Batches:  555  Loss:  0.0015902603045105934\n",
            "Epoch:  56  Batches:  556  Loss:  0.0014300268376246095\n",
            "Epoch:  56  Batches:  557  Loss:  0.001654425053857267\n",
            "Epoch:  56  Batches:  558  Loss:  0.0013044289080426097\n",
            "Epoch:  56  Batches:  559  Loss:  0.0023518968373537064\n",
            "Epoch:  56  Batches:  560  Loss:  0.0022251883056014776\n",
            "Epoch:  57  Batches:  561  Loss:  0.0013091386063024402\n",
            "Epoch:  57  Batches:  562  Loss:  0.002161783864721656\n",
            "Epoch:  57  Batches:  563  Loss:  0.002141712699085474\n",
            "Epoch:  57  Batches:  564  Loss:  0.0013308515772223473\n",
            "Epoch:  57  Batches:  565  Loss:  0.001510385423898697\n",
            "Epoch:  57  Batches:  566  Loss:  0.001530439592897892\n",
            "Epoch:  57  Batches:  567  Loss:  0.002022226108238101\n",
            "Epoch:  57  Batches:  568  Loss:  0.002222466515377164\n",
            "Epoch:  57  Batches:  569  Loss:  0.0023588419426232576\n",
            "Epoch:  57  Batches:  570  Loss:  0.0013379682786762714\n",
            "Epoch:  58  Batches:  571  Loss:  0.0012307234574109316\n",
            "Epoch:  58  Batches:  572  Loss:  0.002259211614727974\n",
            "Epoch:  58  Batches:  573  Loss:  0.001975305611267686\n",
            "Epoch:  58  Batches:  574  Loss:  0.0009371209889650345\n",
            "Epoch:  58  Batches:  575  Loss:  0.0023829257115721703\n",
            "Epoch:  58  Batches:  576  Loss:  0.001664681825786829\n",
            "Epoch:  58  Batches:  577  Loss:  0.0017109350301325321\n",
            "Epoch:  58  Batches:  578  Loss:  0.0017232609679922462\n",
            "Epoch:  58  Batches:  579  Loss:  0.0017967884195968509\n",
            "Epoch:  58  Batches:  580  Loss:  0.0016202415572479367\n",
            "Epoch:  59  Batches:  581  Loss:  0.001679125940427184\n",
            "Epoch:  59  Batches:  582  Loss:  0.0017801752546802163\n",
            "Epoch:  59  Batches:  583  Loss:  0.0014426528941839933\n",
            "Epoch:  59  Batches:  584  Loss:  0.0009515044512227178\n",
            "Epoch:  59  Batches:  585  Loss:  0.001972367987036705\n",
            "Epoch:  59  Batches:  586  Loss:  0.0016983816167339683\n",
            "Epoch:  59  Batches:  587  Loss:  0.002762292046099901\n",
            "Epoch:  59  Batches:  588  Loss:  0.001374283223412931\n",
            "Epoch:  59  Batches:  589  Loss:  0.002329133218154311\n",
            "Epoch:  59  Batches:  590  Loss:  0.001157543738372624\n",
            "Epoch:  60  Batches:  591  Loss:  0.0014948481693863869\n",
            "Epoch:  60  Batches:  592  Loss:  0.002078230492770672\n",
            "Epoch:  60  Batches:  593  Loss:  0.0015879253624007106\n",
            "Epoch:  60  Batches:  594  Loss:  0.0018592807464301586\n",
            "Epoch:  60  Batches:  595  Loss:  0.0011160597205162048\n",
            "Epoch:  60  Batches:  596  Loss:  0.002054614946246147\n",
            "Epoch:  60  Batches:  597  Loss:  0.00199003703892231\n",
            "Epoch:  60  Batches:  598  Loss:  0.0014640378067269921\n",
            "Epoch:  60  Batches:  599  Loss:  0.0017658185679465532\n",
            "Epoch:  60  Batches:  600  Loss:  0.0010081320069730282\n",
            "Epoch:  61  Batches:  601  Loss:  0.001332370680756867\n",
            "Epoch:  61  Batches:  602  Loss:  0.0010244213044643402\n",
            "Epoch:  61  Batches:  603  Loss:  0.0016750850481912494\n",
            "Epoch:  61  Batches:  604  Loss:  0.0015798062086105347\n",
            "Epoch:  61  Batches:  605  Loss:  0.0011030018795281649\n",
            "Epoch:  61  Batches:  606  Loss:  0.0018255896866321564\n",
            "Epoch:  61  Batches:  607  Loss:  0.0018727155402302742\n",
            "Epoch:  61  Batches:  608  Loss:  0.0028981578070670366\n",
            "Epoch:  61  Batches:  609  Loss:  0.001942801522091031\n",
            "Epoch:  61  Batches:  610  Loss:  0.0009842352010309696\n",
            "Epoch:  62  Batches:  611  Loss:  0.0011497100349515676\n",
            "Epoch:  62  Batches:  612  Loss:  0.001080364570952952\n",
            "Epoch:  62  Batches:  613  Loss:  0.0009584453073330224\n",
            "Epoch:  62  Batches:  614  Loss:  0.002517311368137598\n",
            "Epoch:  62  Batches:  615  Loss:  0.00127462949603796\n",
            "Epoch:  62  Batches:  616  Loss:  0.0014371521538123488\n",
            "Epoch:  62  Batches:  617  Loss:  0.0015446795150637627\n",
            "Epoch:  62  Batches:  618  Loss:  0.0018587206723168492\n",
            "Epoch:  62  Batches:  619  Loss:  0.002096843207255006\n",
            "Epoch:  62  Batches:  620  Loss:  0.0010386742651462555\n",
            "Epoch:  63  Batches:  621  Loss:  0.0012043350143358111\n",
            "Epoch:  63  Batches:  622  Loss:  0.0012569979298859835\n",
            "Epoch:  63  Batches:  623  Loss:  0.0025338383857160807\n",
            "Epoch:  63  Batches:  624  Loss:  0.001128794508986175\n",
            "Epoch:  63  Batches:  625  Loss:  0.0014631414087489247\n",
            "Epoch:  63  Batches:  626  Loss:  0.0013518024934455752\n",
            "Epoch:  63  Batches:  627  Loss:  0.0012419220292940736\n",
            "Epoch:  63  Batches:  628  Loss:  0.0009933552937582135\n",
            "Epoch:  63  Batches:  629  Loss:  0.0014673114055767655\n",
            "Epoch:  63  Batches:  630  Loss:  0.0017163120210170746\n",
            "Epoch:  64  Batches:  631  Loss:  0.0014124858425930142\n",
            "Epoch:  64  Batches:  632  Loss:  0.0011091232299804688\n",
            "Epoch:  64  Batches:  633  Loss:  0.002048627007752657\n",
            "Epoch:  64  Batches:  634  Loss:  0.0017155747627839446\n",
            "Epoch:  64  Batches:  635  Loss:  0.0010144573170691729\n",
            "Epoch:  64  Batches:  636  Loss:  0.0012952282559126616\n",
            "Epoch:  64  Batches:  637  Loss:  0.0015534177655354142\n",
            "Epoch:  64  Batches:  638  Loss:  0.0012974700657650828\n",
            "Epoch:  64  Batches:  639  Loss:  0.0013184500858187675\n",
            "Epoch:  64  Batches:  640  Loss:  0.0010719376150518656\n",
            "Epoch:  65  Batches:  641  Loss:  0.002666871063411236\n",
            "Epoch:  65  Batches:  642  Loss:  0.0010672879870980978\n",
            "Epoch:  65  Batches:  643  Loss:  0.0012412420473992825\n",
            "Epoch:  65  Batches:  644  Loss:  0.0010674187215045094\n",
            "Epoch:  65  Batches:  645  Loss:  0.001309792511165142\n",
            "Epoch:  65  Batches:  646  Loss:  0.0016476776218041778\n",
            "Epoch:  65  Batches:  647  Loss:  0.001492523355409503\n",
            "Epoch:  65  Batches:  648  Loss:  0.0012732609175145626\n",
            "Epoch:  65  Batches:  649  Loss:  0.001111855381168425\n",
            "Epoch:  65  Batches:  650  Loss:  0.0008778607589192688\n",
            "Epoch:  66  Batches:  651  Loss:  0.0012532913824543357\n",
            "Epoch:  66  Batches:  652  Loss:  0.0012047463096678257\n",
            "Epoch:  66  Batches:  653  Loss:  0.0017252499237656593\n",
            "Epoch:  66  Batches:  654  Loss:  0.001456388272345066\n",
            "Epoch:  66  Batches:  655  Loss:  0.0009641895885579288\n",
            "Epoch:  66  Batches:  656  Loss:  0.0015238551422953606\n",
            "Epoch:  66  Batches:  657  Loss:  0.001231162459589541\n",
            "Epoch:  66  Batches:  658  Loss:  0.0009556117001920938\n",
            "Epoch:  66  Batches:  659  Loss:  0.0018656227039173245\n",
            "Epoch:  66  Batches:  660  Loss:  0.0013359057484194636\n",
            "Epoch:  67  Batches:  661  Loss:  0.0016319906571879983\n",
            "Epoch:  67  Batches:  662  Loss:  0.0013316558906808496\n",
            "Epoch:  67  Batches:  663  Loss:  0.0012343290727585554\n",
            "Epoch:  67  Batches:  664  Loss:  0.001529638422653079\n",
            "Epoch:  67  Batches:  665  Loss:  0.0016992009477689862\n",
            "Epoch:  67  Batches:  666  Loss:  0.0008386417175643146\n",
            "Epoch:  67  Batches:  667  Loss:  0.0013918961631134152\n",
            "Epoch:  67  Batches:  668  Loss:  0.0009075775742530823\n",
            "Epoch:  67  Batches:  669  Loss:  0.0011618956923484802\n",
            "Epoch:  67  Batches:  670  Loss:  0.0012120322789996862\n",
            "Epoch:  68  Batches:  671  Loss:  0.0014338729670271277\n",
            "Epoch:  68  Batches:  672  Loss:  0.0012776900548487902\n",
            "Epoch:  68  Batches:  673  Loss:  0.0017445700941607356\n",
            "Epoch:  68  Batches:  674  Loss:  0.0010394618147984147\n",
            "Epoch:  68  Batches:  675  Loss:  0.0015501437010243535\n",
            "Epoch:  68  Batches:  676  Loss:  0.0013392023975029588\n",
            "Epoch:  68  Batches:  677  Loss:  0.0008713757270015776\n",
            "Epoch:  68  Batches:  678  Loss:  0.0013664202997460961\n",
            "Epoch:  68  Batches:  679  Loss:  0.0010167865548282862\n",
            "Epoch:  68  Batches:  680  Loss:  0.0008488292805850506\n",
            "Epoch:  69  Batches:  681  Loss:  0.0007884202059358358\n",
            "Epoch:  69  Batches:  682  Loss:  0.001728167524561286\n",
            "Epoch:  69  Batches:  683  Loss:  0.0011610338697209954\n",
            "Epoch:  69  Batches:  684  Loss:  0.001071240403689444\n",
            "Epoch:  69  Batches:  685  Loss:  0.000997862545773387\n",
            "Epoch:  69  Batches:  686  Loss:  0.0017761416966095567\n",
            "Epoch:  69  Batches:  687  Loss:  0.0012844930170103908\n",
            "Epoch:  69  Batches:  688  Loss:  0.0012554555432870984\n",
            "Epoch:  69  Batches:  689  Loss:  0.0013116112677380443\n",
            "Epoch:  69  Batches:  690  Loss:  0.000857781560625881\n",
            "Epoch:  70  Batches:  691  Loss:  0.0009597919997759163\n",
            "Epoch:  70  Batches:  692  Loss:  0.0015656054019927979\n",
            "Epoch:  70  Batches:  693  Loss:  0.000770010519772768\n",
            "Epoch:  70  Batches:  694  Loss:  0.001756894402205944\n",
            "Epoch:  70  Batches:  695  Loss:  0.0009875812102109194\n",
            "Epoch:  70  Batches:  696  Loss:  0.0010333065874874592\n",
            "Epoch:  70  Batches:  697  Loss:  0.0018698610365390778\n",
            "Epoch:  70  Batches:  698  Loss:  0.0013936004834249616\n",
            "Epoch:  70  Batches:  699  Loss:  0.001164546119980514\n",
            "Epoch:  70  Batches:  700  Loss:  0.0007923613302409649\n",
            "Epoch:  71  Batches:  701  Loss:  0.001688258838839829\n",
            "Epoch:  71  Batches:  702  Loss:  0.001464163651689887\n",
            "Epoch:  71  Batches:  703  Loss:  0.002034323988482356\n",
            "Epoch:  71  Batches:  704  Loss:  0.0009444556199014187\n",
            "Epoch:  71  Batches:  705  Loss:  0.0009340247488580644\n",
            "Epoch:  71  Batches:  706  Loss:  0.0008704587817192078\n",
            "Epoch:  71  Batches:  707  Loss:  0.0008644300978630781\n",
            "Epoch:  71  Batches:  708  Loss:  0.0012409797636792064\n",
            "Epoch:  71  Batches:  709  Loss:  0.0008629548246972263\n",
            "Epoch:  71  Batches:  710  Loss:  0.0008365807589143515\n",
            "Epoch:  72  Batches:  711  Loss:  0.0010866343509405851\n",
            "Epoch:  72  Batches:  712  Loss:  0.0015118650626391172\n",
            "Epoch:  72  Batches:  713  Loss:  0.0009834426455199718\n",
            "Epoch:  72  Batches:  714  Loss:  0.0009734765626490116\n",
            "Epoch:  72  Batches:  715  Loss:  0.002081115497276187\n",
            "Epoch:  72  Batches:  716  Loss:  0.0011777881300076842\n",
            "Epoch:  72  Batches:  717  Loss:  0.0011501845438033342\n",
            "Epoch:  72  Batches:  718  Loss:  0.0009458277490921319\n",
            "Epoch:  72  Batches:  719  Loss:  0.001107026357203722\n",
            "Epoch:  72  Batches:  720  Loss:  0.0010352946119382977\n",
            "Epoch:  73  Batches:  721  Loss:  0.0016683117719367146\n",
            "Epoch:  73  Batches:  722  Loss:  0.0013756632106378675\n",
            "Epoch:  73  Batches:  723  Loss:  0.0011937114177271724\n",
            "Epoch:  73  Batches:  724  Loss:  0.0009544202475808561\n",
            "Epoch:  73  Batches:  725  Loss:  0.0010377649450674653\n",
            "Epoch:  73  Batches:  726  Loss:  0.0011988537153229117\n",
            "Epoch:  73  Batches:  727  Loss:  0.0011206241324543953\n",
            "Epoch:  73  Batches:  728  Loss:  0.0008305921219289303\n",
            "Epoch:  73  Batches:  729  Loss:  0.0010997405042871833\n",
            "Epoch:  73  Batches:  730  Loss:  0.0011334270238876343\n",
            "Epoch:  74  Batches:  731  Loss:  0.0009806458838284016\n",
            "Epoch:  74  Batches:  732  Loss:  0.0009557898156344891\n",
            "Epoch:  74  Batches:  733  Loss:  0.0013687082100659609\n",
            "Epoch:  74  Batches:  734  Loss:  0.0010283393785357475\n",
            "Epoch:  74  Batches:  735  Loss:  0.0013177203945815563\n",
            "Epoch:  74  Batches:  736  Loss:  0.0008637220016680658\n",
            "Epoch:  74  Batches:  737  Loss:  0.0008062715060077608\n",
            "Epoch:  74  Batches:  738  Loss:  0.0007062626536935568\n",
            "Epoch:  74  Batches:  739  Loss:  0.002116807736456394\n",
            "Epoch:  74  Batches:  740  Loss:  0.0009730541496537626\n",
            "Epoch:  75  Batches:  741  Loss:  0.0008550477214157581\n",
            "Epoch:  75  Batches:  742  Loss:  0.0011087058810517192\n",
            "Epoch:  75  Batches:  743  Loss:  0.0014581069117411971\n",
            "Epoch:  75  Batches:  744  Loss:  0.0008396922494284809\n",
            "Epoch:  75  Batches:  745  Loss:  0.0008563895826227963\n",
            "Epoch:  75  Batches:  746  Loss:  0.0014246602077037096\n",
            "Epoch:  75  Batches:  747  Loss:  0.001278489362448454\n",
            "Epoch:  75  Batches:  748  Loss:  0.00117316038813442\n",
            "Epoch:  75  Batches:  749  Loss:  0.0007625404396094382\n",
            "Epoch:  75  Batches:  750  Loss:  0.0012021533912047744\n",
            "Epoch:  76  Batches:  751  Loss:  0.0010021396446973085\n",
            "Epoch:  76  Batches:  752  Loss:  0.0016394187696278095\n",
            "Epoch:  76  Batches:  753  Loss:  0.0008838408393785357\n",
            "Epoch:  76  Batches:  754  Loss:  0.001017584465444088\n",
            "Epoch:  76  Batches:  755  Loss:  0.0009341775439679623\n",
            "Epoch:  76  Batches:  756  Loss:  0.0009973678970709443\n",
            "Epoch:  76  Batches:  757  Loss:  0.0015617238823324442\n",
            "Epoch:  76  Batches:  758  Loss:  0.0008323875954374671\n",
            "Epoch:  76  Batches:  759  Loss:  0.0008820385555736721\n",
            "Epoch:  76  Batches:  760  Loss:  0.001039042486809194\n",
            "Epoch:  77  Batches:  761  Loss:  0.0011056279763579369\n",
            "Epoch:  77  Batches:  762  Loss:  0.0010637728264555335\n",
            "Epoch:  77  Batches:  763  Loss:  0.0009632305591367185\n",
            "Epoch:  77  Batches:  764  Loss:  0.0017395656323060393\n",
            "Epoch:  77  Batches:  765  Loss:  0.0007567955763079226\n",
            "Epoch:  77  Batches:  766  Loss:  0.0010295538231730461\n",
            "Epoch:  77  Batches:  767  Loss:  0.0011782207293435931\n",
            "Epoch:  77  Batches:  768  Loss:  0.0011371205328032374\n",
            "Epoch:  77  Batches:  769  Loss:  0.0010267420439049602\n",
            "Epoch:  77  Batches:  770  Loss:  0.0009084022021852434\n",
            "Epoch:  78  Batches:  771  Loss:  0.0009467856143601239\n",
            "Epoch:  78  Batches:  772  Loss:  0.0012716867495328188\n",
            "Epoch:  78  Batches:  773  Loss:  0.0009644586825743318\n",
            "Epoch:  78  Batches:  774  Loss:  0.001105141593143344\n",
            "Epoch:  78  Batches:  775  Loss:  0.0008014212362468243\n",
            "Epoch:  78  Batches:  776  Loss:  0.0011498021194711328\n",
            "Epoch:  78  Batches:  777  Loss:  0.0008683996275067329\n",
            "Epoch:  78  Batches:  778  Loss:  0.0017595766112208366\n",
            "Epoch:  78  Batches:  779  Loss:  0.001117523293942213\n",
            "Epoch:  78  Batches:  780  Loss:  0.0008592206868343055\n",
            "Epoch:  79  Batches:  781  Loss:  0.001009986735880375\n",
            "Epoch:  79  Batches:  782  Loss:  0.0009185407543554902\n",
            "Epoch:  79  Batches:  783  Loss:  0.0013171022292226553\n",
            "Epoch:  79  Batches:  784  Loss:  0.0013265099842101336\n",
            "Epoch:  79  Batches:  785  Loss:  0.0007032043649815023\n",
            "Epoch:  79  Batches:  786  Loss:  0.0009864717721939087\n",
            "Epoch:  79  Batches:  787  Loss:  0.0008286007796414196\n",
            "Epoch:  79  Batches:  788  Loss:  0.0008542862487956882\n",
            "Epoch:  79  Batches:  789  Loss:  0.0013219424290582538\n",
            "Epoch:  79  Batches:  790  Loss:  0.0008247182122431695\n",
            "Epoch:  80  Batches:  791  Loss:  0.0008967474568635225\n",
            "Epoch:  80  Batches:  792  Loss:  0.0008671303512528539\n",
            "Epoch:  80  Batches:  793  Loss:  0.0008736947202123702\n",
            "Epoch:  80  Batches:  794  Loss:  0.0008726882515475154\n",
            "Epoch:  80  Batches:  795  Loss:  0.0009468577918596566\n",
            "Epoch:  80  Batches:  796  Loss:  0.000967226573266089\n",
            "Epoch:  80  Batches:  797  Loss:  0.0010991704184561968\n",
            "Epoch:  80  Batches:  798  Loss:  0.0007753417012281716\n",
            "Epoch:  80  Batches:  799  Loss:  0.0008884937851689756\n",
            "Epoch:  80  Batches:  800  Loss:  0.0018767419969663024\n",
            "Epoch:  81  Batches:  801  Loss:  0.0009632166475057602\n",
            "Epoch:  81  Batches:  802  Loss:  0.000971797970123589\n",
            "Epoch:  81  Batches:  803  Loss:  0.0010462378850206733\n",
            "Epoch:  81  Batches:  804  Loss:  0.000776322849560529\n",
            "Epoch:  81  Batches:  805  Loss:  0.0007957238703966141\n",
            "Epoch:  81  Batches:  806  Loss:  0.0017663700273260474\n",
            "Epoch:  81  Batches:  807  Loss:  0.0007946995901875198\n",
            "Epoch:  81  Batches:  808  Loss:  0.0010471290443092585\n",
            "Epoch:  81  Batches:  809  Loss:  0.0008777789771556854\n",
            "Epoch:  81  Batches:  810  Loss:  0.0008652323158457875\n",
            "Epoch:  82  Batches:  811  Loss:  0.0010354092810302973\n",
            "Epoch:  82  Batches:  812  Loss:  0.0013358099386096\n",
            "Epoch:  82  Batches:  813  Loss:  0.0010012267157435417\n",
            "Epoch:  82  Batches:  814  Loss:  0.0009452014928683639\n",
            "Epoch:  82  Batches:  815  Loss:  0.0008121923310682178\n",
            "Epoch:  82  Batches:  816  Loss:  0.0012362671550363302\n",
            "Epoch:  82  Batches:  817  Loss:  0.0007347678183577955\n",
            "Epoch:  82  Batches:  818  Loss:  0.0006410059286281466\n",
            "Epoch:  82  Batches:  819  Loss:  0.000815342937130481\n",
            "Epoch:  82  Batches:  820  Loss:  0.0009186066454276443\n",
            "Epoch:  83  Batches:  821  Loss:  0.0006606627721339464\n",
            "Epoch:  83  Batches:  822  Loss:  0.0008920181426219642\n",
            "Epoch:  83  Batches:  823  Loss:  0.0010067685507237911\n",
            "Epoch:  83  Batches:  824  Loss:  0.0007102880626916885\n",
            "Epoch:  83  Batches:  825  Loss:  0.0006816485547460616\n",
            "Epoch:  83  Batches:  826  Loss:  0.0007431083358824253\n",
            "Epoch:  83  Batches:  827  Loss:  0.0008768101688474417\n",
            "Epoch:  83  Batches:  828  Loss:  0.0015446821926161647\n",
            "Epoch:  83  Batches:  829  Loss:  0.0012145338114351034\n",
            "Epoch:  83  Batches:  830  Loss:  0.0009654705063439906\n",
            "Epoch:  84  Batches:  831  Loss:  0.0007742728921584785\n",
            "Epoch:  84  Batches:  832  Loss:  0.0010203178972005844\n",
            "Epoch:  84  Batches:  833  Loss:  0.0007608132436871529\n",
            "Epoch:  84  Batches:  834  Loss:  0.0009249279974028468\n",
            "Epoch:  84  Batches:  835  Loss:  0.0008310427656397223\n",
            "Epoch:  84  Batches:  836  Loss:  0.0013831472024321556\n",
            "Epoch:  84  Batches:  837  Loss:  0.0011927617015317082\n",
            "Epoch:  84  Batches:  838  Loss:  0.0006751394830644131\n",
            "Epoch:  84  Batches:  839  Loss:  0.0010729952482506633\n",
            "Epoch:  84  Batches:  840  Loss:  0.0006541874026879668\n",
            "Epoch:  85  Batches:  841  Loss:  0.0009753434569574893\n",
            "Epoch:  85  Batches:  842  Loss:  0.0007669414626434445\n",
            "Epoch:  85  Batches:  843  Loss:  0.0008197572315111756\n",
            "Epoch:  85  Batches:  844  Loss:  0.0008521853596903384\n",
            "Epoch:  85  Batches:  845  Loss:  0.0011249283561483026\n",
            "Epoch:  85  Batches:  846  Loss:  0.0013699791161343455\n",
            "Epoch:  85  Batches:  847  Loss:  0.0007099825888872147\n",
            "Epoch:  85  Batches:  848  Loss:  0.0008123943698592484\n",
            "Epoch:  85  Batches:  849  Loss:  0.0009668415877968073\n",
            "Epoch:  85  Batches:  850  Loss:  0.0007636945811100304\n",
            "Epoch:  86  Batches:  851  Loss:  0.0008165761828422546\n",
            "Epoch:  86  Batches:  852  Loss:  0.000814240425825119\n",
            "Epoch:  86  Batches:  853  Loss:  0.0010305248433724046\n",
            "Epoch:  86  Batches:  854  Loss:  0.000814098515547812\n",
            "Epoch:  86  Batches:  855  Loss:  0.0009476090781390667\n",
            "Epoch:  86  Batches:  856  Loss:  0.0014578080736100674\n",
            "Epoch:  86  Batches:  857  Loss:  0.0006844694726169109\n",
            "Epoch:  86  Batches:  858  Loss:  0.0007245774031616747\n",
            "Epoch:  86  Batches:  859  Loss:  0.0009379102848470211\n",
            "Epoch:  86  Batches:  860  Loss:  0.0007079374045133591\n",
            "Epoch:  87  Batches:  861  Loss:  0.0007584271370433271\n",
            "Epoch:  87  Batches:  862  Loss:  0.0009965194622054696\n",
            "Epoch:  87  Batches:  863  Loss:  0.0007210878538899124\n",
            "Epoch:  87  Batches:  864  Loss:  0.0006394737865775824\n",
            "Epoch:  87  Batches:  865  Loss:  0.0010197468800470233\n",
            "Epoch:  87  Batches:  866  Loss:  0.0009518158622086048\n",
            "Epoch:  87  Batches:  867  Loss:  0.0006023751921020448\n",
            "Epoch:  87  Batches:  868  Loss:  0.0007284943130798638\n",
            "Epoch:  87  Batches:  869  Loss:  0.001367681659758091\n",
            "Epoch:  87  Batches:  870  Loss:  0.0013196589425206184\n",
            "Epoch:  88  Batches:  871  Loss:  0.0009510642266832292\n",
            "Epoch:  88  Batches:  872  Loss:  0.0011472689220681787\n",
            "Epoch:  88  Batches:  873  Loss:  0.0008868772420100868\n",
            "Epoch:  88  Batches:  874  Loss:  0.0006853968370705843\n",
            "Epoch:  88  Batches:  875  Loss:  0.0008265483193099499\n",
            "Epoch:  88  Batches:  876  Loss:  0.0011072290362790227\n",
            "Epoch:  88  Batches:  877  Loss:  0.0009780797408893704\n",
            "Epoch:  88  Batches:  878  Loss:  0.0007387783844023943\n",
            "Epoch:  88  Batches:  879  Loss:  0.0006382669089362025\n",
            "Epoch:  88  Batches:  880  Loss:  0.0008861235692165792\n",
            "Epoch:  89  Batches:  881  Loss:  0.0007680684793740511\n",
            "Epoch:  89  Batches:  882  Loss:  0.0006829546182416379\n",
            "Epoch:  89  Batches:  883  Loss:  0.0008911423501558602\n",
            "Epoch:  89  Batches:  884  Loss:  0.0006445393664762378\n",
            "Epoch:  89  Batches:  885  Loss:  0.0009196773171424866\n",
            "Epoch:  89  Batches:  886  Loss:  0.0009756593499332666\n",
            "Epoch:  89  Batches:  887  Loss:  0.000872091855853796\n",
            "Epoch:  89  Batches:  888  Loss:  0.0008830392616800964\n",
            "Epoch:  89  Batches:  889  Loss:  0.0009105823701247573\n",
            "Epoch:  89  Batches:  890  Loss:  0.001819241908378899\n",
            "Epoch:  90  Batches:  891  Loss:  0.0006208735867403448\n",
            "Epoch:  90  Batches:  892  Loss:  0.0008193090325221419\n",
            "Epoch:  90  Batches:  893  Loss:  0.0008828332647681236\n",
            "Epoch:  90  Batches:  894  Loss:  0.000864509551320225\n",
            "Epoch:  90  Batches:  895  Loss:  0.0008377430494874716\n",
            "Epoch:  90  Batches:  896  Loss:  0.0007379639428108931\n",
            "Epoch:  90  Batches:  897  Loss:  0.0011351760476827621\n",
            "Epoch:  90  Batches:  898  Loss:  0.0008335155434906483\n",
            "Epoch:  90  Batches:  899  Loss:  0.0010658549144864082\n",
            "Epoch:  90  Batches:  900  Loss:  0.001147434231825173\n",
            "Epoch:  91  Batches:  901  Loss:  0.0005822401726618409\n",
            "Epoch:  91  Batches:  902  Loss:  0.0009103927295655012\n",
            "Epoch:  91  Batches:  903  Loss:  0.0012129481183364987\n",
            "Epoch:  91  Batches:  904  Loss:  0.000839738582726568\n",
            "Epoch:  91  Batches:  905  Loss:  0.0007015568553470075\n",
            "Epoch:  91  Batches:  906  Loss:  0.0007957976777106524\n",
            "Epoch:  91  Batches:  907  Loss:  0.0008076796657405794\n",
            "Epoch:  91  Batches:  908  Loss:  0.0010599985253065825\n",
            "Epoch:  91  Batches:  909  Loss:  0.0007049695122987032\n",
            "Epoch:  91  Batches:  910  Loss:  0.0011593877570703626\n",
            "Epoch:  92  Batches:  911  Loss:  0.0007310701766982675\n",
            "Epoch:  92  Batches:  912  Loss:  0.0008580561843700707\n",
            "Epoch:  92  Batches:  913  Loss:  0.0010586469434201717\n",
            "Epoch:  92  Batches:  914  Loss:  0.0006472294917330146\n",
            "Epoch:  92  Batches:  915  Loss:  0.0008664914639666677\n",
            "Epoch:  92  Batches:  916  Loss:  0.000642522529233247\n",
            "Epoch:  92  Batches:  917  Loss:  0.0010006250813603401\n",
            "Epoch:  92  Batches:  918  Loss:  0.0011298557510599494\n",
            "Epoch:  92  Batches:  919  Loss:  0.0007573348702862859\n",
            "Epoch:  92  Batches:  920  Loss:  0.0007244080770760775\n",
            "Epoch:  93  Batches:  921  Loss:  0.000997222843579948\n",
            "Epoch:  93  Batches:  922  Loss:  0.0007384167402051389\n",
            "Epoch:  93  Batches:  923  Loss:  0.0005706945667043328\n",
            "Epoch:  93  Batches:  924  Loss:  0.0008360129431821406\n",
            "Epoch:  93  Batches:  925  Loss:  0.0005363001837395132\n",
            "Epoch:  93  Batches:  926  Loss:  0.0012709142174571753\n",
            "Epoch:  93  Batches:  927  Loss:  0.0007247788598760962\n",
            "Epoch:  93  Batches:  928  Loss:  0.0006820884882472456\n",
            "Epoch:  93  Batches:  929  Loss:  0.0009575041476637125\n",
            "Epoch:  93  Batches:  930  Loss:  0.001052178326062858\n",
            "Epoch:  94  Batches:  931  Loss:  0.0007393174455501139\n",
            "Epoch:  94  Batches:  932  Loss:  0.0007942148949950933\n",
            "Epoch:  94  Batches:  933  Loss:  0.0007466053939424455\n",
            "Epoch:  94  Batches:  934  Loss:  0.0007982216775417328\n",
            "Epoch:  94  Batches:  935  Loss:  0.0011006445856764913\n",
            "Epoch:  94  Batches:  936  Loss:  0.0007472050492651761\n",
            "Epoch:  94  Batches:  937  Loss:  0.0010492784203961492\n",
            "Epoch:  94  Batches:  938  Loss:  0.0008303139475174248\n",
            "Epoch:  94  Batches:  939  Loss:  0.0006625590031035244\n",
            "Epoch:  94  Batches:  940  Loss:  0.0008322485373355448\n",
            "Epoch:  95  Batches:  941  Loss:  0.0010317607084289193\n",
            "Epoch:  95  Batches:  942  Loss:  0.0010703420266509056\n",
            "Epoch:  95  Batches:  943  Loss:  0.0007697258843109012\n",
            "Epoch:  95  Batches:  944  Loss:  0.0007392789120785892\n",
            "Epoch:  95  Batches:  945  Loss:  0.0009363085264340043\n",
            "Epoch:  95  Batches:  946  Loss:  0.0007038585026748478\n",
            "Epoch:  95  Batches:  947  Loss:  0.0008208625949919224\n",
            "Epoch:  95  Batches:  948  Loss:  0.0011001692619174719\n",
            "Epoch:  95  Batches:  949  Loss:  0.0008102416759356856\n",
            "Epoch:  95  Batches:  950  Loss:  0.0007889824919402599\n",
            "Epoch:  96  Batches:  951  Loss:  0.0009914236143231392\n",
            "Epoch:  96  Batches:  952  Loss:  0.0009493572288192809\n",
            "Epoch:  96  Batches:  953  Loss:  0.0008830642909742892\n",
            "Epoch:  96  Batches:  954  Loss:  0.000753114465624094\n",
            "Epoch:  96  Batches:  955  Loss:  0.0009252081508748233\n",
            "Epoch:  96  Batches:  956  Loss:  0.0009976301807910204\n",
            "Epoch:  96  Batches:  957  Loss:  0.0008110360940918326\n",
            "Epoch:  96  Batches:  958  Loss:  0.0008177414420060813\n",
            "Epoch:  96  Batches:  959  Loss:  0.0007579951197840273\n",
            "Epoch:  96  Batches:  960  Loss:  0.0007805851055309176\n",
            "Epoch:  97  Batches:  961  Loss:  0.0006730412133038044\n",
            "Epoch:  97  Batches:  962  Loss:  0.0011502514826133847\n",
            "Epoch:  97  Batches:  963  Loss:  0.0005078106187283993\n",
            "Epoch:  97  Batches:  964  Loss:  0.000912030169274658\n",
            "Epoch:  97  Batches:  965  Loss:  0.0006826364551670849\n",
            "Epoch:  97  Batches:  966  Loss:  0.0007314351387321949\n",
            "Epoch:  97  Batches:  967  Loss:  0.0008410514565184712\n",
            "Epoch:  97  Batches:  968  Loss:  0.0011238270672038198\n",
            "Epoch:  97  Batches:  969  Loss:  0.0006838930421508849\n",
            "Epoch:  97  Batches:  970  Loss:  0.0008362597436644137\n",
            "Epoch:  98  Batches:  971  Loss:  0.0012137644225731492\n",
            "Epoch:  98  Batches:  972  Loss:  0.0006447110208682716\n",
            "Epoch:  98  Batches:  973  Loss:  0.0008676555007696152\n",
            "Epoch:  98  Batches:  974  Loss:  0.0006549526588059962\n",
            "Epoch:  98  Batches:  975  Loss:  0.0007902415818534791\n",
            "Epoch:  98  Batches:  976  Loss:  0.0008294683648273349\n",
            "Epoch:  98  Batches:  977  Loss:  0.0008839541114866734\n",
            "Epoch:  98  Batches:  978  Loss:  0.0005744824302382767\n",
            "Epoch:  98  Batches:  979  Loss:  0.0007651166524738073\n",
            "Epoch:  98  Batches:  980  Loss:  0.0006344508728943765\n",
            "Epoch:  99  Batches:  981  Loss:  0.0005760781932622194\n",
            "Epoch:  99  Batches:  982  Loss:  0.0006213379674591124\n",
            "Epoch:  99  Batches:  983  Loss:  0.0006032645469531417\n",
            "Epoch:  99  Batches:  984  Loss:  0.0010946678230538964\n",
            "Epoch:  99  Batches:  985  Loss:  0.0006449666107073426\n",
            "Epoch:  99  Batches:  986  Loss:  0.0006990936817601323\n",
            "Epoch:  99  Batches:  987  Loss:  0.0007739377324469388\n",
            "Epoch:  99  Batches:  988  Loss:  0.0009053099784068763\n",
            "Epoch:  99  Batches:  989  Loss:  0.0006359213730320334\n",
            "Epoch:  99  Batches:  990  Loss:  0.0009031826048158109\n",
            "Epoch:  100  Batches:  991  Loss:  0.0007188686286099255\n",
            "Epoch:  100  Batches:  992  Loss:  0.000744994671549648\n",
            "Epoch:  100  Batches:  993  Loss:  0.0005367136909626424\n",
            "Epoch:  100  Batches:  994  Loss:  0.0009920761222019792\n",
            "Epoch:  100  Batches:  995  Loss:  0.0007840935722924769\n",
            "Epoch:  100  Batches:  996  Loss:  0.0005782950902357697\n",
            "Epoch:  100  Batches:  997  Loss:  0.0007135640480555594\n",
            "Epoch:  100  Batches:  998  Loss:  0.0006070523522794247\n",
            "Epoch:  100  Batches:  999  Loss:  0.0009185429662466049\n",
            "Epoch:  100  Batches:  1000  Loss:  0.0007222197018563747\n",
            "Epoch:  101  Batches:  1001  Loss:  0.0006144173676148057\n",
            "Epoch:  101  Batches:  1002  Loss:  0.0007826770306564867\n",
            "Epoch:  101  Batches:  1003  Loss:  0.0007118524517863989\n",
            "Epoch:  101  Batches:  1004  Loss:  0.0005454616039060056\n",
            "Epoch:  101  Batches:  1005  Loss:  0.0008005022536963224\n",
            "Epoch:  101  Batches:  1006  Loss:  0.0005695850122720003\n",
            "Epoch:  101  Batches:  1007  Loss:  0.0010731429792940617\n",
            "Epoch:  101  Batches:  1008  Loss:  0.0006532728439196944\n",
            "Epoch:  101  Batches:  1009  Loss:  0.0009866459295153618\n",
            "Epoch:  101  Batches:  1010  Loss:  0.0007114575710147619\n",
            "Epoch:  102  Batches:  1011  Loss:  0.0006941307219676673\n",
            "Epoch:  102  Batches:  1012  Loss:  0.000722720695193857\n",
            "Epoch:  102  Batches:  1013  Loss:  0.00099294469691813\n",
            "Epoch:  102  Batches:  1014  Loss:  0.0006967854569666088\n",
            "Epoch:  102  Batches:  1015  Loss:  0.000682424521073699\n",
            "Epoch:  102  Batches:  1016  Loss:  0.0005614398396573961\n",
            "Epoch:  102  Batches:  1017  Loss:  0.0007241374114528298\n",
            "Epoch:  102  Batches:  1018  Loss:  0.0006953018601052463\n",
            "Epoch:  102  Batches:  1019  Loss:  0.000728997983969748\n",
            "Epoch:  102  Batches:  1020  Loss:  0.0006432061782106757\n",
            "Epoch:  103  Batches:  1021  Loss:  0.0005435390630736947\n",
            "Epoch:  103  Batches:  1022  Loss:  0.0006893530953675508\n",
            "Epoch:  103  Batches:  1023  Loss:  0.0006403534207493067\n",
            "Epoch:  103  Batches:  1024  Loss:  0.0006135852891020477\n",
            "Epoch:  103  Batches:  1025  Loss:  0.0007112951134331524\n",
            "Epoch:  103  Batches:  1026  Loss:  0.0005646360223181546\n",
            "Epoch:  103  Batches:  1027  Loss:  0.000668858818244189\n",
            "Epoch:  103  Batches:  1028  Loss:  0.0008602026500739157\n",
            "Epoch:  103  Batches:  1029  Loss:  0.0011190202785655856\n",
            "Epoch:  103  Batches:  1030  Loss:  0.00048350609722547233\n",
            "Epoch:  104  Batches:  1031  Loss:  0.0005685057258233428\n",
            "Epoch:  104  Batches:  1032  Loss:  0.00077295076334849\n",
            "Epoch:  104  Batches:  1033  Loss:  0.0005878728115931153\n",
            "Epoch:  104  Batches:  1034  Loss:  0.0005316729657351971\n",
            "Epoch:  104  Batches:  1035  Loss:  0.0005489514442160726\n",
            "Epoch:  104  Batches:  1036  Loss:  0.000752249441575259\n",
            "Epoch:  104  Batches:  1037  Loss:  0.0007096530753187835\n",
            "Epoch:  104  Batches:  1038  Loss:  0.0008765922393649817\n",
            "Epoch:  104  Batches:  1039  Loss:  0.001176930032670498\n",
            "Epoch:  104  Batches:  1040  Loss:  0.0005683063063770533\n",
            "Epoch:  105  Batches:  1041  Loss:  0.0006782186683267355\n",
            "Epoch:  105  Batches:  1042  Loss:  0.0006078712758608162\n",
            "Epoch:  105  Batches:  1043  Loss:  0.0008277286542579532\n",
            "Epoch:  105  Batches:  1044  Loss:  0.0006374366930685937\n",
            "Epoch:  105  Batches:  1045  Loss:  0.0006398884579539299\n",
            "Epoch:  105  Batches:  1046  Loss:  0.0008417140925303102\n",
            "Epoch:  105  Batches:  1047  Loss:  0.0006885408656671643\n",
            "Epoch:  105  Batches:  1048  Loss:  0.0008002667455002666\n",
            "Epoch:  105  Batches:  1049  Loss:  0.0005901561817154288\n",
            "Epoch:  105  Batches:  1050  Loss:  0.0006443837191909552\n",
            "Epoch:  106  Batches:  1051  Loss:  0.0004972855094820261\n",
            "Epoch:  106  Batches:  1052  Loss:  0.0005579948192462325\n",
            "Epoch:  106  Batches:  1053  Loss:  0.0007369864033535123\n",
            "Epoch:  106  Batches:  1054  Loss:  0.0010799228912219405\n",
            "Epoch:  106  Batches:  1055  Loss:  0.0006118536693975329\n",
            "Epoch:  106  Batches:  1056  Loss:  0.000603021529968828\n",
            "Epoch:  106  Batches:  1057  Loss:  0.0007737129926681519\n",
            "Epoch:  106  Batches:  1058  Loss:  0.0006555980653502047\n",
            "Epoch:  106  Batches:  1059  Loss:  0.0007839266327209771\n",
            "Epoch:  106  Batches:  1060  Loss:  0.0005046303849667311\n",
            "Epoch:  107  Batches:  1061  Loss:  0.0005228526424616575\n",
            "Epoch:  107  Batches:  1062  Loss:  0.0008697801386006176\n",
            "Epoch:  107  Batches:  1063  Loss:  0.0009167753159999847\n",
            "Epoch:  107  Batches:  1064  Loss:  0.0007271274225786328\n",
            "Epoch:  107  Batches:  1065  Loss:  0.0006623112712986767\n",
            "Epoch:  107  Batches:  1066  Loss:  0.0005828310386277735\n",
            "Epoch:  107  Batches:  1067  Loss:  0.00048613909166306257\n",
            "Epoch:  107  Batches:  1068  Loss:  0.0005861760000698268\n",
            "Epoch:  107  Batches:  1069  Loss:  0.0006293198093771935\n",
            "Epoch:  107  Batches:  1070  Loss:  0.0008965652668848634\n",
            "Epoch:  108  Batches:  1071  Loss:  0.0005532681243494153\n",
            "Epoch:  108  Batches:  1072  Loss:  0.0006670060101896524\n",
            "Epoch:  108  Batches:  1073  Loss:  0.0007776727434247732\n",
            "Epoch:  108  Batches:  1074  Loss:  0.0007285677711479366\n",
            "Epoch:  108  Batches:  1075  Loss:  0.0009236021433025599\n",
            "Epoch:  108  Batches:  1076  Loss:  0.0004639332473743707\n",
            "Epoch:  108  Batches:  1077  Loss:  0.0005951399216428399\n",
            "Epoch:  108  Batches:  1078  Loss:  0.0006274639163166285\n",
            "Epoch:  108  Batches:  1079  Loss:  0.0005459054373204708\n",
            "Epoch:  108  Batches:  1080  Loss:  0.0007980105583555996\n",
            "Epoch:  109  Batches:  1081  Loss:  0.0006625304813496768\n",
            "Epoch:  109  Batches:  1082  Loss:  0.0005407914868555963\n",
            "Epoch:  109  Batches:  1083  Loss:  0.0007375585264526308\n",
            "Epoch:  109  Batches:  1084  Loss:  0.0007904889644123614\n",
            "Epoch:  109  Batches:  1085  Loss:  0.0007026968523859978\n",
            "Epoch:  109  Batches:  1086  Loss:  0.000539565400686115\n",
            "Epoch:  109  Batches:  1087  Loss:  0.0005770471761934459\n",
            "Epoch:  109  Batches:  1088  Loss:  0.0008629250805824995\n",
            "Epoch:  109  Batches:  1089  Loss:  0.0005108713521622121\n",
            "Epoch:  109  Batches:  1090  Loss:  0.000517390959430486\n",
            "Epoch:  110  Batches:  1091  Loss:  0.0006360527477227151\n",
            "Epoch:  110  Batches:  1092  Loss:  0.0006652568117715418\n",
            "Epoch:  110  Batches:  1093  Loss:  0.0007096636691130698\n",
            "Epoch:  110  Batches:  1094  Loss:  0.0007944837561808527\n",
            "Epoch:  110  Batches:  1095  Loss:  0.0005951360799372196\n",
            "Epoch:  110  Batches:  1096  Loss:  0.0008163750171661377\n",
            "Epoch:  110  Batches:  1097  Loss:  0.000780966249294579\n",
            "Epoch:  110  Batches:  1098  Loss:  0.0004710917710326612\n",
            "Epoch:  110  Batches:  1099  Loss:  0.0005619779112748802\n",
            "Epoch:  110  Batches:  1100  Loss:  0.0004342116881161928\n",
            "Epoch:  111  Batches:  1101  Loss:  0.001002651290036738\n",
            "Epoch:  111  Batches:  1102  Loss:  0.0005268382374197245\n",
            "Epoch:  111  Batches:  1103  Loss:  0.0006081657484173775\n",
            "Epoch:  111  Batches:  1104  Loss:  0.0006086820503696799\n",
            "Epoch:  111  Batches:  1105  Loss:  0.00047383178025484085\n",
            "Epoch:  111  Batches:  1106  Loss:  0.0007301431614905596\n",
            "Epoch:  111  Batches:  1107  Loss:  0.00046423805179074407\n",
            "Epoch:  111  Batches:  1108  Loss:  0.0007249381742440164\n",
            "Epoch:  111  Batches:  1109  Loss:  0.0007264853338710964\n",
            "Epoch:  111  Batches:  1110  Loss:  0.0005143070593476295\n",
            "Epoch:  112  Batches:  1111  Loss:  0.0006309597520157695\n",
            "Epoch:  112  Batches:  1112  Loss:  0.0006532750558108091\n",
            "Epoch:  112  Batches:  1113  Loss:  0.0005795150646008551\n",
            "Epoch:  112  Batches:  1114  Loss:  0.0005904589197598398\n",
            "Epoch:  112  Batches:  1115  Loss:  0.0005489413742907345\n",
            "Epoch:  112  Batches:  1116  Loss:  0.0005342636140994728\n",
            "Epoch:  112  Batches:  1117  Loss:  0.0005201312014833093\n",
            "Epoch:  112  Batches:  1118  Loss:  0.0004401753831189126\n",
            "Epoch:  112  Batches:  1119  Loss:  0.0010468991240486503\n",
            "Epoch:  112  Batches:  1120  Loss:  0.000728428247384727\n",
            "Epoch:  113  Batches:  1121  Loss:  0.0005627042846754193\n",
            "Epoch:  113  Batches:  1122  Loss:  0.0005931256455369294\n",
            "Epoch:  113  Batches:  1123  Loss:  0.0006133564747869968\n",
            "Epoch:  113  Batches:  1124  Loss:  0.00043437443673610687\n",
            "Epoch:  113  Batches:  1125  Loss:  0.0006207754486240447\n",
            "Epoch:  113  Batches:  1126  Loss:  0.0009548223461024463\n",
            "Epoch:  113  Batches:  1127  Loss:  0.0004666397289838642\n",
            "Epoch:  113  Batches:  1128  Loss:  0.0005253263516351581\n",
            "Epoch:  113  Batches:  1129  Loss:  0.0005454970523715019\n",
            "Epoch:  113  Batches:  1130  Loss:  0.0007195179932750762\n",
            "Epoch:  114  Batches:  1131  Loss:  0.0005670692771673203\n",
            "Epoch:  114  Batches:  1132  Loss:  0.0005646309000439942\n",
            "Epoch:  114  Batches:  1133  Loss:  0.00052498938748613\n",
            "Epoch:  114  Batches:  1134  Loss:  0.0005435161292552948\n",
            "Epoch:  114  Batches:  1135  Loss:  0.0005514954100362957\n",
            "Epoch:  114  Batches:  1136  Loss:  0.0004998424556106329\n",
            "Epoch:  114  Batches:  1137  Loss:  0.0007921498618088663\n",
            "Epoch:  114  Batches:  1138  Loss:  0.0005305095692165196\n",
            "Epoch:  114  Batches:  1139  Loss:  0.0005319113843142986\n",
            "Epoch:  114  Batches:  1140  Loss:  0.0009010855574160814\n",
            "Epoch:  115  Batches:  1141  Loss:  0.0005253668059594929\n",
            "Epoch:  115  Batches:  1142  Loss:  0.0004869123222306371\n",
            "Epoch:  115  Batches:  1143  Loss:  0.0006423388258554041\n",
            "Epoch:  115  Batches:  1144  Loss:  0.0006279365043155849\n",
            "Epoch:  115  Batches:  1145  Loss:  0.0006136438460089266\n",
            "Epoch:  115  Batches:  1146  Loss:  0.00047793929115869105\n",
            "Epoch:  115  Batches:  1147  Loss:  0.000840366876218468\n",
            "Epoch:  115  Batches:  1148  Loss:  0.0007468716357834637\n",
            "Epoch:  115  Batches:  1149  Loss:  0.0006751194596290588\n",
            "Epoch:  115  Batches:  1150  Loss:  0.0005753191653639078\n",
            "Epoch:  116  Batches:  1151  Loss:  0.000713659857865423\n",
            "Epoch:  116  Batches:  1152  Loss:  0.0006452527595683932\n",
            "Epoch:  116  Batches:  1153  Loss:  0.0005407306016422808\n",
            "Epoch:  116  Batches:  1154  Loss:  0.000696377013809979\n",
            "Epoch:  116  Batches:  1155  Loss:  0.000621213112026453\n",
            "Epoch:  116  Batches:  1156  Loss:  0.0006287801661528647\n",
            "Epoch:  116  Batches:  1157  Loss:  0.0005400163936428726\n",
            "Epoch:  116  Batches:  1158  Loss:  0.00042873132042586803\n",
            "Epoch:  116  Batches:  1159  Loss:  0.0008302954956889153\n",
            "Epoch:  116  Batches:  1160  Loss:  0.0007835891447030008\n",
            "Epoch:  117  Batches:  1161  Loss:  0.0007611460168845952\n",
            "Epoch:  117  Batches:  1162  Loss:  0.0007886862149462104\n",
            "Epoch:  117  Batches:  1163  Loss:  0.0004581772955134511\n",
            "Epoch:  117  Batches:  1164  Loss:  0.0006642217049375176\n",
            "Epoch:  117  Batches:  1165  Loss:  0.0006789644248783588\n",
            "Epoch:  117  Batches:  1166  Loss:  0.0005719305481761694\n",
            "Epoch:  117  Batches:  1167  Loss:  0.0009591128909960389\n",
            "Epoch:  117  Batches:  1168  Loss:  0.0005624446785077453\n",
            "Epoch:  117  Batches:  1169  Loss:  0.0006759102107025683\n",
            "Epoch:  117  Batches:  1170  Loss:  0.0005720967892557383\n",
            "Epoch:  118  Batches:  1171  Loss:  0.000714931869879365\n",
            "Epoch:  118  Batches:  1172  Loss:  0.000560804212000221\n",
            "Epoch:  118  Batches:  1173  Loss:  0.000503222516272217\n",
            "Epoch:  118  Batches:  1174  Loss:  0.000585018889978528\n",
            "Epoch:  118  Batches:  1175  Loss:  0.000583948043640703\n",
            "Epoch:  118  Batches:  1176  Loss:  0.000961676356382668\n",
            "Epoch:  118  Batches:  1177  Loss:  0.0006337641389109194\n",
            "Epoch:  118  Batches:  1178  Loss:  0.0006010422366671264\n",
            "Epoch:  118  Batches:  1179  Loss:  0.0006046967464499176\n",
            "Epoch:  118  Batches:  1180  Loss:  0.0004438987234607339\n",
            "Epoch:  119  Batches:  1181  Loss:  0.0006710025481879711\n",
            "Epoch:  119  Batches:  1182  Loss:  0.00048022318514995277\n",
            "Epoch:  119  Batches:  1183  Loss:  0.00040847487980499864\n",
            "Epoch:  119  Batches:  1184  Loss:  0.0006524054333567619\n",
            "Epoch:  119  Batches:  1185  Loss:  0.0009187672985717654\n",
            "Epoch:  119  Batches:  1186  Loss:  0.0004650228365790099\n",
            "Epoch:  119  Batches:  1187  Loss:  0.0005119041888974607\n",
            "Epoch:  119  Batches:  1188  Loss:  0.0004610081668943167\n",
            "Epoch:  119  Batches:  1189  Loss:  0.0006821901770308614\n",
            "Epoch:  119  Batches:  1190  Loss:  0.0006501240422949195\n",
            "Epoch:  120  Batches:  1191  Loss:  0.0005798395141027868\n",
            "Epoch:  120  Batches:  1192  Loss:  0.0008260216563940048\n",
            "Epoch:  120  Batches:  1193  Loss:  0.0004754741385113448\n",
            "Epoch:  120  Batches:  1194  Loss:  0.0007166180294007063\n",
            "Epoch:  120  Batches:  1195  Loss:  0.000482385657960549\n",
            "Epoch:  120  Batches:  1196  Loss:  0.0004362424951978028\n",
            "Epoch:  120  Batches:  1197  Loss:  0.0005983396549709141\n",
            "Epoch:  120  Batches:  1198  Loss:  0.0003887901548296213\n",
            "Epoch:  120  Batches:  1199  Loss:  0.0007008902612142265\n",
            "Epoch:  120  Batches:  1200  Loss:  0.000603212509304285\n",
            "Epoch:  121  Batches:  1201  Loss:  0.0008524670265614986\n",
            "Epoch:  121  Batches:  1202  Loss:  0.00047263666056096554\n",
            "Epoch:  121  Batches:  1203  Loss:  0.0007138279033824801\n",
            "Epoch:  121  Batches:  1204  Loss:  0.000607175868935883\n",
            "Epoch:  121  Batches:  1205  Loss:  0.0006288946606218815\n",
            "Epoch:  121  Batches:  1206  Loss:  0.00040210181032307446\n",
            "Epoch:  121  Batches:  1207  Loss:  0.000544599024578929\n",
            "Epoch:  121  Batches:  1208  Loss:  0.0004982924438081682\n",
            "Epoch:  121  Batches:  1209  Loss:  0.00042499133269302547\n",
            "Epoch:  121  Batches:  1210  Loss:  0.00040232378523796797\n",
            "Epoch:  122  Batches:  1211  Loss:  0.0003867594641633332\n",
            "Epoch:  122  Batches:  1212  Loss:  0.0007153595797717571\n",
            "Epoch:  122  Batches:  1213  Loss:  0.0004059154016431421\n",
            "Epoch:  122  Batches:  1214  Loss:  0.000487585726659745\n",
            "Epoch:  122  Batches:  1215  Loss:  0.0006628687842749059\n",
            "Epoch:  122  Batches:  1216  Loss:  0.0006630537682212889\n",
            "Epoch:  122  Batches:  1217  Loss:  0.0005210231174714863\n",
            "Epoch:  122  Batches:  1218  Loss:  0.0005140108405612409\n",
            "Epoch:  122  Batches:  1219  Loss:  0.000568223767913878\n",
            "Epoch:  122  Batches:  1220  Loss:  0.0008447704021818936\n",
            "Epoch:  123  Batches:  1221  Loss:  0.00039794089389033616\n",
            "Epoch:  123  Batches:  1222  Loss:  0.00037931668339297175\n",
            "Epoch:  123  Batches:  1223  Loss:  0.0005522959982044995\n",
            "Epoch:  123  Batches:  1224  Loss:  0.0007695407839491963\n",
            "Epoch:  123  Batches:  1225  Loss:  0.0005125787574797869\n",
            "Epoch:  123  Batches:  1226  Loss:  0.0006062383763492107\n",
            "Epoch:  123  Batches:  1227  Loss:  0.0005214709090068936\n",
            "Epoch:  123  Batches:  1228  Loss:  0.00048511859495192766\n",
            "Epoch:  123  Batches:  1229  Loss:  0.0005238587618805468\n",
            "Epoch:  123  Batches:  1230  Loss:  0.0007481009815819561\n",
            "Epoch:  124  Batches:  1231  Loss:  0.0004280490393284708\n",
            "Epoch:  124  Batches:  1232  Loss:  0.00041369537939317524\n",
            "Epoch:  124  Batches:  1233  Loss:  0.0007258612895384431\n",
            "Epoch:  124  Batches:  1234  Loss:  0.0006040461012162268\n",
            "Epoch:  124  Batches:  1235  Loss:  0.0005842057871632278\n",
            "Epoch:  124  Batches:  1236  Loss:  0.0005477471859194338\n",
            "Epoch:  124  Batches:  1237  Loss:  0.0005503938300535083\n",
            "Epoch:  124  Batches:  1238  Loss:  0.0006754843052476645\n",
            "Epoch:  124  Batches:  1239  Loss:  0.000623259344138205\n",
            "Epoch:  124  Batches:  1240  Loss:  0.0005378759233281016\n",
            "Epoch:  125  Batches:  1241  Loss:  0.0006371572380885482\n",
            "Epoch:  125  Batches:  1242  Loss:  0.00036268963594920933\n",
            "Epoch:  125  Batches:  1243  Loss:  0.0004234351508785039\n",
            "Epoch:  125  Batches:  1244  Loss:  0.0005527828470803797\n",
            "Epoch:  125  Batches:  1245  Loss:  0.0004740877775475383\n",
            "Epoch:  125  Batches:  1246  Loss:  0.0004187152080703527\n",
            "Epoch:  125  Batches:  1247  Loss:  0.0007644542492926121\n",
            "Epoch:  125  Batches:  1248  Loss:  0.0006372132338583469\n",
            "Epoch:  125  Batches:  1249  Loss:  0.00045188170042820275\n",
            "Epoch:  125  Batches:  1250  Loss:  0.0006026376504451036\n",
            "Epoch:  126  Batches:  1251  Loss:  0.000511784222908318\n",
            "Epoch:  126  Batches:  1252  Loss:  0.00044498915667645633\n",
            "Epoch:  126  Batches:  1253  Loss:  0.0005044559948146343\n",
            "Epoch:  126  Batches:  1254  Loss:  0.0005769557319581509\n",
            "Epoch:  126  Batches:  1255  Loss:  0.0007938030757941306\n",
            "Epoch:  126  Batches:  1256  Loss:  0.0006303294212557375\n",
            "Epoch:  126  Batches:  1257  Loss:  0.00042955914977937937\n",
            "Epoch:  126  Batches:  1258  Loss:  0.00042967527406290174\n",
            "Epoch:  126  Batches:  1259  Loss:  0.0005819519283249974\n",
            "Epoch:  126  Batches:  1260  Loss:  0.0003671665326692164\n",
            "Epoch:  127  Batches:  1261  Loss:  0.0006361696869134903\n",
            "Epoch:  127  Batches:  1262  Loss:  0.0005418951041065156\n",
            "Epoch:  127  Batches:  1263  Loss:  0.00046316281077452004\n",
            "Epoch:  127  Batches:  1264  Loss:  0.0004514150496106595\n",
            "Epoch:  127  Batches:  1265  Loss:  0.0006043325993232429\n",
            "Epoch:  127  Batches:  1266  Loss:  0.0006121614133007824\n",
            "Epoch:  127  Batches:  1267  Loss:  0.0004980075173079967\n",
            "Epoch:  127  Batches:  1268  Loss:  0.0005277974414639175\n",
            "Epoch:  127  Batches:  1269  Loss:  0.0004295230028219521\n",
            "Epoch:  127  Batches:  1270  Loss:  0.00047044336679391563\n",
            "Epoch:  128  Batches:  1271  Loss:  0.0004832593840546906\n",
            "Epoch:  128  Batches:  1272  Loss:  0.0004390925751067698\n",
            "Epoch:  128  Batches:  1273  Loss:  0.0005497894017025828\n",
            "Epoch:  128  Batches:  1274  Loss:  0.0005770560237579048\n",
            "Epoch:  128  Batches:  1275  Loss:  0.00043788691982626915\n",
            "Epoch:  128  Batches:  1276  Loss:  0.0004759175644721836\n",
            "Epoch:  128  Batches:  1277  Loss:  0.0004960152436979115\n",
            "Epoch:  128  Batches:  1278  Loss:  0.000733359542209655\n",
            "Epoch:  128  Batches:  1279  Loss:  0.0005863315891474485\n",
            "Epoch:  128  Batches:  1280  Loss:  0.000702271587215364\n",
            "Epoch:  129  Batches:  1281  Loss:  0.0005522948922589421\n",
            "Epoch:  129  Batches:  1282  Loss:  0.0005148810450918972\n",
            "Epoch:  129  Batches:  1283  Loss:  0.0005159934516996145\n",
            "Epoch:  129  Batches:  1284  Loss:  0.0006207690457813442\n",
            "Epoch:  129  Batches:  1285  Loss:  0.0003497592406347394\n",
            "Epoch:  129  Batches:  1286  Loss:  0.0004483056836761534\n",
            "Epoch:  129  Batches:  1287  Loss:  0.0006719164084643126\n",
            "Epoch:  129  Batches:  1288  Loss:  0.0006985442014411092\n",
            "Epoch:  129  Batches:  1289  Loss:  0.0004097328637726605\n",
            "Epoch:  129  Batches:  1290  Loss:  0.0004828880191780627\n",
            "Epoch:  130  Batches:  1291  Loss:  0.0005173594690859318\n",
            "Epoch:  130  Batches:  1292  Loss:  0.0005151763907633722\n",
            "Epoch:  130  Batches:  1293  Loss:  0.0006770412437617779\n",
            "Epoch:  130  Batches:  1294  Loss:  0.0007012516143731773\n",
            "Epoch:  130  Batches:  1295  Loss:  0.0005432002362795174\n",
            "Epoch:  130  Batches:  1296  Loss:  0.0005387894925661385\n",
            "Epoch:  130  Batches:  1297  Loss:  0.0004262645961716771\n",
            "Epoch:  130  Batches:  1298  Loss:  0.00040667381836101413\n",
            "Epoch:  130  Batches:  1299  Loss:  0.0004989443114027381\n",
            "Epoch:  130  Batches:  1300  Loss:  0.0004883939400315285\n",
            "Epoch:  131  Batches:  1301  Loss:  0.0006508179358206689\n",
            "Epoch:  131  Batches:  1302  Loss:  0.00043412490049377084\n",
            "Epoch:  131  Batches:  1303  Loss:  0.0005886643775738776\n",
            "Epoch:  131  Batches:  1304  Loss:  0.0007047030958347023\n",
            "Epoch:  131  Batches:  1305  Loss:  0.00044117524521425366\n",
            "Epoch:  131  Batches:  1306  Loss:  0.0005431103054434061\n",
            "Epoch:  131  Batches:  1307  Loss:  0.0005178727442398667\n",
            "Epoch:  131  Batches:  1308  Loss:  0.00043444897164590657\n",
            "Epoch:  131  Batches:  1309  Loss:  0.0006160211050882936\n",
            "Epoch:  131  Batches:  1310  Loss:  0.000441835232777521\n",
            "Epoch:  132  Batches:  1311  Loss:  0.0006627299590036273\n",
            "Epoch:  132  Batches:  1312  Loss:  0.00040538448956795037\n",
            "Epoch:  132  Batches:  1313  Loss:  0.0006250282167457044\n",
            "Epoch:  132  Batches:  1314  Loss:  0.00038956006756052375\n",
            "Epoch:  132  Batches:  1315  Loss:  0.0005378979258239269\n",
            "Epoch:  132  Batches:  1316  Loss:  0.0006031172815710306\n",
            "Epoch:  132  Batches:  1317  Loss:  0.000372299924492836\n",
            "Epoch:  132  Batches:  1318  Loss:  0.0005699224420823157\n",
            "Epoch:  132  Batches:  1319  Loss:  0.00045526615576818585\n",
            "Epoch:  132  Batches:  1320  Loss:  0.00046986399684101343\n",
            "Epoch:  133  Batches:  1321  Loss:  0.0004964317777194083\n",
            "Epoch:  133  Batches:  1322  Loss:  0.0005966058233752847\n",
            "Epoch:  133  Batches:  1323  Loss:  0.0005726867238990963\n",
            "Epoch:  133  Batches:  1324  Loss:  0.0004831496044062078\n",
            "Epoch:  133  Batches:  1325  Loss:  0.0005171236698515713\n",
            "Epoch:  133  Batches:  1326  Loss:  0.0004351407987996936\n",
            "Epoch:  133  Batches:  1327  Loss:  0.0006116959848441184\n",
            "Epoch:  133  Batches:  1328  Loss:  0.00042789551662281156\n",
            "Epoch:  133  Batches:  1329  Loss:  0.00036108223139308393\n",
            "Epoch:  133  Batches:  1330  Loss:  0.0004164961283095181\n",
            "Epoch:  134  Batches:  1331  Loss:  0.0005356952315196395\n",
            "Epoch:  134  Batches:  1332  Loss:  0.0005383868119679391\n",
            "Epoch:  134  Batches:  1333  Loss:  0.00041336959111504257\n",
            "Epoch:  134  Batches:  1334  Loss:  0.00040156603790819645\n",
            "Epoch:  134  Batches:  1335  Loss:  0.00048340874491259456\n",
            "Epoch:  134  Batches:  1336  Loss:  0.00040006553172133863\n",
            "Epoch:  134  Batches:  1337  Loss:  0.0005168936913833022\n",
            "Epoch:  134  Batches:  1338  Loss:  0.0004846908268518746\n",
            "Epoch:  134  Batches:  1339  Loss:  0.0003423642192501575\n",
            "Epoch:  134  Batches:  1340  Loss:  0.0008266386575996876\n",
            "Epoch:  135  Batches:  1341  Loss:  0.0003662185335997492\n",
            "Epoch:  135  Batches:  1342  Loss:  0.000506238080561161\n",
            "Epoch:  135  Batches:  1343  Loss:  0.0005158247658982873\n",
            "Epoch:  135  Batches:  1344  Loss:  0.00047986849676817656\n",
            "Epoch:  135  Batches:  1345  Loss:  0.00034084258368238807\n",
            "Epoch:  135  Batches:  1346  Loss:  0.0005404126131907105\n",
            "Epoch:  135  Batches:  1347  Loss:  0.00093873031437397\n",
            "Epoch:  135  Batches:  1348  Loss:  0.00033059087581932545\n",
            "Epoch:  135  Batches:  1349  Loss:  0.00040008724317885935\n",
            "Epoch:  135  Batches:  1350  Loss:  0.0003699949593283236\n",
            "Epoch:  136  Batches:  1351  Loss:  0.0004002658824902028\n",
            "Epoch:  136  Batches:  1352  Loss:  0.0005904701538383961\n",
            "Epoch:  136  Batches:  1353  Loss:  0.0003632082662079483\n",
            "Epoch:  136  Batches:  1354  Loss:  0.0004670012276619673\n",
            "Epoch:  136  Batches:  1355  Loss:  0.0006266128621064126\n",
            "Epoch:  136  Batches:  1356  Loss:  0.00040263813571073115\n",
            "Epoch:  136  Batches:  1357  Loss:  0.0004904810921289027\n",
            "Epoch:  136  Batches:  1358  Loss:  0.0003909123770426959\n",
            "Epoch:  136  Batches:  1359  Loss:  0.00046925750211812556\n",
            "Epoch:  136  Batches:  1360  Loss:  0.00046253454638645053\n",
            "Epoch:  137  Batches:  1361  Loss:  0.00045605353079736233\n",
            "Epoch:  137  Batches:  1362  Loss:  0.0004454482113942504\n",
            "Epoch:  137  Batches:  1363  Loss:  0.0006420821882784367\n",
            "Epoch:  137  Batches:  1364  Loss:  0.0005961260758340359\n",
            "Epoch:  137  Batches:  1365  Loss:  0.00035593906068243086\n",
            "Epoch:  137  Batches:  1366  Loss:  0.00046186824329197407\n",
            "Epoch:  137  Batches:  1367  Loss:  0.00048257579328492284\n",
            "Epoch:  137  Batches:  1368  Loss:  0.000457166665000841\n",
            "Epoch:  137  Batches:  1369  Loss:  0.0004541020025499165\n",
            "Epoch:  137  Batches:  1370  Loss:  0.0005576937110163271\n",
            "Epoch:  138  Batches:  1371  Loss:  0.0005086461314931512\n",
            "Epoch:  138  Batches:  1372  Loss:  0.00031405602931044996\n",
            "Epoch:  138  Batches:  1373  Loss:  0.0005088791949674487\n",
            "Epoch:  138  Batches:  1374  Loss:  0.00047824333887547255\n",
            "Epoch:  138  Batches:  1375  Loss:  0.0004714239330496639\n",
            "Epoch:  138  Batches:  1376  Loss:  0.000629313406534493\n",
            "Epoch:  138  Batches:  1377  Loss:  0.00048348132986575365\n",
            "Epoch:  138  Batches:  1378  Loss:  0.0003701143723446876\n",
            "Epoch:  138  Batches:  1379  Loss:  0.0004080876533407718\n",
            "Epoch:  138  Batches:  1380  Loss:  0.0005613907123915851\n",
            "Epoch:  139  Batches:  1381  Loss:  0.00047595170326530933\n",
            "Epoch:  139  Batches:  1382  Loss:  0.00039187780930660665\n",
            "Epoch:  139  Batches:  1383  Loss:  0.0003935321292374283\n",
            "Epoch:  139  Batches:  1384  Loss:  0.00045103573938831687\n",
            "Epoch:  139  Batches:  1385  Loss:  0.0004998619551770389\n",
            "Epoch:  139  Batches:  1386  Loss:  0.000493000668939203\n",
            "Epoch:  139  Batches:  1387  Loss:  0.0006340826512314379\n",
            "Epoch:  139  Batches:  1388  Loss:  0.0004589927848428488\n",
            "Epoch:  139  Batches:  1389  Loss:  0.0004521764349192381\n",
            "Epoch:  139  Batches:  1390  Loss:  0.0004079396021552384\n",
            "Epoch:  140  Batches:  1391  Loss:  0.0004298009444028139\n",
            "Epoch:  140  Batches:  1392  Loss:  0.0005183524917811155\n",
            "Epoch:  140  Batches:  1393  Loss:  0.0004471027059480548\n",
            "Epoch:  140  Batches:  1394  Loss:  0.0004977790522389114\n",
            "Epoch:  140  Batches:  1395  Loss:  0.00040745388832874596\n",
            "Epoch:  140  Batches:  1396  Loss:  0.0003359247639309615\n",
            "Epoch:  140  Batches:  1397  Loss:  0.00041728068026714027\n",
            "Epoch:  140  Batches:  1398  Loss:  0.0005873661721125245\n",
            "Epoch:  140  Batches:  1399  Loss:  0.0005083510186523199\n",
            "Epoch:  140  Batches:  1400  Loss:  0.0003815704840235412\n",
            "Epoch:  141  Batches:  1401  Loss:  0.0006035704864189029\n",
            "Epoch:  141  Batches:  1402  Loss:  0.00041088578291237354\n",
            "Epoch:  141  Batches:  1403  Loss:  0.0005294966395013034\n",
            "Epoch:  141  Batches:  1404  Loss:  0.00043667550198733807\n",
            "Epoch:  141  Batches:  1405  Loss:  0.00042481787386350334\n",
            "Epoch:  141  Batches:  1406  Loss:  0.0004360937455203384\n",
            "Epoch:  141  Batches:  1407  Loss:  0.00034073152346536517\n",
            "Epoch:  141  Batches:  1408  Loss:  0.00047457613982260227\n",
            "Epoch:  141  Batches:  1409  Loss:  0.0006922971806488931\n",
            "Epoch:  141  Batches:  1410  Loss:  0.0003585249651223421\n",
            "Epoch:  142  Batches:  1411  Loss:  0.0004485771059989929\n",
            "Epoch:  142  Batches:  1412  Loss:  0.0005271709524095058\n",
            "Epoch:  142  Batches:  1413  Loss:  0.00047561334213241935\n",
            "Epoch:  142  Batches:  1414  Loss:  0.00037039656308479607\n",
            "Epoch:  142  Batches:  1415  Loss:  0.00046025856863707304\n",
            "Epoch:  142  Batches:  1416  Loss:  0.00042060602572746575\n",
            "Epoch:  142  Batches:  1417  Loss:  0.0006368780741468072\n",
            "Epoch:  142  Batches:  1418  Loss:  0.00046302482951432467\n",
            "Epoch:  142  Batches:  1419  Loss:  0.00037318599061109126\n",
            "Epoch:  142  Batches:  1420  Loss:  0.0004480022471398115\n",
            "Epoch:  143  Batches:  1421  Loss:  0.00043484847992658615\n",
            "Epoch:  143  Batches:  1422  Loss:  0.0006069447263143957\n",
            "Epoch:  143  Batches:  1423  Loss:  0.0004796830180566758\n",
            "Epoch:  143  Batches:  1424  Loss:  0.0004346359637565911\n",
            "Epoch:  143  Batches:  1425  Loss:  0.00046476375428028405\n",
            "Epoch:  143  Batches:  1426  Loss:  0.000445151497842744\n",
            "Epoch:  143  Batches:  1427  Loss:  0.00037546191015280783\n",
            "Epoch:  143  Batches:  1428  Loss:  0.0004815043939743191\n",
            "Epoch:  143  Batches:  1429  Loss:  0.0004100714286323637\n",
            "Epoch:  143  Batches:  1430  Loss:  0.0004235294181853533\n",
            "Epoch:  144  Batches:  1431  Loss:  0.0003241684753447771\n",
            "Epoch:  144  Batches:  1432  Loss:  0.0004725062462966889\n",
            "Epoch:  144  Batches:  1433  Loss:  0.0005604791222140193\n",
            "Epoch:  144  Batches:  1434  Loss:  0.00042235085857100785\n",
            "Epoch:  144  Batches:  1435  Loss:  0.00045305423554964364\n",
            "Epoch:  144  Batches:  1436  Loss:  0.0003041675081476569\n",
            "Epoch:  144  Batches:  1437  Loss:  0.0005379848880693316\n",
            "Epoch:  144  Batches:  1438  Loss:  0.0005951750790700316\n",
            "Epoch:  144  Batches:  1439  Loss:  0.00044118432560935616\n",
            "Epoch:  144  Batches:  1440  Loss:  0.00040182031807489693\n",
            "Epoch:  145  Batches:  1441  Loss:  0.00043903852929361165\n",
            "Epoch:  145  Batches:  1442  Loss:  0.00043439422734081745\n",
            "Epoch:  145  Batches:  1443  Loss:  0.00036686030216515064\n",
            "Epoch:  145  Batches:  1444  Loss:  0.00035253778332844377\n",
            "Epoch:  145  Batches:  1445  Loss:  0.00040981246274895966\n",
            "Epoch:  145  Batches:  1446  Loss:  0.000463005097117275\n",
            "Epoch:  145  Batches:  1447  Loss:  0.0003429139615036547\n",
            "Epoch:  145  Batches:  1448  Loss:  0.000399064680095762\n",
            "Epoch:  145  Batches:  1449  Loss:  0.0005617155693471432\n",
            "Epoch:  145  Batches:  1450  Loss:  0.0005160463624633849\n",
            "Epoch:  146  Batches:  1451  Loss:  0.0003768679453060031\n",
            "Epoch:  146  Batches:  1452  Loss:  0.0005969078629277647\n",
            "Epoch:  146  Batches:  1453  Loss:  0.00045105814933776855\n",
            "Epoch:  146  Batches:  1454  Loss:  0.0004386684740893543\n",
            "Epoch:  146  Batches:  1455  Loss:  0.00044831098057329655\n",
            "Epoch:  146  Batches:  1456  Loss:  0.00036616888246499\n",
            "Epoch:  146  Batches:  1457  Loss:  0.00035789248067885637\n",
            "Epoch:  146  Batches:  1458  Loss:  0.00044683177839033306\n",
            "Epoch:  146  Batches:  1459  Loss:  0.00042094881064258516\n",
            "Epoch:  146  Batches:  1460  Loss:  0.00040923795313574374\n",
            "Epoch:  147  Batches:  1461  Loss:  0.0003371753264218569\n",
            "Epoch:  147  Batches:  1462  Loss:  0.00047911019646562636\n",
            "Epoch:  147  Batches:  1463  Loss:  0.0004271416109986603\n",
            "Epoch:  147  Batches:  1464  Loss:  0.0003605507081374526\n",
            "Epoch:  147  Batches:  1465  Loss:  0.0003423112793825567\n",
            "Epoch:  147  Batches:  1466  Loss:  0.00042156019480898976\n",
            "Epoch:  147  Batches:  1467  Loss:  0.0005772904842160642\n",
            "Epoch:  147  Batches:  1468  Loss:  0.0003686891286633909\n",
            "Epoch:  147  Batches:  1469  Loss:  0.0005070692277513444\n",
            "Epoch:  147  Batches:  1470  Loss:  0.0004065818793606013\n",
            "Epoch:  148  Batches:  1471  Loss:  0.00043363971053622663\n",
            "Epoch:  148  Batches:  1472  Loss:  0.00037669885205104947\n",
            "Epoch:  148  Batches:  1473  Loss:  0.0003980639448855072\n",
            "Epoch:  148  Batches:  1474  Loss:  0.000692090077791363\n",
            "Epoch:  148  Batches:  1475  Loss:  0.0003492659016046673\n",
            "Epoch:  148  Batches:  1476  Loss:  0.00034811763907782733\n",
            "Epoch:  148  Batches:  1477  Loss:  0.0004933730233460665\n",
            "Epoch:  148  Batches:  1478  Loss:  0.0004027610120829195\n",
            "Epoch:  148  Batches:  1479  Loss:  0.0004202756972517818\n",
            "Epoch:  148  Batches:  1480  Loss:  0.0003091342805419117\n",
            "Epoch:  149  Batches:  1481  Loss:  0.00031260616378858685\n",
            "Epoch:  149  Batches:  1482  Loss:  0.0003155770536977798\n",
            "Epoch:  149  Batches:  1483  Loss:  0.0004076113982591778\n",
            "Epoch:  149  Batches:  1484  Loss:  0.0004742741584777832\n",
            "Epoch:  149  Batches:  1485  Loss:  0.0003378855180926621\n",
            "Epoch:  149  Batches:  1486  Loss:  0.00037155597237870097\n",
            "Epoch:  149  Batches:  1487  Loss:  0.000410759006626904\n",
            "Epoch:  149  Batches:  1488  Loss:  0.0004995127092115581\n",
            "Epoch:  149  Batches:  1489  Loss:  0.0005103113362565637\n",
            "Epoch:  149  Batches:  1490  Loss:  0.0006506376666948199\n",
            "Epoch:  150  Batches:  1491  Loss:  0.00027917372062802315\n",
            "Epoch:  150  Batches:  1492  Loss:  0.0003442292509134859\n",
            "Epoch:  150  Batches:  1493  Loss:  0.0003891355008818209\n",
            "Epoch:  150  Batches:  1494  Loss:  0.0006184292142279446\n",
            "Epoch:  150  Batches:  1495  Loss:  0.0004480738425627351\n",
            "Epoch:  150  Batches:  1496  Loss:  0.00047278194688260555\n",
            "Epoch:  150  Batches:  1497  Loss:  0.00036062495200894773\n",
            "Epoch:  150  Batches:  1498  Loss:  0.0005244725616648793\n",
            "Epoch:  150  Batches:  1499  Loss:  0.00046769939945079386\n",
            "Epoch:  150  Batches:  1500  Loss:  0.00030477726249955595\n",
            "Epoch:  151  Batches:  1501  Loss:  0.0003955952706746757\n",
            "Epoch:  151  Batches:  1502  Loss:  0.00033649071701802313\n",
            "Epoch:  151  Batches:  1503  Loss:  0.00044260354479774833\n",
            "Epoch:  151  Batches:  1504  Loss:  0.0003439570718910545\n",
            "Epoch:  151  Batches:  1505  Loss:  0.0004629655450116843\n",
            "Epoch:  151  Batches:  1506  Loss:  0.0005358222988434136\n",
            "Epoch:  151  Batches:  1507  Loss:  0.00034580216743052006\n",
            "Epoch:  151  Batches:  1508  Loss:  0.00040061859181150794\n",
            "Epoch:  151  Batches:  1509  Loss:  0.000391828129068017\n",
            "Epoch:  151  Batches:  1510  Loss:  0.0005107694887556136\n",
            "Epoch:  152  Batches:  1511  Loss:  0.0005925320438109338\n",
            "Epoch:  152  Batches:  1512  Loss:  0.00043537854799069464\n",
            "Epoch:  152  Batches:  1513  Loss:  0.0003598193288780749\n",
            "Epoch:  152  Batches:  1514  Loss:  0.0004318700230214745\n",
            "Epoch:  152  Batches:  1515  Loss:  0.00033623239141888916\n",
            "Epoch:  152  Batches:  1516  Loss:  0.0004522989038378\n",
            "Epoch:  152  Batches:  1517  Loss:  0.0002976667892653495\n",
            "Epoch:  152  Batches:  1518  Loss:  0.0003433206584304571\n",
            "Epoch:  152  Batches:  1519  Loss:  0.0003438283456489444\n",
            "Epoch:  152  Batches:  1520  Loss:  0.0004639241669792682\n",
            "Epoch:  153  Batches:  1521  Loss:  0.00030301115475595\n",
            "Epoch:  153  Batches:  1522  Loss:  0.00039506182656623423\n",
            "Epoch:  153  Batches:  1523  Loss:  0.0006236455519683659\n",
            "Epoch:  153  Batches:  1524  Loss:  0.0003845161118078977\n",
            "Epoch:  153  Batches:  1525  Loss:  0.00035383738577365875\n",
            "Epoch:  153  Batches:  1526  Loss:  0.0002699569158721715\n",
            "Epoch:  153  Batches:  1527  Loss:  0.00031642126850783825\n",
            "Epoch:  153  Batches:  1528  Loss:  0.0004051155992783606\n",
            "Epoch:  153  Batches:  1529  Loss:  0.0003152510616928339\n",
            "Epoch:  153  Batches:  1530  Loss:  0.0005345538374967873\n",
            "Epoch:  154  Batches:  1531  Loss:  0.0003690008306875825\n",
            "Epoch:  154  Batches:  1532  Loss:  0.0004218162503093481\n",
            "Epoch:  154  Batches:  1533  Loss:  0.00037731925840489566\n",
            "Epoch:  154  Batches:  1534  Loss:  0.0003465822373982519\n",
            "Epoch:  154  Batches:  1535  Loss:  0.00040091382106766105\n",
            "Epoch:  154  Batches:  1536  Loss:  0.0003270308079663664\n",
            "Epoch:  154  Batches:  1537  Loss:  0.00034241308458149433\n",
            "Epoch:  154  Batches:  1538  Loss:  0.00036268943222239614\n",
            "Epoch:  154  Batches:  1539  Loss:  0.0006938276346772909\n",
            "Epoch:  154  Batches:  1540  Loss:  0.0004537756904028356\n",
            "Epoch:  155  Batches:  1541  Loss:  0.0003625914396252483\n",
            "Epoch:  155  Batches:  1542  Loss:  0.00043184886453673244\n",
            "Epoch:  155  Batches:  1543  Loss:  0.00035614866646938026\n",
            "Epoch:  155  Batches:  1544  Loss:  0.0004314209218136966\n",
            "Epoch:  155  Batches:  1545  Loss:  0.0004954045289196074\n",
            "Epoch:  155  Batches:  1546  Loss:  0.0003302436671219766\n",
            "Epoch:  155  Batches:  1547  Loss:  0.000450100313173607\n",
            "Epoch:  155  Batches:  1548  Loss:  0.00027122790925204754\n",
            "Epoch:  155  Batches:  1549  Loss:  0.00034424220211803913\n",
            "Epoch:  155  Batches:  1550  Loss:  0.00038390891859307885\n",
            "Epoch:  156  Batches:  1551  Loss:  0.0003425620961934328\n",
            "Epoch:  156  Batches:  1552  Loss:  0.00036747954436577857\n",
            "Epoch:  156  Batches:  1553  Loss:  0.00042042694985866547\n",
            "Epoch:  156  Batches:  1554  Loss:  0.0003157328173983842\n",
            "Epoch:  156  Batches:  1555  Loss:  0.0005517267854884267\n",
            "Epoch:  156  Batches:  1556  Loss:  0.0004149788001086563\n",
            "Epoch:  156  Batches:  1557  Loss:  0.00040954237920232117\n",
            "Epoch:  156  Batches:  1558  Loss:  0.000369248038623482\n",
            "Epoch:  156  Batches:  1559  Loss:  0.0003215849574189633\n",
            "Epoch:  156  Batches:  1560  Loss:  0.0003835170064121485\n",
            "Epoch:  157  Batches:  1561  Loss:  0.0004651011840905994\n",
            "Epoch:  157  Batches:  1562  Loss:  0.00037464924389496446\n",
            "Epoch:  157  Batches:  1563  Loss:  0.0002718947653193027\n",
            "Epoch:  157  Batches:  1564  Loss:  0.0005708252429030836\n",
            "Epoch:  157  Batches:  1565  Loss:  0.0005114954547025263\n",
            "Epoch:  157  Batches:  1566  Loss:  0.0002938905672635883\n",
            "Epoch:  157  Batches:  1567  Loss:  0.00038038258207961917\n",
            "Epoch:  157  Batches:  1568  Loss:  0.0003240278165321797\n",
            "Epoch:  157  Batches:  1569  Loss:  0.0003656408516690135\n",
            "Epoch:  157  Batches:  1570  Loss:  0.00036677750176750124\n",
            "Epoch:  158  Batches:  1571  Loss:  0.0003813797375187278\n",
            "Epoch:  158  Batches:  1572  Loss:  0.000318058708216995\n",
            "Epoch:  158  Batches:  1573  Loss:  0.0004662228748202324\n",
            "Epoch:  158  Batches:  1574  Loss:  0.0005154048558324575\n",
            "Epoch:  158  Batches:  1575  Loss:  0.00034334705560468137\n",
            "Epoch:  158  Batches:  1576  Loss:  0.0003783187421504408\n",
            "Epoch:  158  Batches:  1577  Loss:  0.00034577318001538515\n",
            "Epoch:  158  Batches:  1578  Loss:  0.0003642656374722719\n",
            "Epoch:  158  Batches:  1579  Loss:  0.00037527066888287663\n",
            "Epoch:  158  Batches:  1580  Loss:  0.00039235182339325547\n",
            "Epoch:  159  Batches:  1581  Loss:  0.000411509710829705\n",
            "Epoch:  159  Batches:  1582  Loss:  0.0006245208787731826\n",
            "Epoch:  159  Batches:  1583  Loss:  0.0002914052165579051\n",
            "Epoch:  159  Batches:  1584  Loss:  0.0003207746194675565\n",
            "Epoch:  159  Batches:  1585  Loss:  0.0004435383598320186\n",
            "Epoch:  159  Batches:  1586  Loss:  0.0003391834907233715\n",
            "Epoch:  159  Batches:  1587  Loss:  0.0003481796884443611\n",
            "Epoch:  159  Batches:  1588  Loss:  0.0004249511403031647\n",
            "Epoch:  159  Batches:  1589  Loss:  0.0004277486586943269\n",
            "Epoch:  159  Batches:  1590  Loss:  0.00039906756137497723\n",
            "Epoch:  160  Batches:  1591  Loss:  0.00046946792281232774\n",
            "Epoch:  160  Batches:  1592  Loss:  0.0003066434001084417\n",
            "Epoch:  160  Batches:  1593  Loss:  0.00035404026857577264\n",
            "Epoch:  160  Batches:  1594  Loss:  0.0004510216531343758\n",
            "Epoch:  160  Batches:  1595  Loss:  0.00042119072168134153\n",
            "Epoch:  160  Batches:  1596  Loss:  0.00044416089076548815\n",
            "Epoch:  160  Batches:  1597  Loss:  0.0003521507023833692\n",
            "Epoch:  160  Batches:  1598  Loss:  0.00039344406104646623\n",
            "Epoch:  160  Batches:  1599  Loss:  0.00030840118415653706\n",
            "Epoch:  160  Batches:  1600  Loss:  0.00038960680831223726\n",
            "Epoch:  161  Batches:  1601  Loss:  0.00034705502912402153\n",
            "Epoch:  161  Batches:  1602  Loss:  0.00038183244760148227\n",
            "Epoch:  161  Batches:  1603  Loss:  0.0004438711330294609\n",
            "Epoch:  161  Batches:  1604  Loss:  0.0002639056765474379\n",
            "Epoch:  161  Batches:  1605  Loss:  0.0003304007404949516\n",
            "Epoch:  161  Batches:  1606  Loss:  0.0003365381562616676\n",
            "Epoch:  161  Batches:  1607  Loss:  0.00034766478347592056\n",
            "Epoch:  161  Batches:  1608  Loss:  0.0006443075835704803\n",
            "Epoch:  161  Batches:  1609  Loss:  0.0003621864307206124\n",
            "Epoch:  161  Batches:  1610  Loss:  0.00035435351310297847\n",
            "Epoch:  162  Batches:  1611  Loss:  0.00034893190604634583\n",
            "Epoch:  162  Batches:  1612  Loss:  0.0003933445259463042\n",
            "Epoch:  162  Batches:  1613  Loss:  0.00032669404754415154\n",
            "Epoch:  162  Batches:  1614  Loss:  0.00030973387765698135\n",
            "Epoch:  162  Batches:  1615  Loss:  0.00030163038172759116\n",
            "Epoch:  162  Batches:  1616  Loss:  0.0003555287257768214\n",
            "Epoch:  162  Batches:  1617  Loss:  0.00038271016092039645\n",
            "Epoch:  162  Batches:  1618  Loss:  0.0003567908424884081\n",
            "Epoch:  162  Batches:  1619  Loss:  0.0003864472091663629\n",
            "Epoch:  162  Batches:  1620  Loss:  0.0004873685829807073\n",
            "Epoch:  163  Batches:  1621  Loss:  0.000326663488522172\n",
            "Epoch:  163  Batches:  1622  Loss:  0.0005154793616384268\n",
            "Epoch:  163  Batches:  1623  Loss:  0.0003530574613250792\n",
            "Epoch:  163  Batches:  1624  Loss:  0.0002794113534037024\n",
            "Epoch:  163  Batches:  1625  Loss:  0.00033959021675400436\n",
            "Epoch:  163  Batches:  1626  Loss:  0.0002510202757548541\n",
            "Epoch:  163  Batches:  1627  Loss:  0.0003383230359759182\n",
            "Epoch:  163  Batches:  1628  Loss:  0.0003788189496845007\n",
            "Epoch:  163  Batches:  1629  Loss:  0.00035745007335208356\n",
            "Epoch:  163  Batches:  1630  Loss:  0.0005036681541241705\n",
            "Epoch:  164  Batches:  1631  Loss:  0.00035711232339963317\n",
            "Epoch:  164  Batches:  1632  Loss:  0.00048180954763665795\n",
            "Epoch:  164  Batches:  1633  Loss:  0.00031327735632658005\n",
            "Epoch:  164  Batches:  1634  Loss:  0.0002907050948124379\n",
            "Epoch:  164  Batches:  1635  Loss:  0.0002752510190475732\n",
            "Epoch:  164  Batches:  1636  Loss:  0.00044707919005304575\n",
            "Epoch:  164  Batches:  1637  Loss:  0.00044003681978210807\n",
            "Epoch:  164  Batches:  1638  Loss:  0.00037790078204125166\n",
            "Epoch:  164  Batches:  1639  Loss:  0.0003844096208922565\n",
            "Epoch:  164  Batches:  1640  Loss:  0.00036266344250179827\n",
            "Epoch:  165  Batches:  1641  Loss:  0.0003322295378893614\n",
            "Epoch:  165  Batches:  1642  Loss:  0.0004222692805342376\n",
            "Epoch:  165  Batches:  1643  Loss:  0.00031549602863378823\n",
            "Epoch:  165  Batches:  1644  Loss:  0.00025590951554477215\n",
            "Epoch:  165  Batches:  1645  Loss:  0.00047894392628222704\n",
            "Epoch:  165  Batches:  1646  Loss:  0.00038956874050199986\n",
            "Epoch:  165  Batches:  1647  Loss:  0.0003389363409951329\n",
            "Epoch:  165  Batches:  1648  Loss:  0.0003303641569800675\n",
            "Epoch:  165  Batches:  1649  Loss:  0.00033306953264400363\n",
            "Epoch:  165  Batches:  1650  Loss:  0.000317421363433823\n",
            "Epoch:  166  Batches:  1651  Loss:  0.0003650705621112138\n",
            "Epoch:  166  Batches:  1652  Loss:  0.0003207314293831587\n",
            "Epoch:  166  Batches:  1653  Loss:  0.0005576000548899174\n",
            "Epoch:  166  Batches:  1654  Loss:  0.0003820304700639099\n",
            "Epoch:  166  Batches:  1655  Loss:  0.0003022066375706345\n",
            "Epoch:  166  Batches:  1656  Loss:  0.00034485640935599804\n",
            "Epoch:  166  Batches:  1657  Loss:  0.000340538244927302\n",
            "Epoch:  166  Batches:  1658  Loss:  0.000284732726868242\n",
            "Epoch:  166  Batches:  1659  Loss:  0.00032378247124142945\n",
            "Epoch:  166  Batches:  1660  Loss:  0.0002687522501219064\n",
            "Epoch:  167  Batches:  1661  Loss:  0.0003647706180345267\n",
            "Epoch:  167  Batches:  1662  Loss:  0.0002690101391635835\n",
            "Epoch:  167  Batches:  1663  Loss:  0.0003059280279558152\n",
            "Epoch:  167  Batches:  1664  Loss:  0.0003147722454741597\n",
            "Epoch:  167  Batches:  1665  Loss:  0.0004977110656909645\n",
            "Epoch:  167  Batches:  1666  Loss:  0.00037031155079603195\n",
            "Epoch:  167  Batches:  1667  Loss:  0.000301967142149806\n",
            "Epoch:  167  Batches:  1668  Loss:  0.00036037579411640763\n",
            "Epoch:  167  Batches:  1669  Loss:  0.00035402976209297776\n",
            "Epoch:  167  Batches:  1670  Loss:  0.0003204641689080745\n",
            "Epoch:  168  Batches:  1671  Loss:  0.0003329755272716284\n",
            "Epoch:  168  Batches:  1672  Loss:  0.0003321600961498916\n",
            "Epoch:  168  Batches:  1673  Loss:  0.00039400297100655735\n",
            "Epoch:  168  Batches:  1674  Loss:  0.00028691021725535393\n",
            "Epoch:  168  Batches:  1675  Loss:  0.000543002737686038\n",
            "Epoch:  168  Batches:  1676  Loss:  0.0002457613591104746\n",
            "Epoch:  168  Batches:  1677  Loss:  0.0003303323464933783\n",
            "Epoch:  168  Batches:  1678  Loss:  0.00038622505962848663\n",
            "Epoch:  168  Batches:  1679  Loss:  0.0003168232797179371\n",
            "Epoch:  168  Batches:  1680  Loss:  0.0002323821245227009\n",
            "Epoch:  169  Batches:  1681  Loss:  0.000309444178128615\n",
            "Epoch:  169  Batches:  1682  Loss:  0.00043351814383640885\n",
            "Epoch:  169  Batches:  1683  Loss:  0.00029072436154820025\n",
            "Epoch:  169  Batches:  1684  Loss:  0.0003289473825134337\n",
            "Epoch:  169  Batches:  1685  Loss:  0.00033289045677520335\n",
            "Epoch:  169  Batches:  1686  Loss:  0.00033475144300609827\n",
            "Epoch:  169  Batches:  1687  Loss:  0.0003895788686349988\n",
            "Epoch:  169  Batches:  1688  Loss:  0.00031681053224019706\n",
            "Epoch:  169  Batches:  1689  Loss:  0.0003373082145117223\n",
            "Epoch:  169  Batches:  1690  Loss:  0.0004762442549690604\n",
            "Epoch:  170  Batches:  1691  Loss:  0.0004155962960794568\n",
            "Epoch:  170  Batches:  1692  Loss:  0.00036649513640441\n",
            "Epoch:  170  Batches:  1693  Loss:  0.00032812549034133554\n",
            "Epoch:  170  Batches:  1694  Loss:  0.0003640914801508188\n",
            "Epoch:  170  Batches:  1695  Loss:  0.0003793044597841799\n",
            "Epoch:  170  Batches:  1696  Loss:  0.0003919943992514163\n",
            "Epoch:  170  Batches:  1697  Loss:  0.0003570984408725053\n",
            "Epoch:  170  Batches:  1698  Loss:  0.00024317530915141106\n",
            "Epoch:  170  Batches:  1699  Loss:  0.00024107008357532322\n",
            "Epoch:  170  Batches:  1700  Loss:  0.0004457313916645944\n",
            "Epoch:  171  Batches:  1701  Loss:  0.00029520111274905503\n",
            "Epoch:  171  Batches:  1702  Loss:  0.00035992622724734247\n",
            "Epoch:  171  Batches:  1703  Loss:  0.0003398947883397341\n",
            "Epoch:  171  Batches:  1704  Loss:  0.0004699400451499969\n",
            "Epoch:  171  Batches:  1705  Loss:  0.00032995600486174226\n",
            "Epoch:  171  Batches:  1706  Loss:  0.0003007993218488991\n",
            "Epoch:  171  Batches:  1707  Loss:  0.00031353795202448964\n",
            "Epoch:  171  Batches:  1708  Loss:  0.0004301079025026411\n",
            "Epoch:  171  Batches:  1709  Loss:  0.00041940732626244426\n",
            "Epoch:  171  Batches:  1710  Loss:  0.00028670596657320857\n",
            "Epoch:  172  Batches:  1711  Loss:  0.00038100950769148767\n",
            "Epoch:  172  Batches:  1712  Loss:  0.00033348880242556334\n",
            "Epoch:  172  Batches:  1713  Loss:  0.00037680569221265614\n",
            "Epoch:  172  Batches:  1714  Loss:  0.0003354847140144557\n",
            "Epoch:  172  Batches:  1715  Loss:  0.0003032133972737938\n",
            "Epoch:  172  Batches:  1716  Loss:  0.0003283882688265294\n",
            "Epoch:  172  Batches:  1717  Loss:  0.00031597583438269794\n",
            "Epoch:  172  Batches:  1718  Loss:  0.00041535613127052784\n",
            "Epoch:  172  Batches:  1719  Loss:  0.00024386153381783515\n",
            "Epoch:  172  Batches:  1720  Loss:  0.000340660015353933\n",
            "Epoch:  173  Batches:  1721  Loss:  0.0002810512378346175\n",
            "Epoch:  173  Batches:  1722  Loss:  0.00031293381471186876\n",
            "Epoch:  173  Batches:  1723  Loss:  0.00029643444577232003\n",
            "Epoch:  173  Batches:  1724  Loss:  0.0003486327186692506\n",
            "Epoch:  173  Batches:  1725  Loss:  0.00034603176754899323\n",
            "Epoch:  173  Batches:  1726  Loss:  0.00031191052403301\n",
            "Epoch:  173  Batches:  1727  Loss:  0.0003261577512603253\n",
            "Epoch:  173  Batches:  1728  Loss:  0.0002812842431012541\n",
            "Epoch:  173  Batches:  1729  Loss:  0.0004882976645603776\n",
            "Epoch:  173  Batches:  1730  Loss:  0.00029819607152603567\n",
            "Epoch:  174  Batches:  1731  Loss:  0.00031993151060305536\n",
            "Epoch:  174  Batches:  1732  Loss:  0.00021922250743955374\n",
            "Epoch:  174  Batches:  1733  Loss:  0.000339470396284014\n",
            "Epoch:  174  Batches:  1734  Loss:  0.000258393120020628\n",
            "Epoch:  174  Batches:  1735  Loss:  0.00026602408615872264\n",
            "Epoch:  174  Batches:  1736  Loss:  0.0003652712039183825\n",
            "Epoch:  174  Batches:  1737  Loss:  0.0003077367728110403\n",
            "Epoch:  174  Batches:  1738  Loss:  0.00025952671421691775\n",
            "Epoch:  174  Batches:  1739  Loss:  0.0004970019799657166\n",
            "Epoch:  174  Batches:  1740  Loss:  0.0002735639864113182\n",
            "Epoch:  175  Batches:  1741  Loss:  0.0002624373009894043\n",
            "Epoch:  175  Batches:  1742  Loss:  0.0002959974226541817\n",
            "Epoch:  175  Batches:  1743  Loss:  0.0002633605618029833\n",
            "Epoch:  175  Batches:  1744  Loss:  0.00024560411111451685\n",
            "Epoch:  175  Batches:  1745  Loss:  0.00035036003100685775\n",
            "Epoch:  175  Batches:  1746  Loss:  0.0003006378829013556\n",
            "Epoch:  175  Batches:  1747  Loss:  0.000274947436992079\n",
            "Epoch:  175  Batches:  1748  Loss:  0.00026233389507979155\n",
            "Epoch:  175  Batches:  1749  Loss:  0.0005693905986845493\n",
            "Epoch:  175  Batches:  1750  Loss:  0.000300098501611501\n",
            "Epoch:  176  Batches:  1751  Loss:  0.0002614194236230105\n",
            "Epoch:  176  Batches:  1752  Loss:  0.00029949561576358974\n",
            "Epoch:  176  Batches:  1753  Loss:  0.0004484651144593954\n",
            "Epoch:  176  Batches:  1754  Loss:  0.00027461975696496665\n",
            "Epoch:  176  Batches:  1755  Loss:  0.0002729024563450366\n",
            "Epoch:  176  Batches:  1756  Loss:  0.00030334899201989174\n",
            "Epoch:  176  Batches:  1757  Loss:  0.0003477160935290158\n",
            "Epoch:  176  Batches:  1758  Loss:  0.0003009212086908519\n",
            "Epoch:  176  Batches:  1759  Loss:  0.0003840515564661473\n",
            "Epoch:  176  Batches:  1760  Loss:  0.0002649266389198601\n",
            "Epoch:  177  Batches:  1761  Loss:  0.0003930414968635887\n",
            "Epoch:  177  Batches:  1762  Loss:  0.0003628561971709132\n",
            "Epoch:  177  Batches:  1763  Loss:  0.0002654031559359282\n",
            "Epoch:  177  Batches:  1764  Loss:  0.00027661732747219503\n",
            "Epoch:  177  Batches:  1765  Loss:  0.0003601445641834289\n",
            "Epoch:  177  Batches:  1766  Loss:  0.00032547989394515753\n",
            "Epoch:  177  Batches:  1767  Loss:  0.00020787007815670222\n",
            "Epoch:  177  Batches:  1768  Loss:  0.0002969087508972734\n",
            "Epoch:  177  Batches:  1769  Loss:  0.0002472663181833923\n",
            "Epoch:  177  Batches:  1770  Loss:  0.00032267661299556494\n",
            "Epoch:  178  Batches:  1771  Loss:  0.00030532851815223694\n",
            "Epoch:  178  Batches:  1772  Loss:  0.00037013550172559917\n",
            "Epoch:  178  Batches:  1773  Loss:  0.000258703192230314\n",
            "Epoch:  178  Batches:  1774  Loss:  0.0004283890302758664\n",
            "Epoch:  178  Batches:  1775  Loss:  0.0002648730587679893\n",
            "Epoch:  178  Batches:  1776  Loss:  0.0003779117832891643\n",
            "Epoch:  178  Batches:  1777  Loss:  0.0002852347679436207\n",
            "Epoch:  178  Batches:  1778  Loss:  0.00022770254872739315\n",
            "Epoch:  178  Batches:  1779  Loss:  0.00035206336178816855\n",
            "Epoch:  178  Batches:  1780  Loss:  0.0003372442733962089\n",
            "Epoch:  179  Batches:  1781  Loss:  0.00037464580964297056\n",
            "Epoch:  179  Batches:  1782  Loss:  0.0002967479231301695\n",
            "Epoch:  179  Batches:  1783  Loss:  0.00035085654235444963\n",
            "Epoch:  179  Batches:  1784  Loss:  0.0002932822098955512\n",
            "Epoch:  179  Batches:  1785  Loss:  0.00035091699101030827\n",
            "Epoch:  179  Batches:  1786  Loss:  0.0002146774932043627\n",
            "Epoch:  179  Batches:  1787  Loss:  0.00029367179377004504\n",
            "Epoch:  179  Batches:  1788  Loss:  0.0002690208493731916\n",
            "Epoch:  179  Batches:  1789  Loss:  0.000296248123049736\n",
            "Epoch:  179  Batches:  1790  Loss:  0.0002608992508612573\n",
            "Epoch:  180  Batches:  1791  Loss:  0.0003221007063984871\n",
            "Epoch:  180  Batches:  1792  Loss:  0.0002670333196874708\n",
            "Epoch:  180  Batches:  1793  Loss:  0.000265147362370044\n",
            "Epoch:  180  Batches:  1794  Loss:  0.00039045134326443076\n",
            "Epoch:  180  Batches:  1795  Loss:  0.00022722648282069713\n",
            "Epoch:  180  Batches:  1796  Loss:  0.00042231264524161816\n",
            "Epoch:  180  Batches:  1797  Loss:  0.0003308959712740034\n",
            "Epoch:  180  Batches:  1798  Loss:  0.0002742163778748363\n",
            "Epoch:  180  Batches:  1799  Loss:  0.0003058912989217788\n",
            "Epoch:  180  Batches:  1800  Loss:  0.00028887329972349107\n",
            "Epoch:  181  Batches:  1801  Loss:  0.00021089997608214617\n",
            "Epoch:  181  Batches:  1802  Loss:  0.00031259816023521125\n",
            "Epoch:  181  Batches:  1803  Loss:  0.0002527071628719568\n",
            "Epoch:  181  Batches:  1804  Loss:  0.0002739289775490761\n",
            "Epoch:  181  Batches:  1805  Loss:  0.00028395294793881476\n",
            "Epoch:  181  Batches:  1806  Loss:  0.00039501505671069026\n",
            "Epoch:  181  Batches:  1807  Loss:  0.0002344846143387258\n",
            "Epoch:  181  Batches:  1808  Loss:  0.00034660488017834723\n",
            "Epoch:  181  Batches:  1809  Loss:  0.0003832549846265465\n",
            "Epoch:  181  Batches:  1810  Loss:  0.0003802496939897537\n",
            "Epoch:  182  Batches:  1811  Loss:  0.000377024058252573\n",
            "Epoch:  182  Batches:  1812  Loss:  0.0002980038698296994\n",
            "Epoch:  182  Batches:  1813  Loss:  0.0002186082274420187\n",
            "Epoch:  182  Batches:  1814  Loss:  0.0003056196728721261\n",
            "Epoch:  182  Batches:  1815  Loss:  0.00032630597706884146\n",
            "Epoch:  182  Batches:  1816  Loss:  0.0003509375383146107\n",
            "Epoch:  182  Batches:  1817  Loss:  0.0003412433434277773\n",
            "Epoch:  182  Batches:  1818  Loss:  0.00032021960942074656\n",
            "Epoch:  182  Batches:  1819  Loss:  0.0002522845461498946\n",
            "Epoch:  182  Batches:  1820  Loss:  0.0003171485150232911\n",
            "Epoch:  183  Batches:  1821  Loss:  0.000397642666939646\n",
            "Epoch:  183  Batches:  1822  Loss:  0.0005846032872796059\n",
            "Epoch:  183  Batches:  1823  Loss:  0.00024798675440251827\n",
            "Epoch:  183  Batches:  1824  Loss:  0.0003241337544750422\n",
            "Epoch:  183  Batches:  1825  Loss:  0.00032232061494141817\n",
            "Epoch:  183  Batches:  1826  Loss:  0.0003313959459774196\n",
            "Epoch:  183  Batches:  1827  Loss:  0.0002833753533195704\n",
            "Epoch:  183  Batches:  1828  Loss:  0.00041753865662030876\n",
            "Epoch:  183  Batches:  1829  Loss:  0.00031352307996712625\n",
            "Epoch:  183  Batches:  1830  Loss:  0.0002973444643430412\n",
            "Epoch:  184  Batches:  1831  Loss:  0.0002759555936791003\n",
            "Epoch:  184  Batches:  1832  Loss:  0.00033822606201283634\n",
            "Epoch:  184  Batches:  1833  Loss:  0.00043241828097961843\n",
            "Epoch:  184  Batches:  1834  Loss:  0.0002866681606974453\n",
            "Epoch:  184  Batches:  1835  Loss:  0.0002802891831379384\n",
            "Epoch:  184  Batches:  1836  Loss:  0.00024176161969080567\n",
            "Epoch:  184  Batches:  1837  Loss:  0.0003280120436102152\n",
            "Epoch:  184  Batches:  1838  Loss:  0.0003752892080228776\n",
            "Epoch:  184  Batches:  1839  Loss:  0.00023831159342080355\n",
            "Epoch:  184  Batches:  1840  Loss:  0.000293312274152413\n",
            "Epoch:  185  Batches:  1841  Loss:  0.0004371476825326681\n",
            "Epoch:  185  Batches:  1842  Loss:  0.00022065256780479103\n",
            "Epoch:  185  Batches:  1843  Loss:  0.0003003787132911384\n",
            "Epoch:  185  Batches:  1844  Loss:  0.0002957817050628364\n",
            "Epoch:  185  Batches:  1845  Loss:  0.00033046287717297673\n",
            "Epoch:  185  Batches:  1846  Loss:  0.0002466082223691046\n",
            "Epoch:  185  Batches:  1847  Loss:  0.0003164008667226881\n",
            "Epoch:  185  Batches:  1848  Loss:  0.00034506304655224085\n",
            "Epoch:  185  Batches:  1849  Loss:  0.0002957558899652213\n",
            "Epoch:  185  Batches:  1850  Loss:  0.00032054653274826705\n",
            "Epoch:  186  Batches:  1851  Loss:  0.0003367681347299367\n",
            "Epoch:  186  Batches:  1852  Loss:  0.00023345003137364984\n",
            "Epoch:  186  Batches:  1853  Loss:  0.0002406941057415679\n",
            "Epoch:  186  Batches:  1854  Loss:  0.0004473085864447057\n",
            "Epoch:  186  Batches:  1855  Loss:  0.00022417072614189237\n",
            "Epoch:  186  Batches:  1856  Loss:  0.000281899148831144\n",
            "Epoch:  186  Batches:  1857  Loss:  0.00031156899058260024\n",
            "Epoch:  186  Batches:  1858  Loss:  0.0002798348432406783\n",
            "Epoch:  186  Batches:  1859  Loss:  0.00025312459911219776\n",
            "Epoch:  186  Batches:  1860  Loss:  0.0003208847192581743\n",
            "Epoch:  187  Batches:  1861  Loss:  0.0002661252801772207\n",
            "Epoch:  187  Batches:  1862  Loss:  0.00033036433160305023\n",
            "Epoch:  187  Batches:  1863  Loss:  0.00031851185485720634\n",
            "Epoch:  187  Batches:  1864  Loss:  0.00022351869847625494\n",
            "Epoch:  187  Batches:  1865  Loss:  0.00024927136837504804\n",
            "Epoch:  187  Batches:  1866  Loss:  0.0003980263718403876\n",
            "Epoch:  187  Batches:  1867  Loss:  0.00024135240528266877\n",
            "Epoch:  187  Batches:  1868  Loss:  0.0002931243216153234\n",
            "Epoch:  187  Batches:  1869  Loss:  0.0002786641416605562\n",
            "Epoch:  187  Batches:  1870  Loss:  0.00034733282518573105\n",
            "Epoch:  188  Batches:  1871  Loss:  0.0003661251103039831\n",
            "Epoch:  188  Batches:  1872  Loss:  0.00020295160356909037\n",
            "Epoch:  188  Batches:  1873  Loss:  0.00022426813666243106\n",
            "Epoch:  188  Batches:  1874  Loss:  0.0002823818940669298\n",
            "Epoch:  188  Batches:  1875  Loss:  0.0004228813631925732\n",
            "Epoch:  188  Batches:  1876  Loss:  0.0002227268269052729\n",
            "Epoch:  188  Batches:  1877  Loss:  0.0003350292972754687\n",
            "Epoch:  188  Batches:  1878  Loss:  0.0002409649023320526\n",
            "Epoch:  188  Batches:  1879  Loss:  0.0002946140884887427\n",
            "Epoch:  188  Batches:  1880  Loss:  0.0003638562629930675\n",
            "Epoch:  189  Batches:  1881  Loss:  0.00022828590590506792\n",
            "Epoch:  189  Batches:  1882  Loss:  0.0003526953514665365\n",
            "Epoch:  189  Batches:  1883  Loss:  0.00031453155679628253\n",
            "Epoch:  189  Batches:  1884  Loss:  0.0002510463527869433\n",
            "Epoch:  189  Batches:  1885  Loss:  0.0002172099775634706\n",
            "Epoch:  189  Batches:  1886  Loss:  0.00043225366971455514\n",
            "Epoch:  189  Batches:  1887  Loss:  0.00026715401327237487\n",
            "Epoch:  189  Batches:  1888  Loss:  0.00035590611514635384\n",
            "Epoch:  189  Batches:  1889  Loss:  0.00032055494375526905\n",
            "Epoch:  189  Batches:  1890  Loss:  0.0002201694733230397\n",
            "Epoch:  190  Batches:  1891  Loss:  0.00033064582385122776\n",
            "Epoch:  190  Batches:  1892  Loss:  0.0004258727713022381\n",
            "Epoch:  190  Batches:  1893  Loss:  0.0003208260459359735\n",
            "Epoch:  190  Batches:  1894  Loss:  0.00024945964105427265\n",
            "Epoch:  190  Batches:  1895  Loss:  0.0003320710966363549\n",
            "Epoch:  190  Batches:  1896  Loss:  0.00029379650368355215\n",
            "Epoch:  190  Batches:  1897  Loss:  0.00023889511066954583\n",
            "Epoch:  190  Batches:  1898  Loss:  0.00027979855076409876\n",
            "Epoch:  190  Batches:  1899  Loss:  0.0003517511358950287\n",
            "Epoch:  190  Batches:  1900  Loss:  0.00039064069278538227\n",
            "Epoch:  191  Batches:  1901  Loss:  0.0002886845322791487\n",
            "Epoch:  191  Batches:  1902  Loss:  0.00023266678908839822\n",
            "Epoch:  191  Batches:  1903  Loss:  0.00033044852898456156\n",
            "Epoch:  191  Batches:  1904  Loss:  0.00040082933264784515\n",
            "Epoch:  191  Batches:  1905  Loss:  0.0003601606877055019\n",
            "Epoch:  191  Batches:  1906  Loss:  0.00034714967478066683\n",
            "Epoch:  191  Batches:  1907  Loss:  0.0003291047178208828\n",
            "Epoch:  191  Batches:  1908  Loss:  0.00030187261290848255\n",
            "Epoch:  191  Batches:  1909  Loss:  0.00029762316262349486\n",
            "Epoch:  191  Batches:  1910  Loss:  0.0002823362883646041\n",
            "Epoch:  192  Batches:  1911  Loss:  0.0003241139929741621\n",
            "Epoch:  192  Batches:  1912  Loss:  0.0003840811550617218\n",
            "Epoch:  192  Batches:  1913  Loss:  0.0003009475185535848\n",
            "Epoch:  192  Batches:  1914  Loss:  0.0003071557148359716\n",
            "Epoch:  192  Batches:  1915  Loss:  0.0002507659955881536\n",
            "Epoch:  192  Batches:  1916  Loss:  0.0006696026539430022\n",
            "Epoch:  192  Batches:  1917  Loss:  0.0002751972060650587\n",
            "Epoch:  192  Batches:  1918  Loss:  0.00025523846852593124\n",
            "Epoch:  192  Batches:  1919  Loss:  0.0002521576825529337\n",
            "Epoch:  192  Batches:  1920  Loss:  0.0003252241585869342\n",
            "Epoch:  193  Batches:  1921  Loss:  0.00033024162985384464\n",
            "Epoch:  193  Batches:  1922  Loss:  0.00023857112682890147\n",
            "Epoch:  193  Batches:  1923  Loss:  0.0002876359794754535\n",
            "Epoch:  193  Batches:  1924  Loss:  0.00041970793972723186\n",
            "Epoch:  193  Batches:  1925  Loss:  0.00025248361635021865\n",
            "Epoch:  193  Batches:  1926  Loss:  0.00027138006407767534\n",
            "Epoch:  193  Batches:  1927  Loss:  0.0002862135006580502\n",
            "Epoch:  193  Batches:  1928  Loss:  0.0002737977774813771\n",
            "Epoch:  193  Batches:  1929  Loss:  0.0005031026084907353\n",
            "Epoch:  193  Batches:  1930  Loss:  0.00027175972354598343\n",
            "Epoch:  194  Batches:  1931  Loss:  0.0005265907384455204\n",
            "Epoch:  194  Batches:  1932  Loss:  0.0002442040713503957\n",
            "Epoch:  194  Batches:  1933  Loss:  0.00031540601048618555\n",
            "Epoch:  194  Batches:  1934  Loss:  0.0003248719731345773\n",
            "Epoch:  194  Batches:  1935  Loss:  0.00027195739676244557\n",
            "Epoch:  194  Batches:  1936  Loss:  0.00023017823696136475\n",
            "Epoch:  194  Batches:  1937  Loss:  0.00022236150107346475\n",
            "Epoch:  194  Batches:  1938  Loss:  0.00033980648731812835\n",
            "Epoch:  194  Batches:  1939  Loss:  0.0002343760133953765\n",
            "Epoch:  194  Batches:  1940  Loss:  0.0002508447796572\n",
            "Epoch:  195  Batches:  1941  Loss:  0.0003442414745222777\n",
            "Epoch:  195  Batches:  1942  Loss:  0.0002734861627686769\n",
            "Epoch:  195  Batches:  1943  Loss:  0.0002671235124580562\n",
            "Epoch:  195  Batches:  1944  Loss:  0.0002792621380649507\n",
            "Epoch:  195  Batches:  1945  Loss:  0.00038901716470718384\n",
            "Epoch:  195  Batches:  1946  Loss:  0.00027402708656154573\n",
            "Epoch:  195  Batches:  1947  Loss:  0.0002742568904068321\n",
            "Epoch:  195  Batches:  1948  Loss:  0.00022509532573167235\n",
            "Epoch:  195  Batches:  1949  Loss:  0.0002226006326964125\n",
            "Epoch:  195  Batches:  1950  Loss:  0.0003966095100622624\n",
            "Epoch:  196  Batches:  1951  Loss:  0.00044479849748313427\n",
            "Epoch:  196  Batches:  1952  Loss:  0.00028020425816066563\n",
            "Epoch:  196  Batches:  1953  Loss:  0.00025238507078029215\n",
            "Epoch:  196  Batches:  1954  Loss:  0.0003966893127653748\n",
            "Epoch:  196  Batches:  1955  Loss:  0.00024605379439890385\n",
            "Epoch:  196  Batches:  1956  Loss:  0.00027705568936653435\n",
            "Epoch:  196  Batches:  1957  Loss:  0.0003947524237446487\n",
            "Epoch:  196  Batches:  1958  Loss:  0.0002655861317180097\n",
            "Epoch:  196  Batches:  1959  Loss:  0.0002844040282070637\n",
            "Epoch:  196  Batches:  1960  Loss:  0.00043601999641396105\n",
            "Epoch:  197  Batches:  1961  Loss:  0.0003665408294182271\n",
            "Epoch:  197  Batches:  1962  Loss:  0.000280367792584002\n",
            "Epoch:  197  Batches:  1963  Loss:  0.0003118258900940418\n",
            "Epoch:  197  Batches:  1964  Loss:  0.0003286889404989779\n",
            "Epoch:  197  Batches:  1965  Loss:  0.0002652640687301755\n",
            "Epoch:  197  Batches:  1966  Loss:  0.00026737895677797496\n",
            "Epoch:  197  Batches:  1967  Loss:  0.00025691287009976804\n",
            "Epoch:  197  Batches:  1968  Loss:  0.0002984956372529268\n",
            "Epoch:  197  Batches:  1969  Loss:  0.0003640276554506272\n",
            "Epoch:  197  Batches:  1970  Loss:  0.0002144336758647114\n",
            "Epoch:  198  Batches:  1971  Loss:  0.0002576012338977307\n",
            "Epoch:  198  Batches:  1972  Loss:  0.0003274849441368133\n",
            "Epoch:  198  Batches:  1973  Loss:  0.00029542544507421553\n",
            "Epoch:  198  Batches:  1974  Loss:  0.00024109601508826017\n",
            "Epoch:  198  Batches:  1975  Loss:  0.00026911223540082574\n",
            "Epoch:  198  Batches:  1976  Loss:  0.0002815317129716277\n",
            "Epoch:  198  Batches:  1977  Loss:  0.0002977601543534547\n",
            "Epoch:  198  Batches:  1978  Loss:  0.00042317502084188163\n",
            "Epoch:  198  Batches:  1979  Loss:  0.00030259881168603897\n",
            "Epoch:  198  Batches:  1980  Loss:  0.00021197416936047375\n",
            "Epoch:  199  Batches:  1981  Loss:  0.00025693688075989485\n",
            "Epoch:  199  Batches:  1982  Loss:  0.00025901998742483556\n",
            "Epoch:  199  Batches:  1983  Loss:  0.0003343482094351202\n",
            "Epoch:  199  Batches:  1984  Loss:  0.0002461266121827066\n",
            "Epoch:  199  Batches:  1985  Loss:  0.000254552491242066\n",
            "Epoch:  199  Batches:  1986  Loss:  0.0003881947777699679\n",
            "Epoch:  199  Batches:  1987  Loss:  0.0003415757673792541\n",
            "Epoch:  199  Batches:  1988  Loss:  0.0002874816709663719\n",
            "Epoch:  199  Batches:  1989  Loss:  0.00028692660271190107\n",
            "Epoch:  199  Batches:  1990  Loss:  0.0002452464832458645\n",
            "Epoch:  200  Batches:  1991  Loss:  0.00023753196001052856\n",
            "Epoch:  200  Batches:  1992  Loss:  0.00022906024241819978\n",
            "Epoch:  200  Batches:  1993  Loss:  0.0003759558603633195\n",
            "Epoch:  200  Batches:  1994  Loss:  0.00023820632486604154\n",
            "Epoch:  200  Batches:  1995  Loss:  0.00023972481722012162\n",
            "Epoch:  200  Batches:  1996  Loss:  0.00039466776070185006\n",
            "Epoch:  200  Batches:  1997  Loss:  0.0002755465102382004\n",
            "Epoch:  200  Batches:  1998  Loss:  0.00029431775328703225\n",
            "Epoch:  200  Batches:  1999  Loss:  0.00024659844348207116\n",
            "Epoch:  200  Batches:  2000  Loss:  0.0003668954595923424\n",
            "The loss of training is:  0.0003668954595923424\n",
            "The loss of validation is:  0.00025786308106034994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svQb6BjvG4RJ",
        "colab_type": "text"
      },
      "source": [
        "**Perform Predictions of a given input (numpy)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrP_EzpgG6dL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1f12d0e8-f875-445a-904d-84d35e3ebf30"
      },
      "source": [
        "\n",
        "#This is how you make predicitons\n",
        "fs = ANN.predict(X)\n",
        "\n",
        "plt.plot(F,fs,'o')\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Predicitons')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Predicitons')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
