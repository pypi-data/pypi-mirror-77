project:
  name: null
  # One can specify project-level extra requirements, but this normally not
  # needed. Roberto will try to derive all requirements automatically from
  # the default configuration and the conda recipes present in the project.
  conda_requirements: []
  pip_requirements: []
  packages: [
  # Repeat as many as you like. The order determines the order in which they
  # are linted, tested, built, etc.
  # - conda_name: 'name of the conda package' # deprecated, use dist_name
  #   dist_name: 'name_for_distribution_packages' # can include python-
  #   tools: [] # list, any of the tools defined below
  #   name: '' # python or cpp name, defaults to project.name
  #   path: # root of the package relative to the project root defaults to '.'
  ]

# Custom configuration below this line should be rarely needed.

run:
  echo: true
conda:
  download_dir: '${HOME}/Downloads/'  # Used for downloading miniconda and optionally macosx sdk
  linux_url: 'https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh'
  osx_url: 'https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh'
  base_path: '${HOME}/miniconda3'
  # The pinning config must be a string of words separated by whitespace,
  # alternating package and version number. These packages will be pinned at
  # the given version in conda. Don't use wildcards.
  pinning: 'python 3.7'
  channels: [conda-forge]  # you practically always need to include conda-forge.
  # The following are set automatically...
  env_name: null  # The name of the conda development environment.
  env_path: null  # The path of the conda development environment.
  # Needed to make clang happy inside conda...
  macosx: '10.9'
  maxosx_sdk_release: 'https://github.com/phracker/MacOSX-SDKs/releases/download/10.13'
deploy_noarch: false  # Set to true for deployment of architecture-independent files.
deploy_binary: false  # Set to true for deployment of architecture-dependent files.
upload_coverage: false  # Set to true for uploading coverage
abs: false  # Force absolute comparison for cardboardlint
git:
  # Name of the branch being merged into, may be same as git.branch, if not a PR.
  merge_branch: master
  # The following are set automatically...
  branch: null  # name of the current branch
  describe: null  # output of git describe --tags
  tag: null  # the most recent tag
  tag_version: null  # version name derived from git describe, i.e. major.minor.patch{.post}
  tag_soversion: null  # the so-version, i.e. major.minor
  tag_version_major: null
  tag_version_minor: null
  tag_version_patch: null
  tag_version_suffix: null
  tag_stable: false  # is the current build a stable or main release?
  tag_test: false  # is the current build a test or beta release?
  tag_dev: false  # is the current build a development or alpha release?
  tag_release: false  # is the current build a release

tools:
  # Write a version.py file
  write-py-version:
    task: write-version
    # A template for the file to be written, usually short.
    # You can use python string formatting to insert fields
    # from the configuration. When Roberto starts, it derives
    # the version from the git tag and stores it in the
    # configuration. It can be inserted as {config.git.tag_version}.
    template: |
      """Autogenerated version information file.

      Manual changes to this file will be overwritten.

      The output of git describe was: ``{config.git.describe}``

      The derived version is: ``{config.git.tag_version}``
      """
      __version__ = '{config.git.tag_version}'
      DEV_CLASSIFIER = '{config.git.dev_classifier}'
    # The file to be written. Existing files are overwritten.
    destination: '{package.name}/version.py'

  # Write a CMakeListsVersion.txt.in file
  write-cmake-version:
    task: write-version
    template: |
      # This file is automatically generated. Manual changes will be overwritten.
      set(GIT_DESCRIBE "{config.git.describe}")
      set(GIT_TAG_VERSION "{config.git.tag_version}")
      set(GIT_TAG_SOVERSION "{config.git.tag_soversion}")
      set(GIT_TAG_VERSION_MAJOR "{config.git.tag_version_major}")
      set(GIT_TAG_VERSION_MINOR "{config.git.tag_version_minor}")
      set(GIT_TAG_VERSION_PATCH "{config.git.tag_version_patch}")
    destination: 'CMakeListsVersion.txt.in'

  # Run the static linters with for cardboardlint as configured in in ~/.cardboardlint.yaml
  # (Static liners do not require compilation or interpretaton of code.)
  # See https://github.com/theochem/cardboardlint/
  cardboardlint-static:
    task: lint-static
    conda_requirements:
      - theochem::cardboardlint
      - pycodestyle
      - pydocstyle
      - cppcheck
      - cpplint
      - yamllint
      - flake8
      - doxygen
      - autopep8
      - black
      - yapf
      - restructuredtext_lint
      # Following is needed for rst-lint.
      # See https://github.com/twolfson/restructuredtext-lint/issues/41
      - pygments
    # Commands to be executed when working in the merge_branch, usually
    # this is the master branch.
    commands_master:
      - cardboardlinter -f static -n auto
    # Commdands to be executed when working in a feature banch, which is to
    # be merged into the merge_branch. This makes it possible to filter out
    # linting messages that are unrelated to the changes in the feature branch.
    commands_feature:
      - 'cardboardlinter -r {config.git.merge_branch} -f static -n auto'

  # Perform an in-place build of a Python package. This may even be useful for
  # pure python packages, to generate a suitable activate script.
  build-py-inplace:
    task: build-inplace
    # Additions to *PATH variables due to this in-place build, possibly needed
    # by other packages, or when one would like to use the in-place build.
    export_paths:
      # Variables with null will be initialized with an empty value.
      PYTHONPATH: '{package.abspath}'
    # Environment variables that could affect this in-place build. When listed
    # here, their contents before any in-place build are printed, so one sees
    # pre-existing variables that may affect the in-place build.
    check_vars:
      - CXXFLAGS
      - CFLAGS
      - LDFLAGS
      - PYTHONPATH
      - RPATH
    # Bash commands to build in-place.
    commands:
      # Note that distutils is not very clever with an empty -R argument.
      # This would simply result in a disfunctional compiler argument,
      # which we have to avoid. Hence, some ugly bash is needed.
      # In case of OSX, i.e. when SDKROOT is specified, we need to set CFLAGS
      # and CXXFLAGS explicitly because distutils does not pick this up for us.
      - if [[ -n "${{SDKROOT}}" ]]; then
          export CXXFLAGS="${{CXXFLAGS}} -isysroot ${{CONDA_BUILD_SYSROOT}}";
          export CFLAGS="${{CFLAGS}} -isysroot ${{CONDA_BUILD_SYSROOT}}";
        fi;
        python setup.py build_ext -i --define CYTHON_TRACE_NOGIL
        $([[ -n "${{RPATH}}" ]] && echo -R "${{RPATH}}")
        -I ${{CONDA_PREFIX}}/include

  # Perform an in-place build of a CMake package.
  build-cmake-inplace:
    task: build-inplace
    conda_requirements:
      - cmake
      - make
    export_paths:
      # Different compilers have incompatible arguments to set the RPATH
      # for the linker, so we cannot use LDFLAGS for this, alas.
      RPATH: '{package.abspath}/build/{package.name}'
    export_flags:
      CXXFLAGS: >-
        -I {package.abspath}
      CFLAGS: >-
        -I {package.abspath}
      LDFLAGS: >-
        -L {package.abspath}/build/{package.name}
    check_vars:
      - CXXFLAGS
      - CFLAGS
      - LDFLAGS
    commands:
      # On OSX, CMake will pick up SDKROOT automatically. See CMake docs
      # https://cmake.org/cmake/help/latest/variable/CMAKE_OSX_SYSROOT.html
      - mkdir -p build
      - cd build; cmake .. -DCMAKE_BUILD_TYPE=debug
      - cd build; VERBOSE=1 make -j $(getconf _NPROCESSORS_ONLN)

  # Run unit tests on an in-place build with pytest.
  pytest:
    task: test-inplace
    conda_requirements:
      - pytest
      - pytest-cov
      - pytest-xdist
      - pytest-regressions
    # Bash commands to perform the tests. Environment variables defined in the
    # in-place build will still be set.
    commands:
      - pytest {package.name} -v --cov={package.name}
          --cov-report xml:coverage_pytest.xml
          --cov-report term-missing
          --cov-branch --color=yes
          -n auto

  # Run unit tests on an in-place build with nosetests.
  nose:
    task: test-inplace
    conda_requirements:
      - nose
      - coverage
    commands:
      - rm -f .coverage
      - nosetests {package.name} -v --detailed-errors --with-coverage
          --cover-package={package.name} --cover-tests --cover-inclusive
          --cover-branches
      - coverage xml -i -o coverage_nose.xml

  # Run unit tests on an in-place build with `make test`.
  maketest:
    task: test-inplace
    conda_requirements:
      - gcovr
      - make
    commands:
      # Remove old coverage results, if any.
      - cd build; find . | grep '\\.gcda$' | xargs rm -vf
      # Run the tests.
      - cd build; make test
      # Run gcov manually, only on Linux for now.
      - if [[ -z "${{SDKROOT}}" ]]; then
          cd build; find . -type f -name '*.gcno' -exec ${{HOST}}-gcov -pbc {{}} +;
        fi
      # Remove coverage files related to libraries in the conda env.
      - cd build; rm -vf *'#envs#{config.conda.env_name}#'*.gcov
      # gcovr searches for all *.gcov files and produces a nice text report on
      # stdout.
      - gcovr -gk

  # Upload coverage result to codecov
  upload-codecov:
    task: upload-coverage
    pip_requirements:
      - codecov
    commands:
      # We have already executed gcov previously, so no need to run it again.
      - codecov -X gcov

  # Run the dynamic linters with for cardboardlint as configured in in ~/.cardboardlint.yaml
  # (Dynamic liners require compilation or interpretaton of code.)
  # See https://github.com/theochem/cardboardlint/
  cardboardlint-dynamic:
    task: lint-dynamic
    conda_requirements:
      - theochem::cardboardlint
      - pylint
    # Commands to be executed when working in the merge_branch, usually
    # this is the master branch.
    commands_master:
      - cardboardlinter -f dynamic -n auto
    # Commdands to be executed when working in a feature banch, which is to
    # be merged into the merge_branch. This makes it possible to filter out
    # linting messages that are unrelated to the changes in the feature branch.
    commands_feature:
      - 'cardboardlinter -r {config.git.merge_branch} -f dynamic -n auto'

  # Build the sphinx documentation
  build-sphinx-doc:
    # The task build-docs compiles the (online) documentation.
    task: build-docs
    conda_requirements:
      - sphinx >=1.8
      - sphinx_rtd_theme
      - sphinx-autodoc-typehints
      - sphinxcontrib-apidoc
    # Bash commands to build the documentation
    commands:
      - cd doc; make html

  # Upload (force-push) the sphinx documentation to gh-pages on github.com
  upload-docs-gh:
    # The task upload-docs-git pushes the previously built documentation to a
    # git branch of choice.
    task: upload-docs-git
    # The root of the documentation website.
    docroot: '{package.path}/doc/_build/html'
    # The branch to push to. Changes will be amended and force-pushed.
    docbranch: gh-pages
    # The name of the git remote to push to.
    docremote: origin
    # Use labels to decide for which releases an upload is needed.
    deploy_labels:
      - main
      - test

  # Build a python sdist
  build-py-source:
    # The task build-packages prepares packages for deployment.
    task: build-packages
    # Bash commands to make the packages.
    commands:
      - python setup.py sdist

  # Build a source distribution with cmake
  build-cmake-source:
    task: build-packages
    conda_requirements:
      - cmake
      - make
    commands:
      - mkdir -p dist
      - cd dist; cmake .. -DCMAKE_BUILD_TYPE=release
      - cd dist; make sdist

  # Build a conda package
  build-conda:
    task: build-packages
    conda_requirements:
      - conda-build
      - conda-verify
    commands:
      # CONDA_BLD_PATH must be set because otherwise local dependencies
      # cannot be found when working in any other environment than base.
      # See https://github.com/conda/conda-build/issues/2592
      # Some SDK kung-fu is needed to make everything reproducible on OSX. The
      # lines below are very sensitive to subtle changes, so don't touch
      # without proper testing on both Linux and OSX.
      - if [[ -n "${{SDKROOT}}" ]]; then
          printf "CONDA_BUILD_SYSROOT:\n  - ${{SDKROOT}}\n"
          > {config.conda.env_path}/conda_build_config.yaml;
          cd {config.conda.env_path};
        fi;
        CONDA_BLD_PATH={config.conda.build_path}
        PROJECT_VERSION={config.git.tag_version}
        conda build {package.abspath}/tools/conda.recipe --override-channel
          --skip-existing --no-anaconda-upload --variants {config.conda.variants}

  # Upload python source packages to pypi
  deploy-pypi:
    # The task deploy uploads software packages to distribution servers.
    task: deploy
    conda_requirements:
      - twine
    deploy_vars:
      - TWINE_USERNAME
      - TWINE_PASSWORD
    # Include a file with the sha256 sum when uploading, optional.
    include_sha256: false
    # Glob patterns to locate architecture-independent "assets".
    noarch_asset_patterns:
      - '{package.path}/dist/{package.name}-{config.git.tag_version}.*'
    # Glob patterns to locate binary "assets".
    binary_asset_patterns: []
    # List labels here for when the deployment is required. With main, only
    # stable releases are uploaded.
    deploy_labels:
      - main
      - test
      - dev
    # Finally, the commands to perform the upload.
    commands:
      - twine upload --non-interactive {assets}

  # Upload any source packages to github.com
  deploy-github:
    task: deploy
    conda_requirements:
      - hub
    deploy_vars:
      - GITHUB_TOKEN
    include_sha256: true
    noarch_asset_patterns:
      - '{package.path}/*dist*/{package.dist_name}-{config.git.tag_version}.*'
    binary_asset_patterns: []
    deploy_labels:
      - main
      - test
      - dev
    commands:
      # Clumsy way of calling hub, just to make it possible for multiple
      # packages to upload assets. The create command will fail from the second
      # time it is called. By calling create once succesfully, all subsequent
      # edit calls will work fine.
      - if [[ "{config.git.deploy_label}" == "main" ]]; then
            hub release create
            -m "Automatic release of version {config.git.tag_version}"
            {config.git.tag_version};
        else
            hub release create -p
            -m "Automatic release of version {config.git.tag_version}"
            {config.git.tag_version};
        fi
      - hub release edit
            -m "Automatic release of version {config.git.tag_version}"
            {hub_assets} {config.git.tag_version}

  # Upload conda packages to anaconda.org
  deploy-conda:
    task: deploy
    conda_requirements:
      - anaconda-client
    deploy_vars:
      - ANACONDA_API_TOKEN
    noarch_asset_patterns:
      - '{config.conda.build_path}/noarch/{package.dist_name}-{config.git.tag_version}-*.*'
    binary_asset_patterns:
      - '{config.conda.build_path}/linux-64/{package.dist_name}-{config.git.tag_version}-*.*'
      - '{config.conda.build_path}/osx-64/{package.dist_name}-{config.git.tag_version}-*.*'
    deploy_labels:
      - main
      - test
      - dev
    commands:
      - 'anaconda -v upload --force -l {config.git.deploy_label} {assets}'
