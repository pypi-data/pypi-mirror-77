# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import warnings
import pulumi
import pulumi.runtime
from typing import Union
from .. import utilities, tables


class SparkPool(pulumi.CustomResource):
    auto_pause: pulumi.Output[dict]
    """
    An `auto_pause` block as defined below.

      * `delayInMinutes` (`float`) - Number of minutes of idle time before the Spark Pool is automatically paused. Must be between `5` and `10080`.
    """
    auto_scale: pulumi.Output[dict]
    """
    An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.

      * `maxNodeCount` (`float`) - The maximum number of nodes the Spark Pool can support. Must be between `3` and `200`.
      * `minNodeCount` (`float`) - The minimum number of nodes the Spark Pool can support. Must be between `3` and `200`.
    """
    library_requirement: pulumi.Output[dict]
    """
    A `library_requirement` block as defined below.

      * `content` (`str`) - The content of library requirements.
      * `filename` (`str`) - The name of the library requirements file.
    """
    name: pulumi.Output[str]
    """
    The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
    """
    node_count: pulumi.Output[float]
    """
    The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.
    """
    node_size: pulumi.Output[str]
    """
    The level of node in the Spark Pool. Possible value is `Small`, `Medium` and `Large`.
    """
    node_size_family: pulumi.Output[str]
    """
    The kind of nodes that the Spark Pool provides. Possible value is `MemoryOptimized`.
    """
    spark_events_folder: pulumi.Output[str]
    """
    The Spark events folder. Defaults to `/events`.
    """
    spark_log_folder: pulumi.Output[str]
    """
    The default folder where Spark logs will be written. Defaults to `/logs`.
    """
    spark_version: pulumi.Output[str]
    """
    The Apache Spark version. Possible value is `2.4`. Defaults to `2.4`.
    """
    synapse_workspace_id: pulumi.Output[str]
    """
    The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
    """
    tags: pulumi.Output[dict]
    """
    A mapping of tags which should be assigned to the Synapse Spark Pool.
    """
    def __init__(__self__, resource_name, opts=None, auto_pause=None, auto_scale=None, library_requirement=None, name=None, node_count=None, node_size=None, node_size_family=None, spark_events_folder=None, spark_log_folder=None, spark_version=None, synapse_workspace_id=None, tags=None, __props__=None, __name__=None, __opts__=None):
        """
        Manages a Synapse Spark Pool.

        ## Example Usage

        ```python
        import pulumi
        import pulumi_azure as azure

        example_resource_group = azure.core.ResourceGroup("exampleResourceGroup", location="West Europe")
        example_account = azure.storage.Account("exampleAccount",
            resource_group_name=example_resource_group.name,
            location=example_resource_group.location,
            account_tier="Standard",
            account_replication_type="LRS",
            account_kind="StorageV2",
            is_hns_enabled="true")
        example_data_lake_gen2_filesystem = azure.storage.DataLakeGen2Filesystem("exampleDataLakeGen2Filesystem", storage_account_id=example_account.id)
        example_workspace = azure.synapse.Workspace("exampleWorkspace",
            resource_group_name=example_resource_group.name,
            location=example_resource_group.location,
            storage_data_lake_gen2_filesystem_id=example_data_lake_gen2_filesystem.id,
            sql_administrator_login="sqladminuser",
            sql_administrator_login_password="H@Sh1CoR3!")
        example_spark_pool = azure.synapse.SparkPool("exampleSparkPool",
            synapse_workspace_id=example_workspace.id,
            node_size_family="MemoryOptimized",
            node_size="Small",
            auto_scale={
                "maxNodeCount": 50,
                "minNodeCount": 3,
            },
            auto_pause={
                "delayInMinutes": 15,
            },
            tags={
                "ENV": "Production",
            })
        ```

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[dict] auto_pause: An `auto_pause` block as defined below.
        :param pulumi.Input[dict] auto_scale: An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.
        :param pulumi.Input[dict] library_requirement: A `library_requirement` block as defined below.
        :param pulumi.Input[str] name: The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
        :param pulumi.Input[float] node_count: The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.
        :param pulumi.Input[str] node_size: The level of node in the Spark Pool. Possible value is `Small`, `Medium` and `Large`.
        :param pulumi.Input[str] node_size_family: The kind of nodes that the Spark Pool provides. Possible value is `MemoryOptimized`.
        :param pulumi.Input[str] spark_events_folder: The Spark events folder. Defaults to `/events`.
        :param pulumi.Input[str] spark_log_folder: The default folder where Spark logs will be written. Defaults to `/logs`.
        :param pulumi.Input[str] spark_version: The Apache Spark version. Possible value is `2.4`. Defaults to `2.4`.
        :param pulumi.Input[str] synapse_workspace_id: The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
        :param pulumi.Input[dict] tags: A mapping of tags which should be assigned to the Synapse Spark Pool.

        The **auto_pause** object supports the following:

          * `delayInMinutes` (`pulumi.Input[float]`) - Number of minutes of idle time before the Spark Pool is automatically paused. Must be between `5` and `10080`.

        The **auto_scale** object supports the following:

          * `maxNodeCount` (`pulumi.Input[float]`) - The maximum number of nodes the Spark Pool can support. Must be between `3` and `200`.
          * `minNodeCount` (`pulumi.Input[float]`) - The minimum number of nodes the Spark Pool can support. Must be between `3` and `200`.

        The **library_requirement** object supports the following:

          * `content` (`pulumi.Input[str]`) - The content of library requirements.
          * `filename` (`pulumi.Input[str]`) - The name of the library requirements file.
        """
        if __name__ is not None:
            warnings.warn("explicit use of __name__ is deprecated", DeprecationWarning)
            resource_name = __name__
        if __opts__ is not None:
            warnings.warn("explicit use of __opts__ is deprecated, use 'opts' instead", DeprecationWarning)
            opts = __opts__
        if opts is None:
            opts = pulumi.ResourceOptions()
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.version is None:
            opts.version = utilities.get_version()
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = dict()

            __props__['auto_pause'] = auto_pause
            __props__['auto_scale'] = auto_scale
            __props__['library_requirement'] = library_requirement
            __props__['name'] = name
            __props__['node_count'] = node_count
            if node_size is None:
                raise TypeError("Missing required property 'node_size'")
            __props__['node_size'] = node_size
            if node_size_family is None:
                raise TypeError("Missing required property 'node_size_family'")
            __props__['node_size_family'] = node_size_family
            __props__['spark_events_folder'] = spark_events_folder
            __props__['spark_log_folder'] = spark_log_folder
            __props__['spark_version'] = spark_version
            if synapse_workspace_id is None:
                raise TypeError("Missing required property 'synapse_workspace_id'")
            __props__['synapse_workspace_id'] = synapse_workspace_id
            __props__['tags'] = tags
        super(SparkPool, __self__).__init__(
            'azure:synapse/sparkPool:SparkPool',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name, id, opts=None, auto_pause=None, auto_scale=None, library_requirement=None, name=None, node_count=None, node_size=None, node_size_family=None, spark_events_folder=None, spark_log_folder=None, spark_version=None, synapse_workspace_id=None, tags=None):
        """
        Get an existing SparkPool resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param str id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[dict] auto_pause: An `auto_pause` block as defined below.
        :param pulumi.Input[dict] auto_scale: An `auto_scale` block as defined below. Exactly one of `node_count` or `auto_scale` must be specified.
        :param pulumi.Input[dict] library_requirement: A `library_requirement` block as defined below.
        :param pulumi.Input[str] name: The name which should be used for this Synapse Spark Pool. Changing this forces a new Synapse Spark Pool to be created.
        :param pulumi.Input[float] node_count: The number of nodes in the Spark Pool. Exactly one of `node_count` or `auto_scale` must be specified.
        :param pulumi.Input[str] node_size: The level of node in the Spark Pool. Possible value is `Small`, `Medium` and `Large`.
        :param pulumi.Input[str] node_size_family: The kind of nodes that the Spark Pool provides. Possible value is `MemoryOptimized`.
        :param pulumi.Input[str] spark_events_folder: The Spark events folder. Defaults to `/events`.
        :param pulumi.Input[str] spark_log_folder: The default folder where Spark logs will be written. Defaults to `/logs`.
        :param pulumi.Input[str] spark_version: The Apache Spark version. Possible value is `2.4`. Defaults to `2.4`.
        :param pulumi.Input[str] synapse_workspace_id: The ID of the Synapse Workspace where the Synapse Spark Pool should exist. Changing this forces a new Synapse Spark Pool to be created.
        :param pulumi.Input[dict] tags: A mapping of tags which should be assigned to the Synapse Spark Pool.

        The **auto_pause** object supports the following:

          * `delayInMinutes` (`pulumi.Input[float]`) - Number of minutes of idle time before the Spark Pool is automatically paused. Must be between `5` and `10080`.

        The **auto_scale** object supports the following:

          * `maxNodeCount` (`pulumi.Input[float]`) - The maximum number of nodes the Spark Pool can support. Must be between `3` and `200`.
          * `minNodeCount` (`pulumi.Input[float]`) - The minimum number of nodes the Spark Pool can support. Must be between `3` and `200`.

        The **library_requirement** object supports the following:

          * `content` (`pulumi.Input[str]`) - The content of library requirements.
          * `filename` (`pulumi.Input[str]`) - The name of the library requirements file.
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = dict()

        __props__["auto_pause"] = auto_pause
        __props__["auto_scale"] = auto_scale
        __props__["library_requirement"] = library_requirement
        __props__["name"] = name
        __props__["node_count"] = node_count
        __props__["node_size"] = node_size
        __props__["node_size_family"] = node_size_family
        __props__["spark_events_folder"] = spark_events_folder
        __props__["spark_log_folder"] = spark_log_folder
        __props__["spark_version"] = spark_version
        __props__["synapse_workspace_id"] = synapse_workspace_id
        __props__["tags"] = tags
        return SparkPool(resource_name, opts=opts, __props__=__props__)

    def translate_output_property(self, prop):
        return tables._CAMEL_TO_SNAKE_CASE_TABLE.get(prop) or prop

    def translate_input_property(self, prop):
        return tables._SNAKE_TO_CAMEL_CASE_TABLE.get(prop) or prop
