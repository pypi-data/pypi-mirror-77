# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: model_config.proto

import sys
_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
from google.protobuf.internal import enum_type_wrapper
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='model_config.proto',
  package='nvidia.inferenceserver',
  syntax='proto3',
  serialized_options=None,
  serialized_pb=_b('\n\x12model_config.proto\x12\x16nvidia.inferenceserver\"\xd2\x01\n\x12ModelInstanceGroup\x12\x0c\n\x04name\x18\x01 \x01(\t\x12=\n\x04kind\x18\x04 \x01(\x0e\x32/.nvidia.inferenceserver.ModelInstanceGroup.Kind\x12\r\n\x05\x63ount\x18\x02 \x01(\x05\x12\x0c\n\x04gpus\x18\x03 \x03(\x05\x12\x0f\n\x07profile\x18\x05 \x03(\t\"A\n\x04Kind\x12\r\n\tKIND_AUTO\x10\x00\x12\x0c\n\x08KIND_GPU\x10\x01\x12\x0c\n\x08KIND_CPU\x10\x02\x12\x0e\n\nKIND_MODEL\x10\x03\"#\n\x12ModelTensorReshape\x12\r\n\x05shape\x18\x01 \x03(\x03\"\xc7\x02\n\nModelInput\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x33\n\tdata_type\x18\x02 \x01(\x0e\x32 .nvidia.inferenceserver.DataType\x12\x39\n\x06\x66ormat\x18\x03 \x01(\x0e\x32).nvidia.inferenceserver.ModelInput.Format\x12\x0c\n\x04\x64ims\x18\x04 \x03(\x03\x12;\n\x07reshape\x18\x05 \x01(\x0b\x32*.nvidia.inferenceserver.ModelTensorReshape\x12\x17\n\x0fis_shape_tensor\x18\x06 \x01(\x08\x12\x1a\n\x12\x61llow_ragged_batch\x18\x07 \x01(\x08\";\n\x06\x46ormat\x12\x0f\n\x0b\x46ORMAT_NONE\x10\x00\x12\x0f\n\x0b\x46ORMAT_NHWC\x10\x01\x12\x0f\n\x0b\x46ORMAT_NCHW\x10\x02\"\xcc\x01\n\x0bModelOutput\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x33\n\tdata_type\x18\x02 \x01(\x0e\x32 .nvidia.inferenceserver.DataType\x12\x0c\n\x04\x64ims\x18\x03 \x03(\x03\x12;\n\x07reshape\x18\x05 \x01(\x0b\x32*.nvidia.inferenceserver.ModelTensorReshape\x12\x16\n\x0elabel_filename\x18\x04 \x01(\t\x12\x17\n\x0fis_shape_tensor\x18\x06 \x01(\x08\"\xb7\x02\n\x12ModelVersionPolicy\x12\x43\n\x06latest\x18\x01 \x01(\x0b\x32\x31.nvidia.inferenceserver.ModelVersionPolicy.LatestH\x00\x12=\n\x03\x61ll\x18\x02 \x01(\x0b\x32..nvidia.inferenceserver.ModelVersionPolicy.AllH\x00\x12G\n\x08specific\x18\x03 \x01(\x0b\x32\x33.nvidia.inferenceserver.ModelVersionPolicy.SpecificH\x00\x1a\x1e\n\x06Latest\x12\x14\n\x0cnum_versions\x18\x01 \x01(\r\x1a\x05\n\x03\x41ll\x1a\x1c\n\x08Specific\x12\x10\n\x08versions\x18\x01 \x03(\x03\x42\x0f\n\rpolicy_choice\"\x8d\t\n\x17ModelOptimizationPolicy\x12\x44\n\x05graph\x18\x01 \x01(\x0b\x32\x35.nvidia.inferenceserver.ModelOptimizationPolicy.Graph\x12O\n\x08priority\x18\x02 \x01(\x0e\x32=.nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority\x12\x42\n\x04\x63uda\x18\x03 \x01(\x0b\x32\x34.nvidia.inferenceserver.ModelOptimizationPolicy.Cuda\x12\x65\n\x16\x65xecution_accelerators\x18\x04 \x01(\x0b\x32\x45.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators\x12_\n\x13input_pinned_memory\x18\x05 \x01(\x0b\x32\x42.nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer\x12`\n\x14output_pinned_memory\x18\x06 \x01(\x0b\x32\x42.nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer\x1a\x16\n\x05Graph\x12\r\n\x05level\x18\x01 \x01(\x05\x1a\x16\n\x04\x43uda\x12\x0e\n\x06graphs\x18\x01 \x01(\x08\x1a\xcb\x03\n\x15\x45xecutionAccelerators\x12t\n\x19gpu_execution_accelerator\x18\x01 \x03(\x0b\x32Q.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator\x12t\n\x19\x63pu_execution_accelerator\x18\x02 \x03(\x0b\x32Q.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator\x1a\xc5\x01\n\x0b\x41\x63\x63\x65lerator\x12\x0c\n\x04name\x18\x01 \x01(\t\x12u\n\nparameters\x18\x02 \x03(\x0b\x32\x61.nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry\x1a\x31\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1a$\n\x12PinnedMemoryBuffer\x12\x0e\n\x06\x65nable\x18\x01 \x01(\x08\"I\n\rModelPriority\x12\x14\n\x10PRIORITY_DEFAULT\x10\x00\x12\x10\n\x0cPRIORITY_MAX\x10\x01\x12\x10\n\x0cPRIORITY_MIN\x10\x02\"u\n\x14ModelDynamicBatching\x12\x1c\n\x14preferred_batch_size\x18\x01 \x03(\x05\x12$\n\x1cmax_queue_delay_microseconds\x18\x02 \x01(\x04\x12\x19\n\x11preserve_ordering\x18\x03 \x01(\x08\"\xe9\x06\n\x15ModelSequenceBatching\x12N\n\x06\x64irect\x18\x03 \x01(\x0b\x32<.nvidia.inferenceserver.ModelSequenceBatching.StrategyDirectH\x00\x12N\n\x06oldest\x18\x04 \x01(\x0b\x32<.nvidia.inferenceserver.ModelSequenceBatching.StrategyOldestH\x00\x12&\n\x1emax_sequence_idle_microseconds\x18\x01 \x01(\x04\x12Q\n\rcontrol_input\x18\x02 \x03(\x0b\x32:.nvidia.inferenceserver.ModelSequenceBatching.ControlInput\x1a\xb2\x02\n\x07\x43ontrol\x12H\n\x04kind\x18\x01 \x01(\x0e\x32:.nvidia.inferenceserver.ModelSequenceBatching.Control.Kind\x12\x18\n\x10int32_false_true\x18\x02 \x03(\x05\x12\x17\n\x0f\x66p32_false_true\x18\x03 \x03(\x02\x12\x33\n\tdata_type\x18\x04 \x01(\x0e\x32 .nvidia.inferenceserver.DataType\"u\n\x04Kind\x12\x1a\n\x16\x43ONTROL_SEQUENCE_START\x10\x00\x12\x1a\n\x16\x43ONTROL_SEQUENCE_READY\x10\x01\x12\x18\n\x14\x43ONTROL_SEQUENCE_END\x10\x02\x12\x1b\n\x17\x43ONTROL_SEQUENCE_CORRID\x10\x03\x1a\x64\n\x0c\x43ontrolInput\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x46\n\x07\x63ontrol\x18\x02 \x03(\x0b\x32\x35.nvidia.inferenceserver.ModelSequenceBatching.Control\x1a\x10\n\x0eStrategyDirect\x1au\n\x0eStrategyOldest\x12\x1f\n\x17max_candidate_sequences\x18\x01 \x01(\x05\x12\x1c\n\x14preferred_batch_size\x18\x02 \x03(\x05\x12$\n\x1cmax_queue_delay_microseconds\x18\x03 \x01(\x04\x42\x11\n\x0fstrategy_choice\"\x84\x03\n\x0fModelEnsembling\x12:\n\x04step\x18\x01 \x03(\x0b\x32,.nvidia.inferenceserver.ModelEnsembling.Step\x1a\xb4\x02\n\x04Step\x12\x12\n\nmodel_name\x18\x01 \x01(\t\x12\x15\n\rmodel_version\x18\x02 \x01(\x03\x12M\n\tinput_map\x18\x03 \x03(\x0b\x32:.nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry\x12O\n\noutput_map\x18\x04 \x03(\x0b\x32;.nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry\x1a/\n\rInputMapEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1a\x30\n\x0eOutputMapEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\"&\n\x0eModelParameter\x12\x14\n\x0cstring_value\x18\x01 \x01(\t\"\xf1\x02\n\x0bModelWarmup\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x12\n\nbatch_size\x18\x02 \x01(\r\x12?\n\x06inputs\x18\x03 \x03(\x0b\x32/.nvidia.inferenceserver.ModelWarmup.InputsEntry\x1a\xa4\x01\n\x05Input\x12\x33\n\tdata_type\x18\x01 \x01(\x0e\x32 .nvidia.inferenceserver.DataType\x12\x0c\n\x04\x64ims\x18\x02 \x03(\x03\x12\x13\n\tzero_data\x18\x03 \x01(\x08H\x00\x12\x15\n\x0brandom_data\x18\x04 \x01(\x08H\x00\x12\x19\n\x0finput_data_file\x18\x05 \x01(\tH\x00\x42\x11\n\x0finput_data_type\x1aX\n\x0bInputsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x38\n\x05value\x18\x02 \x01(\x0b\x32).nvidia.inferenceserver.ModelWarmup.Input:\x02\x38\x01\"\xfb\x08\n\x0bModelConfig\x12\x0c\n\x04name\x18\x01 \x01(\t\x12\x10\n\x08platform\x18\x02 \x01(\t\x12\x42\n\x0eversion_policy\x18\x03 \x01(\x0b\x32*.nvidia.inferenceserver.ModelVersionPolicy\x12\x16\n\x0emax_batch_size\x18\x04 \x01(\x05\x12\x31\n\x05input\x18\x05 \x03(\x0b\x32\".nvidia.inferenceserver.ModelInput\x12\x33\n\x06output\x18\x06 \x03(\x0b\x32#.nvidia.inferenceserver.ModelOutput\x12\x45\n\x0coptimization\x18\x0c \x01(\x0b\x32/.nvidia.inferenceserver.ModelOptimizationPolicy\x12H\n\x10\x64ynamic_batching\x18\x0b \x01(\x0b\x32,.nvidia.inferenceserver.ModelDynamicBatchingH\x00\x12J\n\x11sequence_batching\x18\r \x01(\x0b\x32-.nvidia.inferenceserver.ModelSequenceBatchingH\x00\x12\x46\n\x13\x65nsemble_scheduling\x18\x0f \x01(\x0b\x32\'.nvidia.inferenceserver.ModelEnsemblingH\x00\x12\x42\n\x0einstance_group\x18\x07 \x03(\x0b\x32*.nvidia.inferenceserver.ModelInstanceGroup\x12\x1e\n\x16\x64\x65\x66\x61ult_model_filename\x18\x08 \x01(\t\x12U\n\x12\x63\x63_model_filenames\x18\t \x03(\x0b\x32\x39.nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry\x12H\n\x0bmetric_tags\x18\n \x03(\x0b\x32\x33.nvidia.inferenceserver.ModelConfig.MetricTagsEntry\x12G\n\nparameters\x18\x0e \x03(\x0b\x32\x33.nvidia.inferenceserver.ModelConfig.ParametersEntry\x12\x39\n\x0cmodel_warmup\x18\x10 \x03(\x0b\x32#.nvidia.inferenceserver.ModelWarmup\x1a\x37\n\x15\x43\x63ModelFilenamesEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1a\x31\n\x0fMetricTagsEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\r\n\x05value\x18\x02 \x01(\t:\x02\x38\x01\x1aY\n\x0fParametersEntry\x12\x0b\n\x03key\x18\x01 \x01(\t\x12\x35\n\x05value\x18\x02 \x01(\x0b\x32&.nvidia.inferenceserver.ModelParameter:\x02\x38\x01\x42\x13\n\x11scheduling_choice*\xeb\x01\n\x08\x44\x61taType\x12\x10\n\x0cTYPE_INVALID\x10\x00\x12\r\n\tTYPE_BOOL\x10\x01\x12\x0e\n\nTYPE_UINT8\x10\x02\x12\x0f\n\x0bTYPE_UINT16\x10\x03\x12\x0f\n\x0bTYPE_UINT32\x10\x04\x12\x0f\n\x0bTYPE_UINT64\x10\x05\x12\r\n\tTYPE_INT8\x10\x06\x12\x0e\n\nTYPE_INT16\x10\x07\x12\x0e\n\nTYPE_INT32\x10\x08\x12\x0e\n\nTYPE_INT64\x10\t\x12\r\n\tTYPE_FP16\x10\n\x12\r\n\tTYPE_FP32\x10\x0b\x12\r\n\tTYPE_FP64\x10\x0c\x12\x0f\n\x0bTYPE_STRING\x10\rb\x06proto3')
)

_DATATYPE = _descriptor.EnumDescriptor(
  name='DataType',
  full_name='nvidia.inferenceserver.DataType',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='TYPE_INVALID', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_BOOL', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT8', index=2, number=2,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT16', index=3, number=3,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT32', index=4, number=4,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_UINT64', index=5, number=5,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT8', index=6, number=6,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT16', index=7, number=7,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT32', index=8, number=8,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_INT64', index=9, number=9,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_FP16', index=10, number=10,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_FP32', index=11, number=11,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_FP64', index=12, number=12,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='TYPE_STRING', index=13, number=13,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=5264,
  serialized_end=5499,
)
_sym_db.RegisterEnumDescriptor(_DATATYPE)

DataType = enum_type_wrapper.EnumTypeWrapper(_DATATYPE)
TYPE_INVALID = 0
TYPE_BOOL = 1
TYPE_UINT8 = 2
TYPE_UINT16 = 3
TYPE_UINT32 = 4
TYPE_UINT64 = 5
TYPE_INT8 = 6
TYPE_INT16 = 7
TYPE_INT32 = 8
TYPE_INT64 = 9
TYPE_FP16 = 10
TYPE_FP32 = 11
TYPE_FP64 = 12
TYPE_STRING = 13


_MODELINSTANCEGROUP_KIND = _descriptor.EnumDescriptor(
  name='Kind',
  full_name='nvidia.inferenceserver.ModelInstanceGroup.Kind',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='KIND_AUTO', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='KIND_GPU', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='KIND_CPU', index=2, number=2,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='KIND_MODEL', index=3, number=3,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=192,
  serialized_end=257,
)
_sym_db.RegisterEnumDescriptor(_MODELINSTANCEGROUP_KIND)

_MODELINPUT_FORMAT = _descriptor.EnumDescriptor(
  name='Format',
  full_name='nvidia.inferenceserver.ModelInput.Format',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='FORMAT_NONE', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='FORMAT_NHWC', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='FORMAT_NCHW', index=2, number=2,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=565,
  serialized_end=624,
)
_sym_db.RegisterEnumDescriptor(_MODELINPUT_FORMAT)

_MODELOPTIMIZATIONPOLICY_MODELPRIORITY = _descriptor.EnumDescriptor(
  name='ModelPriority',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ModelPriority',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='PRIORITY_DEFAULT', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='PRIORITY_MAX', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='PRIORITY_MIN', index=2, number=2,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=2240,
  serialized_end=2313,
)
_sym_db.RegisterEnumDescriptor(_MODELOPTIMIZATIONPOLICY_MODELPRIORITY)

_MODELSEQUENCEBATCHING_CONTROL_KIND = _descriptor.EnumDescriptor(
  name='Kind',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.Kind',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='CONTROL_SEQUENCE_START', index=0, number=0,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='CONTROL_SEQUENCE_READY', index=1, number=1,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='CONTROL_SEQUENCE_END', index=2, number=2,
      serialized_options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='CONTROL_SEQUENCE_CORRID', index=3, number=3,
      serialized_options=None,
      type=None),
  ],
  containing_type=None,
  serialized_options=None,
  serialized_start=2933,
  serialized_end=3050,
)
_sym_db.RegisterEnumDescriptor(_MODELSEQUENCEBATCHING_CONTROL_KIND)


_MODELINSTANCEGROUP = _descriptor.Descriptor(
  name='ModelInstanceGroup',
  full_name='nvidia.inferenceserver.ModelInstanceGroup',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInstanceGroup.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='kind', full_name='nvidia.inferenceserver.ModelInstanceGroup.kind', index=1,
      number=4, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='count', full_name='nvidia.inferenceserver.ModelInstanceGroup.count', index=2,
      number=2, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='gpus', full_name='nvidia.inferenceserver.ModelInstanceGroup.gpus', index=3,
      number=3, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='profile', full_name='nvidia.inferenceserver.ModelInstanceGroup.profile', index=4,
      number=5, type=9, cpp_type=9, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _MODELINSTANCEGROUP_KIND,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=47,
  serialized_end=257,
)


_MODELTENSORRESHAPE = _descriptor.Descriptor(
  name='ModelTensorReshape',
  full_name='nvidia.inferenceserver.ModelTensorReshape',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='shape', full_name='nvidia.inferenceserver.ModelTensorReshape.shape', index=0,
      number=1, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=259,
  serialized_end=294,
)


_MODELINPUT = _descriptor.Descriptor(
  name='ModelInput',
  full_name='nvidia.inferenceserver.ModelInput',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelInput.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='data_type', full_name='nvidia.inferenceserver.ModelInput.data_type', index=1,
      number=2, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='format', full_name='nvidia.inferenceserver.ModelInput.format', index=2,
      number=3, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='dims', full_name='nvidia.inferenceserver.ModelInput.dims', index=3,
      number=4, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='reshape', full_name='nvidia.inferenceserver.ModelInput.reshape', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='is_shape_tensor', full_name='nvidia.inferenceserver.ModelInput.is_shape_tensor', index=5,
      number=6, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='allow_ragged_batch', full_name='nvidia.inferenceserver.ModelInput.allow_ragged_batch', index=6,
      number=7, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _MODELINPUT_FORMAT,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=297,
  serialized_end=624,
)


_MODELOUTPUT = _descriptor.Descriptor(
  name='ModelOutput',
  full_name='nvidia.inferenceserver.ModelOutput',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelOutput.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='data_type', full_name='nvidia.inferenceserver.ModelOutput.data_type', index=1,
      number=2, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='dims', full_name='nvidia.inferenceserver.ModelOutput.dims', index=2,
      number=3, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='reshape', full_name='nvidia.inferenceserver.ModelOutput.reshape', index=3,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='label_filename', full_name='nvidia.inferenceserver.ModelOutput.label_filename', index=4,
      number=4, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='is_shape_tensor', full_name='nvidia.inferenceserver.ModelOutput.is_shape_tensor', index=5,
      number=6, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=627,
  serialized_end=831,
)


_MODELVERSIONPOLICY_LATEST = _descriptor.Descriptor(
  name='Latest',
  full_name='nvidia.inferenceserver.ModelVersionPolicy.Latest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='num_versions', full_name='nvidia.inferenceserver.ModelVersionPolicy.Latest.num_versions', index=0,
      number=1, type=13, cpp_type=3, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1061,
  serialized_end=1091,
)

_MODELVERSIONPOLICY_ALL = _descriptor.Descriptor(
  name='All',
  full_name='nvidia.inferenceserver.ModelVersionPolicy.All',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1093,
  serialized_end=1098,
)

_MODELVERSIONPOLICY_SPECIFIC = _descriptor.Descriptor(
  name='Specific',
  full_name='nvidia.inferenceserver.ModelVersionPolicy.Specific',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='versions', full_name='nvidia.inferenceserver.ModelVersionPolicy.Specific.versions', index=0,
      number=1, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1100,
  serialized_end=1128,
)

_MODELVERSIONPOLICY = _descriptor.Descriptor(
  name='ModelVersionPolicy',
  full_name='nvidia.inferenceserver.ModelVersionPolicy',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='latest', full_name='nvidia.inferenceserver.ModelVersionPolicy.latest', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='all', full_name='nvidia.inferenceserver.ModelVersionPolicy.all', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='specific', full_name='nvidia.inferenceserver.ModelVersionPolicy.specific', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELVERSIONPOLICY_LATEST, _MODELVERSIONPOLICY_ALL, _MODELVERSIONPOLICY_SPECIFIC, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='policy_choice', full_name='nvidia.inferenceserver.ModelVersionPolicy.policy_choice',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=834,
  serialized_end=1145,
)


_MODELOPTIMIZATIONPOLICY_GRAPH = _descriptor.Descriptor(
  name='Graph',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.Graph',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='level', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.Graph.level', index=0,
      number=1, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1692,
  serialized_end=1714,
)

_MODELOPTIMIZATIONPOLICY_CUDA = _descriptor.Descriptor(
  name='Cuda',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.Cuda',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='graphs', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.Cuda.graphs', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1716,
  serialized_end=1738,
)

_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2151,
  serialized_end=2200,
)

_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR = _descriptor.Descriptor(
  name='Accelerator',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.parameters', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2003,
  serialized_end=2200,
)

_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS = _descriptor.Descriptor(
  name='ExecutionAccelerators',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='gpu_execution_accelerator', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.gpu_execution_accelerator', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='cpu_execution_accelerator', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.cpu_execution_accelerator', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1741,
  serialized_end=2200,
)

_MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER = _descriptor.Descriptor(
  name='PinnedMemoryBuffer',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='enable', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer.enable', index=0,
      number=1, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2202,
  serialized_end=2238,
)

_MODELOPTIMIZATIONPOLICY = _descriptor.Descriptor(
  name='ModelOptimizationPolicy',
  full_name='nvidia.inferenceserver.ModelOptimizationPolicy',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='graph', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.graph', index=0,
      number=1, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='priority', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.priority', index=1,
      number=2, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='cuda', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.cuda', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='execution_accelerators', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.execution_accelerators', index=3,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='input_pinned_memory', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.input_pinned_memory', index=4,
      number=5, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='output_pinned_memory', full_name='nvidia.inferenceserver.ModelOptimizationPolicy.output_pinned_memory', index=5,
      number=6, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELOPTIMIZATIONPOLICY_GRAPH, _MODELOPTIMIZATIONPOLICY_CUDA, _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS, _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER, ],
  enum_types=[
    _MODELOPTIMIZATIONPOLICY_MODELPRIORITY,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1148,
  serialized_end=2313,
)


_MODELDYNAMICBATCHING = _descriptor.Descriptor(
  name='ModelDynamicBatching',
  full_name='nvidia.inferenceserver.ModelDynamicBatching',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='preferred_batch_size', full_name='nvidia.inferenceserver.ModelDynamicBatching.preferred_batch_size', index=0,
      number=1, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_queue_delay_microseconds', full_name='nvidia.inferenceserver.ModelDynamicBatching.max_queue_delay_microseconds', index=1,
      number=2, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='preserve_ordering', full_name='nvidia.inferenceserver.ModelDynamicBatching.preserve_ordering', index=2,
      number=3, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2315,
  serialized_end=2432,
)


_MODELSEQUENCEBATCHING_CONTROL = _descriptor.Descriptor(
  name='Control',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.Control',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='kind', full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.kind', index=0,
      number=1, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='int32_false_true', full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.int32_false_true', index=1,
      number=2, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='fp32_false_true', full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.fp32_false_true', index=2,
      number=3, type=2, cpp_type=6, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='data_type', full_name='nvidia.inferenceserver.ModelSequenceBatching.Control.data_type', index=3,
      number=4, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
    _MODELSEQUENCEBATCHING_CONTROL_KIND,
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=2744,
  serialized_end=3050,
)

_MODELSEQUENCEBATCHING_CONTROLINPUT = _descriptor.Descriptor(
  name='ControlInput',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.ControlInput',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelSequenceBatching.ControlInput.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='control', full_name='nvidia.inferenceserver.ModelSequenceBatching.ControlInput.control', index=1,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3052,
  serialized_end=3152,
)

_MODELSEQUENCEBATCHING_STRATEGYDIRECT = _descriptor.Descriptor(
  name='StrategyDirect',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3154,
  serialized_end=3170,
)

_MODELSEQUENCEBATCHING_STRATEGYOLDEST = _descriptor.Descriptor(
  name='StrategyOldest',
  full_name='nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='max_candidate_sequences', full_name='nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.max_candidate_sequences', index=0,
      number=1, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='preferred_batch_size', full_name='nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.preferred_batch_size', index=1,
      number=2, type=5, cpp_type=1, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_queue_delay_microseconds', full_name='nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest.max_queue_delay_microseconds', index=2,
      number=3, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3172,
  serialized_end=3289,
)

_MODELSEQUENCEBATCHING = _descriptor.Descriptor(
  name='ModelSequenceBatching',
  full_name='nvidia.inferenceserver.ModelSequenceBatching',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='direct', full_name='nvidia.inferenceserver.ModelSequenceBatching.direct', index=0,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='oldest', full_name='nvidia.inferenceserver.ModelSequenceBatching.oldest', index=1,
      number=4, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_sequence_idle_microseconds', full_name='nvidia.inferenceserver.ModelSequenceBatching.max_sequence_idle_microseconds', index=2,
      number=1, type=4, cpp_type=4, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='control_input', full_name='nvidia.inferenceserver.ModelSequenceBatching.control_input', index=3,
      number=2, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELSEQUENCEBATCHING_CONTROL, _MODELSEQUENCEBATCHING_CONTROLINPUT, _MODELSEQUENCEBATCHING_STRATEGYDIRECT, _MODELSEQUENCEBATCHING_STRATEGYOLDEST, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='strategy_choice', full_name='nvidia.inferenceserver.ModelSequenceBatching.strategy_choice',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=2435,
  serialized_end=3308,
)


_MODELENSEMBLING_STEP_INPUTMAPENTRY = _descriptor.Descriptor(
  name='InputMapEntry',
  full_name='nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3602,
  serialized_end=3649,
)

_MODELENSEMBLING_STEP_OUTPUTMAPENTRY = _descriptor.Descriptor(
  name='OutputMapEntry',
  full_name='nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3651,
  serialized_end=3699,
)

_MODELENSEMBLING_STEP = _descriptor.Descriptor(
  name='Step',
  full_name='nvidia.inferenceserver.ModelEnsembling.Step',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='model_name', full_name='nvidia.inferenceserver.ModelEnsembling.Step.model_name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_version', full_name='nvidia.inferenceserver.ModelEnsembling.Step.model_version', index=1,
      number=2, type=3, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='input_map', full_name='nvidia.inferenceserver.ModelEnsembling.Step.input_map', index=2,
      number=3, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='output_map', full_name='nvidia.inferenceserver.ModelEnsembling.Step.output_map', index=3,
      number=4, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELENSEMBLING_STEP_INPUTMAPENTRY, _MODELENSEMBLING_STEP_OUTPUTMAPENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3391,
  serialized_end=3699,
)

_MODELENSEMBLING = _descriptor.Descriptor(
  name='ModelEnsembling',
  full_name='nvidia.inferenceserver.ModelEnsembling',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='step', full_name='nvidia.inferenceserver.ModelEnsembling.step', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELENSEMBLING_STEP, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3311,
  serialized_end=3699,
)


_MODELPARAMETER = _descriptor.Descriptor(
  name='ModelParameter',
  full_name='nvidia.inferenceserver.ModelParameter',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='string_value', full_name='nvidia.inferenceserver.ModelParameter.string_value', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3701,
  serialized_end=3739,
)


_MODELWARMUP_INPUT = _descriptor.Descriptor(
  name='Input',
  full_name='nvidia.inferenceserver.ModelWarmup.Input',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='data_type', full_name='nvidia.inferenceserver.ModelWarmup.Input.data_type', index=0,
      number=1, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='dims', full_name='nvidia.inferenceserver.ModelWarmup.Input.dims', index=1,
      number=2, type=3, cpp_type=2, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='zero_data', full_name='nvidia.inferenceserver.ModelWarmup.Input.zero_data', index=2,
      number=3, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='random_data', full_name='nvidia.inferenceserver.ModelWarmup.Input.random_data', index=3,
      number=4, type=8, cpp_type=7, label=1,
      has_default_value=False, default_value=False,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='input_data_file', full_name='nvidia.inferenceserver.ModelWarmup.Input.input_data_file', index=4,
      number=5, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='input_data_type', full_name='nvidia.inferenceserver.ModelWarmup.Input.input_data_type',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=3857,
  serialized_end=4021,
)

_MODELWARMUP_INPUTSENTRY = _descriptor.Descriptor(
  name='InputsEntry',
  full_name='nvidia.inferenceserver.ModelWarmup.InputsEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelWarmup.InputsEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelWarmup.InputsEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=4023,
  serialized_end=4111,
)

_MODELWARMUP = _descriptor.Descriptor(
  name='ModelWarmup',
  full_name='nvidia.inferenceserver.ModelWarmup',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelWarmup.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='batch_size', full_name='nvidia.inferenceserver.ModelWarmup.batch_size', index=1,
      number=2, type=13, cpp_type=3, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='inputs', full_name='nvidia.inferenceserver.ModelWarmup.inputs', index=2,
      number=3, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELWARMUP_INPUT, _MODELWARMUP_INPUTSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=3742,
  serialized_end=4111,
)


_MODELCONFIG_CCMODELFILENAMESENTRY = _descriptor.Descriptor(
  name='CcModelFilenamesEntry',
  full_name='nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=5043,
  serialized_end=5098,
)

_MODELCONFIG_METRICTAGSENTRY = _descriptor.Descriptor(
  name='MetricTagsEntry',
  full_name='nvidia.inferenceserver.ModelConfig.MetricTagsEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelConfig.MetricTagsEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelConfig.MetricTagsEntry.value', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=5100,
  serialized_end=5149,
)

_MODELCONFIG_PARAMETERSENTRY = _descriptor.Descriptor(
  name='ParametersEntry',
  full_name='nvidia.inferenceserver.ModelConfig.ParametersEntry',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='key', full_name='nvidia.inferenceserver.ModelConfig.ParametersEntry.key', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='value', full_name='nvidia.inferenceserver.ModelConfig.ParametersEntry.value', index=1,
      number=2, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  serialized_options=_b('8\001'),
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=5151,
  serialized_end=5240,
)

_MODELCONFIG = _descriptor.Descriptor(
  name='ModelConfig',
  full_name='nvidia.inferenceserver.ModelConfig',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='name', full_name='nvidia.inferenceserver.ModelConfig.name', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='platform', full_name='nvidia.inferenceserver.ModelConfig.platform', index=1,
      number=2, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='version_policy', full_name='nvidia.inferenceserver.ModelConfig.version_policy', index=2,
      number=3, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='max_batch_size', full_name='nvidia.inferenceserver.ModelConfig.max_batch_size', index=3,
      number=4, type=5, cpp_type=1, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='input', full_name='nvidia.inferenceserver.ModelConfig.input', index=4,
      number=5, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='output', full_name='nvidia.inferenceserver.ModelConfig.output', index=5,
      number=6, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='optimization', full_name='nvidia.inferenceserver.ModelConfig.optimization', index=6,
      number=12, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='dynamic_batching', full_name='nvidia.inferenceserver.ModelConfig.dynamic_batching', index=7,
      number=11, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='sequence_batching', full_name='nvidia.inferenceserver.ModelConfig.sequence_batching', index=8,
      number=13, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='ensemble_scheduling', full_name='nvidia.inferenceserver.ModelConfig.ensemble_scheduling', index=9,
      number=15, type=11, cpp_type=10, label=1,
      has_default_value=False, default_value=None,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='instance_group', full_name='nvidia.inferenceserver.ModelConfig.instance_group', index=10,
      number=7, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='default_model_filename', full_name='nvidia.inferenceserver.ModelConfig.default_model_filename', index=11,
      number=8, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='cc_model_filenames', full_name='nvidia.inferenceserver.ModelConfig.cc_model_filenames', index=12,
      number=9, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='metric_tags', full_name='nvidia.inferenceserver.ModelConfig.metric_tags', index=13,
      number=10, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='parameters', full_name='nvidia.inferenceserver.ModelConfig.parameters', index=14,
      number=14, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
    _descriptor.FieldDescriptor(
      name='model_warmup', full_name='nvidia.inferenceserver.ModelConfig.model_warmup', index=15,
      number=16, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      serialized_options=None, file=DESCRIPTOR),
  ],
  extensions=[
  ],
  nested_types=[_MODELCONFIG_CCMODELFILENAMESENTRY, _MODELCONFIG_METRICTAGSENTRY, _MODELCONFIG_PARAMETERSENTRY, ],
  enum_types=[
  ],
  serialized_options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
    _descriptor.OneofDescriptor(
      name='scheduling_choice', full_name='nvidia.inferenceserver.ModelConfig.scheduling_choice',
      index=0, containing_type=None, fields=[]),
  ],
  serialized_start=4114,
  serialized_end=5261,
)

_MODELINSTANCEGROUP.fields_by_name['kind'].enum_type = _MODELINSTANCEGROUP_KIND
_MODELINSTANCEGROUP_KIND.containing_type = _MODELINSTANCEGROUP
_MODELINPUT.fields_by_name['data_type'].enum_type = _DATATYPE
_MODELINPUT.fields_by_name['format'].enum_type = _MODELINPUT_FORMAT
_MODELINPUT.fields_by_name['reshape'].message_type = _MODELTENSORRESHAPE
_MODELINPUT_FORMAT.containing_type = _MODELINPUT
_MODELOUTPUT.fields_by_name['data_type'].enum_type = _DATATYPE
_MODELOUTPUT.fields_by_name['reshape'].message_type = _MODELTENSORRESHAPE
_MODELVERSIONPOLICY_LATEST.containing_type = _MODELVERSIONPOLICY
_MODELVERSIONPOLICY_ALL.containing_type = _MODELVERSIONPOLICY
_MODELVERSIONPOLICY_SPECIFIC.containing_type = _MODELVERSIONPOLICY
_MODELVERSIONPOLICY.fields_by_name['latest'].message_type = _MODELVERSIONPOLICY_LATEST
_MODELVERSIONPOLICY.fields_by_name['all'].message_type = _MODELVERSIONPOLICY_ALL
_MODELVERSIONPOLICY.fields_by_name['specific'].message_type = _MODELVERSIONPOLICY_SPECIFIC
_MODELVERSIONPOLICY.oneofs_by_name['policy_choice'].fields.append(
  _MODELVERSIONPOLICY.fields_by_name['latest'])
_MODELVERSIONPOLICY.fields_by_name['latest'].containing_oneof = _MODELVERSIONPOLICY.oneofs_by_name['policy_choice']
_MODELVERSIONPOLICY.oneofs_by_name['policy_choice'].fields.append(
  _MODELVERSIONPOLICY.fields_by_name['all'])
_MODELVERSIONPOLICY.fields_by_name['all'].containing_oneof = _MODELVERSIONPOLICY.oneofs_by_name['policy_choice']
_MODELVERSIONPOLICY.oneofs_by_name['policy_choice'].fields.append(
  _MODELVERSIONPOLICY.fields_by_name['specific'])
_MODELVERSIONPOLICY.fields_by_name['specific'].containing_oneof = _MODELVERSIONPOLICY.oneofs_by_name['policy_choice']
_MODELOPTIMIZATIONPOLICY_GRAPH.containing_type = _MODELOPTIMIZATIONPOLICY
_MODELOPTIMIZATIONPOLICY_CUDA.containing_type = _MODELOPTIMIZATIONPOLICY
_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY.containing_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR
_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR.fields_by_name['parameters'].message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY
_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR.containing_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS
_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS.fields_by_name['gpu_execution_accelerator'].message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR
_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS.fields_by_name['cpu_execution_accelerator'].message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR
_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS.containing_type = _MODELOPTIMIZATIONPOLICY
_MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER.containing_type = _MODELOPTIMIZATIONPOLICY
_MODELOPTIMIZATIONPOLICY.fields_by_name['graph'].message_type = _MODELOPTIMIZATIONPOLICY_GRAPH
_MODELOPTIMIZATIONPOLICY.fields_by_name['priority'].enum_type = _MODELOPTIMIZATIONPOLICY_MODELPRIORITY
_MODELOPTIMIZATIONPOLICY.fields_by_name['cuda'].message_type = _MODELOPTIMIZATIONPOLICY_CUDA
_MODELOPTIMIZATIONPOLICY.fields_by_name['execution_accelerators'].message_type = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS
_MODELOPTIMIZATIONPOLICY.fields_by_name['input_pinned_memory'].message_type = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER
_MODELOPTIMIZATIONPOLICY.fields_by_name['output_pinned_memory'].message_type = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER
_MODELOPTIMIZATIONPOLICY_MODELPRIORITY.containing_type = _MODELOPTIMIZATIONPOLICY
_MODELSEQUENCEBATCHING_CONTROL.fields_by_name['kind'].enum_type = _MODELSEQUENCEBATCHING_CONTROL_KIND
_MODELSEQUENCEBATCHING_CONTROL.fields_by_name['data_type'].enum_type = _DATATYPE
_MODELSEQUENCEBATCHING_CONTROL.containing_type = _MODELSEQUENCEBATCHING
_MODELSEQUENCEBATCHING_CONTROL_KIND.containing_type = _MODELSEQUENCEBATCHING_CONTROL
_MODELSEQUENCEBATCHING_CONTROLINPUT.fields_by_name['control'].message_type = _MODELSEQUENCEBATCHING_CONTROL
_MODELSEQUENCEBATCHING_CONTROLINPUT.containing_type = _MODELSEQUENCEBATCHING
_MODELSEQUENCEBATCHING_STRATEGYDIRECT.containing_type = _MODELSEQUENCEBATCHING
_MODELSEQUENCEBATCHING_STRATEGYOLDEST.containing_type = _MODELSEQUENCEBATCHING
_MODELSEQUENCEBATCHING.fields_by_name['direct'].message_type = _MODELSEQUENCEBATCHING_STRATEGYDIRECT
_MODELSEQUENCEBATCHING.fields_by_name['oldest'].message_type = _MODELSEQUENCEBATCHING_STRATEGYOLDEST
_MODELSEQUENCEBATCHING.fields_by_name['control_input'].message_type = _MODELSEQUENCEBATCHING_CONTROLINPUT
_MODELSEQUENCEBATCHING.oneofs_by_name['strategy_choice'].fields.append(
  _MODELSEQUENCEBATCHING.fields_by_name['direct'])
_MODELSEQUENCEBATCHING.fields_by_name['direct'].containing_oneof = _MODELSEQUENCEBATCHING.oneofs_by_name['strategy_choice']
_MODELSEQUENCEBATCHING.oneofs_by_name['strategy_choice'].fields.append(
  _MODELSEQUENCEBATCHING.fields_by_name['oldest'])
_MODELSEQUENCEBATCHING.fields_by_name['oldest'].containing_oneof = _MODELSEQUENCEBATCHING.oneofs_by_name['strategy_choice']
_MODELENSEMBLING_STEP_INPUTMAPENTRY.containing_type = _MODELENSEMBLING_STEP
_MODELENSEMBLING_STEP_OUTPUTMAPENTRY.containing_type = _MODELENSEMBLING_STEP
_MODELENSEMBLING_STEP.fields_by_name['input_map'].message_type = _MODELENSEMBLING_STEP_INPUTMAPENTRY
_MODELENSEMBLING_STEP.fields_by_name['output_map'].message_type = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY
_MODELENSEMBLING_STEP.containing_type = _MODELENSEMBLING
_MODELENSEMBLING.fields_by_name['step'].message_type = _MODELENSEMBLING_STEP
_MODELWARMUP_INPUT.fields_by_name['data_type'].enum_type = _DATATYPE
_MODELWARMUP_INPUT.containing_type = _MODELWARMUP
_MODELWARMUP_INPUT.oneofs_by_name['input_data_type'].fields.append(
  _MODELWARMUP_INPUT.fields_by_name['zero_data'])
_MODELWARMUP_INPUT.fields_by_name['zero_data'].containing_oneof = _MODELWARMUP_INPUT.oneofs_by_name['input_data_type']
_MODELWARMUP_INPUT.oneofs_by_name['input_data_type'].fields.append(
  _MODELWARMUP_INPUT.fields_by_name['random_data'])
_MODELWARMUP_INPUT.fields_by_name['random_data'].containing_oneof = _MODELWARMUP_INPUT.oneofs_by_name['input_data_type']
_MODELWARMUP_INPUT.oneofs_by_name['input_data_type'].fields.append(
  _MODELWARMUP_INPUT.fields_by_name['input_data_file'])
_MODELWARMUP_INPUT.fields_by_name['input_data_file'].containing_oneof = _MODELWARMUP_INPUT.oneofs_by_name['input_data_type']
_MODELWARMUP_INPUTSENTRY.fields_by_name['value'].message_type = _MODELWARMUP_INPUT
_MODELWARMUP_INPUTSENTRY.containing_type = _MODELWARMUP
_MODELWARMUP.fields_by_name['inputs'].message_type = _MODELWARMUP_INPUTSENTRY
_MODELCONFIG_CCMODELFILENAMESENTRY.containing_type = _MODELCONFIG
_MODELCONFIG_METRICTAGSENTRY.containing_type = _MODELCONFIG
_MODELCONFIG_PARAMETERSENTRY.fields_by_name['value'].message_type = _MODELPARAMETER
_MODELCONFIG_PARAMETERSENTRY.containing_type = _MODELCONFIG
_MODELCONFIG.fields_by_name['version_policy'].message_type = _MODELVERSIONPOLICY
_MODELCONFIG.fields_by_name['input'].message_type = _MODELINPUT
_MODELCONFIG.fields_by_name['output'].message_type = _MODELOUTPUT
_MODELCONFIG.fields_by_name['optimization'].message_type = _MODELOPTIMIZATIONPOLICY
_MODELCONFIG.fields_by_name['dynamic_batching'].message_type = _MODELDYNAMICBATCHING
_MODELCONFIG.fields_by_name['sequence_batching'].message_type = _MODELSEQUENCEBATCHING
_MODELCONFIG.fields_by_name['ensemble_scheduling'].message_type = _MODELENSEMBLING
_MODELCONFIG.fields_by_name['instance_group'].message_type = _MODELINSTANCEGROUP
_MODELCONFIG.fields_by_name['cc_model_filenames'].message_type = _MODELCONFIG_CCMODELFILENAMESENTRY
_MODELCONFIG.fields_by_name['metric_tags'].message_type = _MODELCONFIG_METRICTAGSENTRY
_MODELCONFIG.fields_by_name['parameters'].message_type = _MODELCONFIG_PARAMETERSENTRY
_MODELCONFIG.fields_by_name['model_warmup'].message_type = _MODELWARMUP
_MODELCONFIG.oneofs_by_name['scheduling_choice'].fields.append(
  _MODELCONFIG.fields_by_name['dynamic_batching'])
_MODELCONFIG.fields_by_name['dynamic_batching'].containing_oneof = _MODELCONFIG.oneofs_by_name['scheduling_choice']
_MODELCONFIG.oneofs_by_name['scheduling_choice'].fields.append(
  _MODELCONFIG.fields_by_name['sequence_batching'])
_MODELCONFIG.fields_by_name['sequence_batching'].containing_oneof = _MODELCONFIG.oneofs_by_name['scheduling_choice']
_MODELCONFIG.oneofs_by_name['scheduling_choice'].fields.append(
  _MODELCONFIG.fields_by_name['ensemble_scheduling'])
_MODELCONFIG.fields_by_name['ensemble_scheduling'].containing_oneof = _MODELCONFIG.oneofs_by_name['scheduling_choice']
DESCRIPTOR.message_types_by_name['ModelInstanceGroup'] = _MODELINSTANCEGROUP
DESCRIPTOR.message_types_by_name['ModelTensorReshape'] = _MODELTENSORRESHAPE
DESCRIPTOR.message_types_by_name['ModelInput'] = _MODELINPUT
DESCRIPTOR.message_types_by_name['ModelOutput'] = _MODELOUTPUT
DESCRIPTOR.message_types_by_name['ModelVersionPolicy'] = _MODELVERSIONPOLICY
DESCRIPTOR.message_types_by_name['ModelOptimizationPolicy'] = _MODELOPTIMIZATIONPOLICY
DESCRIPTOR.message_types_by_name['ModelDynamicBatching'] = _MODELDYNAMICBATCHING
DESCRIPTOR.message_types_by_name['ModelSequenceBatching'] = _MODELSEQUENCEBATCHING
DESCRIPTOR.message_types_by_name['ModelEnsembling'] = _MODELENSEMBLING
DESCRIPTOR.message_types_by_name['ModelParameter'] = _MODELPARAMETER
DESCRIPTOR.message_types_by_name['ModelWarmup'] = _MODELWARMUP
DESCRIPTOR.message_types_by_name['ModelConfig'] = _MODELCONFIG
DESCRIPTOR.enum_types_by_name['DataType'] = _DATATYPE
_sym_db.RegisterFileDescriptor(DESCRIPTOR)

ModelInstanceGroup = _reflection.GeneratedProtocolMessageType('ModelInstanceGroup', (_message.Message,), dict(
  DESCRIPTOR = _MODELINSTANCEGROUP,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInstanceGroup)
  ))
_sym_db.RegisterMessage(ModelInstanceGroup)

ModelTensorReshape = _reflection.GeneratedProtocolMessageType('ModelTensorReshape', (_message.Message,), dict(
  DESCRIPTOR = _MODELTENSORRESHAPE,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelTensorReshape)
  ))
_sym_db.RegisterMessage(ModelTensorReshape)

ModelInput = _reflection.GeneratedProtocolMessageType('ModelInput', (_message.Message,), dict(
  DESCRIPTOR = _MODELINPUT,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelInput)
  ))
_sym_db.RegisterMessage(ModelInput)

ModelOutput = _reflection.GeneratedProtocolMessageType('ModelOutput', (_message.Message,), dict(
  DESCRIPTOR = _MODELOUTPUT,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOutput)
  ))
_sym_db.RegisterMessage(ModelOutput)

ModelVersionPolicy = _reflection.GeneratedProtocolMessageType('ModelVersionPolicy', (_message.Message,), dict(

  Latest = _reflection.GeneratedProtocolMessageType('Latest', (_message.Message,), dict(
    DESCRIPTOR = _MODELVERSIONPOLICY_LATEST,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Latest)
    ))
  ,

  All = _reflection.GeneratedProtocolMessageType('All', (_message.Message,), dict(
    DESCRIPTOR = _MODELVERSIONPOLICY_ALL,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.All)
    ))
  ,

  Specific = _reflection.GeneratedProtocolMessageType('Specific', (_message.Message,), dict(
    DESCRIPTOR = _MODELVERSIONPOLICY_SPECIFIC,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy.Specific)
    ))
  ,
  DESCRIPTOR = _MODELVERSIONPOLICY,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelVersionPolicy)
  ))
_sym_db.RegisterMessage(ModelVersionPolicy)
_sym_db.RegisterMessage(ModelVersionPolicy.Latest)
_sym_db.RegisterMessage(ModelVersionPolicy.All)
_sym_db.RegisterMessage(ModelVersionPolicy.Specific)

ModelOptimizationPolicy = _reflection.GeneratedProtocolMessageType('ModelOptimizationPolicy', (_message.Message,), dict(

  Graph = _reflection.GeneratedProtocolMessageType('Graph', (_message.Message,), dict(
    DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_GRAPH,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Graph)
    ))
  ,

  Cuda = _reflection.GeneratedProtocolMessageType('Cuda', (_message.Message,), dict(
    DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_CUDA,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.Cuda)
    ))
  ,

  ExecutionAccelerators = _reflection.GeneratedProtocolMessageType('ExecutionAccelerators', (_message.Message,), dict(

    Accelerator = _reflection.GeneratedProtocolMessageType('Accelerator', (_message.Message,), dict(

      ParametersEntry = _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), dict(
        DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY,
        __module__ = 'model_config_pb2'
        # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry)
        ))
      ,
      DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR,
      __module__ = 'model_config_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
      ))
    ,
    DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.ExecutionAccelerators)
    ))
  ,

  PinnedMemoryBuffer = _reflection.GeneratedProtocolMessageType('PinnedMemoryBuffer', (_message.Message,), dict(
    DESCRIPTOR = _MODELOPTIMIZATIONPOLICY_PINNEDMEMORYBUFFER,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy.PinnedMemoryBuffer)
    ))
  ,
  DESCRIPTOR = _MODELOPTIMIZATIONPOLICY,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelOptimizationPolicy)
  ))
_sym_db.RegisterMessage(ModelOptimizationPolicy)
_sym_db.RegisterMessage(ModelOptimizationPolicy.Graph)
_sym_db.RegisterMessage(ModelOptimizationPolicy.Cuda)
_sym_db.RegisterMessage(ModelOptimizationPolicy.ExecutionAccelerators)
_sym_db.RegisterMessage(ModelOptimizationPolicy.ExecutionAccelerators.Accelerator)
_sym_db.RegisterMessage(ModelOptimizationPolicy.ExecutionAccelerators.Accelerator.ParametersEntry)
_sym_db.RegisterMessage(ModelOptimizationPolicy.PinnedMemoryBuffer)

ModelDynamicBatching = _reflection.GeneratedProtocolMessageType('ModelDynamicBatching', (_message.Message,), dict(
  DESCRIPTOR = _MODELDYNAMICBATCHING,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelDynamicBatching)
  ))
_sym_db.RegisterMessage(ModelDynamicBatching)

ModelSequenceBatching = _reflection.GeneratedProtocolMessageType('ModelSequenceBatching', (_message.Message,), dict(

  Control = _reflection.GeneratedProtocolMessageType('Control', (_message.Message,), dict(
    DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROL,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.Control)
    ))
  ,

  ControlInput = _reflection.GeneratedProtocolMessageType('ControlInput', (_message.Message,), dict(
    DESCRIPTOR = _MODELSEQUENCEBATCHING_CONTROLINPUT,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.ControlInput)
    ))
  ,

  StrategyDirect = _reflection.GeneratedProtocolMessageType('StrategyDirect', (_message.Message,), dict(
    DESCRIPTOR = _MODELSEQUENCEBATCHING_STRATEGYDIRECT,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyDirect)
    ))
  ,

  StrategyOldest = _reflection.GeneratedProtocolMessageType('StrategyOldest', (_message.Message,), dict(
    DESCRIPTOR = _MODELSEQUENCEBATCHING_STRATEGYOLDEST,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching.StrategyOldest)
    ))
  ,
  DESCRIPTOR = _MODELSEQUENCEBATCHING,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelSequenceBatching)
  ))
_sym_db.RegisterMessage(ModelSequenceBatching)
_sym_db.RegisterMessage(ModelSequenceBatching.Control)
_sym_db.RegisterMessage(ModelSequenceBatching.ControlInput)
_sym_db.RegisterMessage(ModelSequenceBatching.StrategyDirect)
_sym_db.RegisterMessage(ModelSequenceBatching.StrategyOldest)

ModelEnsembling = _reflection.GeneratedProtocolMessageType('ModelEnsembling', (_message.Message,), dict(

  Step = _reflection.GeneratedProtocolMessageType('Step', (_message.Message,), dict(

    InputMapEntry = _reflection.GeneratedProtocolMessageType('InputMapEntry', (_message.Message,), dict(
      DESCRIPTOR = _MODELENSEMBLING_STEP_INPUTMAPENTRY,
      __module__ = 'model_config_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step.InputMapEntry)
      ))
    ,

    OutputMapEntry = _reflection.GeneratedProtocolMessageType('OutputMapEntry', (_message.Message,), dict(
      DESCRIPTOR = _MODELENSEMBLING_STEP_OUTPUTMAPENTRY,
      __module__ = 'model_config_pb2'
      # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step.OutputMapEntry)
      ))
    ,
    DESCRIPTOR = _MODELENSEMBLING_STEP,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling.Step)
    ))
  ,
  DESCRIPTOR = _MODELENSEMBLING,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelEnsembling)
  ))
_sym_db.RegisterMessage(ModelEnsembling)
_sym_db.RegisterMessage(ModelEnsembling.Step)
_sym_db.RegisterMessage(ModelEnsembling.Step.InputMapEntry)
_sym_db.RegisterMessage(ModelEnsembling.Step.OutputMapEntry)

ModelParameter = _reflection.GeneratedProtocolMessageType('ModelParameter', (_message.Message,), dict(
  DESCRIPTOR = _MODELPARAMETER,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelParameter)
  ))
_sym_db.RegisterMessage(ModelParameter)

ModelWarmup = _reflection.GeneratedProtocolMessageType('ModelWarmup', (_message.Message,), dict(

  Input = _reflection.GeneratedProtocolMessageType('Input', (_message.Message,), dict(
    DESCRIPTOR = _MODELWARMUP_INPUT,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelWarmup.Input)
    ))
  ,

  InputsEntry = _reflection.GeneratedProtocolMessageType('InputsEntry', (_message.Message,), dict(
    DESCRIPTOR = _MODELWARMUP_INPUTSENTRY,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelWarmup.InputsEntry)
    ))
  ,
  DESCRIPTOR = _MODELWARMUP,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelWarmup)
  ))
_sym_db.RegisterMessage(ModelWarmup)
_sym_db.RegisterMessage(ModelWarmup.Input)
_sym_db.RegisterMessage(ModelWarmup.InputsEntry)

ModelConfig = _reflection.GeneratedProtocolMessageType('ModelConfig', (_message.Message,), dict(

  CcModelFilenamesEntry = _reflection.GeneratedProtocolMessageType('CcModelFilenamesEntry', (_message.Message,), dict(
    DESCRIPTOR = _MODELCONFIG_CCMODELFILENAMESENTRY,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig.CcModelFilenamesEntry)
    ))
  ,

  MetricTagsEntry = _reflection.GeneratedProtocolMessageType('MetricTagsEntry', (_message.Message,), dict(
    DESCRIPTOR = _MODELCONFIG_METRICTAGSENTRY,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig.MetricTagsEntry)
    ))
  ,

  ParametersEntry = _reflection.GeneratedProtocolMessageType('ParametersEntry', (_message.Message,), dict(
    DESCRIPTOR = _MODELCONFIG_PARAMETERSENTRY,
    __module__ = 'model_config_pb2'
    # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig.ParametersEntry)
    ))
  ,
  DESCRIPTOR = _MODELCONFIG,
  __module__ = 'model_config_pb2'
  # @@protoc_insertion_point(class_scope:nvidia.inferenceserver.ModelConfig)
  ))
_sym_db.RegisterMessage(ModelConfig)
_sym_db.RegisterMessage(ModelConfig.CcModelFilenamesEntry)
_sym_db.RegisterMessage(ModelConfig.MetricTagsEntry)
_sym_db.RegisterMessage(ModelConfig.ParametersEntry)


_MODELOPTIMIZATIONPOLICY_EXECUTIONACCELERATORS_ACCELERATOR_PARAMETERSENTRY._options = None
_MODELENSEMBLING_STEP_INPUTMAPENTRY._options = None
_MODELENSEMBLING_STEP_OUTPUTMAPENTRY._options = None
_MODELWARMUP_INPUTSENTRY._options = None
_MODELCONFIG_CCMODELFILENAMESENTRY._options = None
_MODELCONFIG_METRICTAGSENTRY._options = None
_MODELCONFIG_PARAMETERSENTRY._options = None
# @@protoc_insertion_point(module_scope)
